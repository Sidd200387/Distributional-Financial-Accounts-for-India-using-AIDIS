{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 723,
   "id": "838b48c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "os.chdir(r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\1992\")\n",
    "os.getcwd()\n",
    "\n",
    "# Filepaths for the datasets\n",
    "filepaths = {\n",
    "    'df1': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\1992_datasets\\Blocks 1,2,3,4,5_Visit 1_Household characteristics.dta\",\n",
    "    'df2': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\1992_datasets\\Blocks 14,15,16_Visit 1_Ownership of shares etc.dta\",\n",
    "    'df3': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\1992_datasets\\Block 17pt2_Visit 1_Particulars of cash loans payable by the household and transactions of loans.dta\",\n",
    "    'df4': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\1992_datasets\\Block 10_Visit 1_Owned buildings and other constructions and their transactions.dta\",\n",
    "    'df5': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\1992_datasets\\Block 7_Visit 1_Area of land owned as on date of survey and transactions.dta\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "c15eabab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 57031\n",
      "Unique HHIDs in df2: 46458\n",
      "Unique HHIDs in df3: 25133\n",
      "Unique HHIDs in df4: 47174\n",
      "Unique HHIDs in df5: 46548\n",
      "Common HHIDs across: 16893\n",
      "df1: 16893 rows after filtering\n",
      "df2: 52501 rows after filtering\n",
      "df3: 23622 rows after filtering\n",
      "df4: 83797 rows after filtering\n",
      "df5: 61175 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Load dataframes\n",
    "dataframes = {name: pd.read_stata(path) for name, path in filepaths.items()}\n",
    "\n",
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 6):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs across the first five datasets\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 6)\n",
    "]) #& set.union(*[\n",
    "    #set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(6, 6)\n",
    "#])\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 6):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5 = [dataframes[f'df{i}'] for i in range(1, 6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "d60f48ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Block_No\n",
       "15    44081\n",
       "14     6611\n",
       "16     1809\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 725,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Block_No'].value_counts(dropna=False)\n",
    "#df2['Block_No'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "df7da71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Number of common unique HHIDs between Block_No 15 and 14: 2899\n"
     ]
    }
   ],
   "source": [
    "# Get unique HHIDs for Block_No 15 and 14\n",
    "hhid_15 = set(df2.loc[df2['Block_No'] == 15, 'HHID'].unique())\n",
    "hhid_14 = set(df2.loc[df2['Block_No'] == 14, 'HHID'].unique())\n",
    "\n",
    "# Find common HHIDs\n",
    "common_hhids1 = hhid_15.intersection(hhid_14)\n",
    "\n",
    "# Print result\n",
    "print(f\"✅ Number of common unique HHIDs between Block_No 15 and 14: {len(common_hhids1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "96c99dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16628"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hhid_15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "id": "869da9f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in Shares and Debentures: 3118\n",
      "Unique HHIDs in Fin Assets (excl. shares and deb): 16628\n"
     ]
    }
   ],
   "source": [
    "print(f'Unique HHIDs in Shares and Debentures: {len(hhid_14)}')\n",
    "print(f'Unique HHIDs in Fin Assets (excl. shares and deb): {len(hhid_15)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "id": "a73968f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1: 2899 rows after filtering\n",
      "df2: 15991 rows after filtering\n",
      "df3: 4893 rows after filtering\n",
      "df4: 14933 rows after filtering\n",
      "df5: 11410 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Filter each dataframe to keep only rows with HHID in common_hhids1\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids1)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 6):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5 = [dataframes[f'df{i}'] for i in range(1, 6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "id": "a8ae0b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'RoundSchedule', 'Sample', 'Visit_no', 'State_Region', 'State',\n",
      "       'FlotNo', 'Sector', 'Stratum', 'SubSample', 'SubRound', 'Vill_Blk_Slno',\n",
      "       'SubStratum', 'Hhold_no', 'Level', 'Block_No', 'B14_15_16_q1',\n",
      "       'Sign_of_next_field', 'B14_15_16_q3', 'B14_15_16_q4', 'B14_15_16_q5',\n",
      "       'Sign_of_next_field2', 'B14_15_16_q6', 'Update_Code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "id": "da5ff758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ HHIDs retained: 2897\n",
      "❌ HHIDs dropped (missing B5_q16): 2\n"
     ]
    }
   ],
   "source": [
    "# Keeping relevant variables and merging\n",
    "\n",
    "# Initial row count\n",
    "initial_rows = len(df1)\n",
    "\n",
    "# Keep only rows where B5_q16 is not missing\n",
    "df1 = df1[df1['B5_q16'].notna()]\n",
    "df1 = df1[df1['HHID'].notna()]\n",
    "\n",
    "# Final row count\n",
    "final_rows = len(df1)\n",
    "dropped_rows = initial_rows - final_rows\n",
    "\n",
    "# Print summary\n",
    "print(f\"✅ HHIDs retained: {final_rows}\")\n",
    "print(f\"❌ HHIDs dropped (missing B5_q16): {dropped_rows}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "id": "ad0c0552",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_hhid = set(df1['HHID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "0d552a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2897\n"
     ]
    }
   ],
   "source": [
    "print(len(df1_hhid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "7e2a9c0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1: 2897 rows after filtering\n",
      "df2: 15980 rows after filtering\n",
      "df3: 4890 rows after filtering\n",
      "df4: 14923 rows after filtering\n",
      "df5: 11393 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(df1_hhid)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 6):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5 = [dataframes[f'df{i}'] for i in range(1, 6)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "534357bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_stata(\"df1_1992.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_1992.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_1992.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_1992.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_1992.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "758359b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'RoundSchedule', 'Sample', 'Visit_no', 'State_Region', 'State',\n",
      "       'FlotNo', 'Sector', 'Stratum', 'SubSample', 'SubRound', 'Vill_Blk_Slno',\n",
      "       'SubStratum', 'Hhold_no', 'Level', 'Block_No', 'B14_15_16_q1',\n",
      "       'Sign_of_next_field', 'B14_15_16_q3', 'B14_15_16_q4', 'B14_15_16_q5',\n",
      "       'Sign_of_next_field2', 'B14_15_16_q6', 'Update_Code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df2.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "id": "05205c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1\n",
    "df1 = df1[['HHID', 'State', 'B5_q16']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "c217eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df2: 2897\n",
      "Unique HHIDs in df2_1: 2897\n",
      "Unique HHIDs in df2_2: 2886\n"
     ]
    }
   ],
   "source": [
    "# df2\n",
    "\n",
    "# Filter df2 for Block_No 14 or 15 and select desired columns\n",
    "df2 = df2[df2['Block_No'].isin([14, 15])][['HHID', 'Block_No', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6']].copy()       # Fin Assets AND Shares, Debentures\n",
    "\n",
    "hhid2 = set(df2['HHID'].dropna())\n",
    "print(f'Unique HHIDs in df2: {len(hhid2)}')\n",
    "\n",
    "# df2 already contains only Block_No 14 and 15\n",
    "# Split into two based on Block_No\n",
    "df2_1 = df2[df2['Block_No'] == 14].copy()                    # Shares\n",
    "df2_1 = df2_1[df2_1['B14_15_16_q1'].isin([8])]\n",
    "\n",
    "df2_2 = df2[df2['Block_No'] == 15].copy()                    # Fin Assets\n",
    "df2_2 = df2_2[df2_2['B14_15_16_q1'].isin([1,2,3,4,5,6,9,10,11,12,13,14])]\n",
    "\n",
    "hhid2_1 = set(df2_1['HHID'].dropna())\n",
    "print(f'Unique HHIDs in df2_1: {len(hhid2_1)}')\n",
    "\n",
    "hhid2_2 = set(df2_2['HHID'].dropna())\n",
    "print(f'Unique HHIDs in df2_2: {len(hhid2_2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "2ed8d498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3\n",
    "df3 = df3[['HHID', 'B17_2_q1', 'B17_2_q6', 'B17_2_q24', 'B17_2_q25']].copy()         # Loan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 740,
   "id": "6a16cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3\n",
    "df3 = df3[df3['B17_2_q6'].isin([1,2,3,4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "id": "7ebda104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'RoundSchedule', 'Sample', 'Visit_no', 'State_Region', 'State',\n",
      "       'FlotNo', 'Sector', 'Stratum', 'SubSample', 'SubRound', 'Vill_Blk_Slno',\n",
      "       'SubStratum', 'Hhold_no', 'Level', 'B10_q1', 'Record_No', 'B10_q3_q13',\n",
      "       'B10_q4_q14', 'B10_q5_q15', 'B10_q6_q16', 'B10_q7_q17', 'B10_q8_q18',\n",
      "       'B10_q9_q19', 'B10_q10_q20', 'B10_q11_q21', 'B10_q12_q22',\n",
      "       'Update_Code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df4.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "cd1b7b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['B10_q12_q22'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "id": "b4274350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 743,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['B10_q1'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 744,
   "id": "438aad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4\n",
    "df4 = df4[['HHID', 'B10_q1', 'B10_q12_q22']].copy()         # Buildings- Block 10\n",
    "\n",
    "df4 = df4[df4['B10_q1'] == 11] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 745,
   "id": "ef0390b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'RoundSchedule', 'Sample', 'Visit_no', 'State_Region', 'State',\n",
      "       'FlotNo', 'Sector', 'Stratum', 'SubSample', 'SubRound', 'Vill_Blk_Slno',\n",
      "       'SubStratum', 'Hhold_no', 'Level', 'B7_q1', 'B7_q3', 'B7_q4', 'B7_q5',\n",
      "       'B7_q6', 'B7_q7', 'B7_q8', 'B7_q9', 'B7_q10', 'B7_q11', 'B7_q12',\n",
      "       'B7_q13', 'Update_Code'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 746,
   "id": "34d0dc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5\n",
    "df5 = df5[['HHID', 'B7_q1', 'B7_q7', 'B7_q13']].copy()     # Land- Block 5\n",
    "\n",
    "df5 = df5[df5['B7_q1'] == 99] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "89593ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing vals in MCE\n",
    "df1['B5_q16'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "id": "fe691744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'Block_No', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df2_1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 749,
   "id": "8f969ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Block_No\n",
      "14    2897\n",
      "Name: count, dtype: int64\n",
      "Block_No\n",
      "15    6112\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df2_1['Block_No'].value_counts(dropna=False))             # df2_1 is block 14- Shares and Debentures\n",
    "print(df2_2['Block_No'].value_counts(dropna=False))             # df2_2 is block 15- Fin Assets (excl. shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 750,
   "id": "b3f4f520",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_1.drop(columns=['Block_No'], inplace= True)\n",
    "df2_2.drop(columns=['Block_No'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "id": "f5557bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'State', 'B5_q16'], dtype='object')\n",
      "Index(['HHID', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n",
      "Index(['HHID', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n",
      "Index(['HHID', 'B17_2_q1', 'B17_2_q6', 'B17_2_q24', 'B17_2_q25'], dtype='object')\n",
      "Index(['HHID', 'B10_q1', 'B10_q12_q22'], dtype='object')\n",
      "Index(['HHID', 'B7_q1', 'B7_q7', 'B7_q13'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2_1.columns)\n",
    "print(df2_2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)\n",
    "print(df5.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "id": "746c5f47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n'df1': MCE,\\n\\n'df2_1': Shares and Debentures,                         # rename df3\\n\\n'df2_2': Fin Assets excluding shares,                   # rename df2\\n\\n'df3': Loan,                                            # rename df4\\n\\n'df4': Buildings,                                       # rename df5\\n\\n'df5': Land                                             # rename df6\\n\\n\""
      ]
     },
     "execution_count": 752,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "'df1': MCE,\n",
    "\n",
    "'df2_1': Shares and Debentures,                         # rename df3\n",
    "\n",
    "'df2_2': Fin Assets excluding shares,                   # rename df2\n",
    "\n",
    "'df3': Loan,                                            # rename df4\n",
    "\n",
    "'df4': Buildings,                                       # rename df5\n",
    "\n",
    "'df5': Land                                             # rename df6\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 753,
   "id": "065ecf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = df5.copy()\n",
    "df5 = df4.copy()\n",
    "df4 = df3.copy()\n",
    "df3 = df2_1.copy()\n",
    "df2 = df2_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 754,
   "id": "e42b85ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_stata(\"df1_1992.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_1992.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_1992.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_1992.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_1992.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_1992.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "id": "3a81436e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of households with >1 entries in B10_q1: 2896\n"
     ]
    }
   ],
   "source": [
    "#### df5\n",
    "\n",
    "# Filter only rows where B10_q1 == 11\n",
    "df_filtered = df5[df5['B10_q1'] == 11]\n",
    "\n",
    "# Count entries per household\n",
    "counts = df_filtered.groupby('HHID').size()\n",
    "\n",
    "# Find households with more than 1 entry\n",
    "multi_entry_households = counts[counts > 1]\n",
    "\n",
    "# Number of such households\n",
    "num_households = multi_entry_households.shape[0]\n",
    "\n",
    "print(\"Number of households with >1 entries in B10_q1:\", num_households)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 756,
   "id": "38fbb932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of households with >1 rows and non-zero in all rows: 212\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where B10_q1 == 11\n",
    "df_filtered = df5[df5['B10_q1'] == 11]\n",
    "\n",
    "# Keep only households with more than 1 row\n",
    "multi_entry_households = df_filtered.groupby('HHID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# Group by HHID and check if all rows have non-zero B10_q12_q22\n",
    "valid_households = multi_entry_households.groupby('HHID').filter(\n",
    "    lambda x: (x['B10_q12_q22'] != 0).all()\n",
    ")\n",
    "\n",
    "# Count number of such households\n",
    "num_valid_households = valid_households['HHID'].nunique()\n",
    "print(\"Number of households with >1 rows and non-zero in all rows:\", num_valid_households)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 757,
   "id": "ff4aaacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported 212 households to 'households_nonzero_B10_q12_q22.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Step 1: Filter for B10_q1 == 11 ---\n",
    "df_filtered = df5[df5['B10_q1'] == 11]\n",
    "\n",
    "# --- Step 2: Keep only households with more than 1 row ---\n",
    "multi_entry_households = df_filtered.groupby('HHID').filter(lambda x: len(x) > 1)\n",
    "\n",
    "# --- Step 3: Keep only households where all rows have non-zero B10_q12_q22 ---\n",
    "valid_households = multi_entry_households.groupby('HHID').filter(\n",
    "    lambda x: (x['B10_q12_q22'] != 0).all()\n",
    ")\n",
    "\n",
    "# --- Step 4: Sort for readability ---\n",
    "valid_households = valid_households.sort_values(by=['HHID'])\n",
    "\n",
    "# --- Step 5: Export to CSV ---\n",
    "valid_households.to_csv(\"households_nonzero_B10_q12_q22.csv\", index=False)\n",
    "\n",
    "print(\"✅ Exported 212 households to 'households_nonzero_B10_q12_q22.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "id": "ea533153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          HHID  B10_q1  B10_q12_q22\n",
      "11842  4123461      11       225000\n"
     ]
    }
   ],
   "source": [
    "# Filter only rows where B10_q1 == 11\n",
    "df_filtered = df5[df5['B10_q1'] == 11]\n",
    "\n",
    "# Count entries per household\n",
    "counts = df_filtered.groupby('HHID').size()\n",
    "\n",
    "# Get household(s) with exactly 1 entry\n",
    "single_entry_households = counts[counts == 1].index\n",
    "\n",
    "# Show their rows\n",
    "single_entry_rows = df_filtered[df_filtered['HHID'].isin(single_entry_households)]\n",
    "print(single_entry_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "id": "bc0827b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported all 2899 households (max per HHID) to 'yupp.dta'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# One-liner: filter, then pick max per household (works for 1-row households too)\n",
    "final_rows = (\n",
    "    df5.loc[df5['B10_q1'] == 11]\n",
    "       .loc[lambda d: d.groupby('HHID')['B10_q12_q22'].idxmax()]\n",
    "       .sort_values('HHID')\n",
    ")\n",
    "\n",
    "# Export to Stata\n",
    "final_rows.to_stata('yupp.dta', write_index=False)\n",
    "\n",
    "print(\"✅ Exported all 2899 households (max per HHID) to 'yupp.dta'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "id": "49b0b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = final_rows.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "id": "a637fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_stata(\"df1_1992.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_1992.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_1992.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_1992.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_1992.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_1992.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 762,
   "id": "43145f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'State', 'B5_q16'], dtype='object')\n",
      "Index(['HHID', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n",
      "Index(['HHID', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n",
      "Index(['HHID', 'B17_2_q1', 'B17_2_q6', 'B17_2_q24', 'B17_2_q25'], dtype='object')\n",
      "Index(['HHID', 'B10_q1', 'B10_q12_q22'], dtype='object')\n",
      "Index(['HHID', 'B7_q1', 'B7_q7', 'B7_q13'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)\n",
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "db63f109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B14_15_16_q1\n",
      "8    2897\n",
      "Name: count, dtype: int64\n",
      "B14_15_16_q1\n",
      "14    2750\n",
      "5      795\n",
      "9      674\n",
      "10     657\n",
      "4      336\n",
      "1      255\n",
      "3      236\n",
      "6      132\n",
      "2      118\n",
      "12      93\n",
      "13      58\n",
      "11       8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df3['B14_15_16_q1'].value_counts(dropna=False))             # df3 is block 14- Shares and Debentures\n",
    "print(df2['B14_15_16_q1'].value_counts(dropna=False))             # df2 is block 15- Fin Assets (excl. shares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 764,
   "id": "210c593d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'State', 'B5_q16'], dtype='object')\n",
      "Index(['HHID', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n",
      "Index(['HHID', 'B14_15_16_q1', 'B14_15_16_q3', 'B14_15_16_q6'], dtype='object')\n",
      "Index(['HHID', 'B17_2_q1', 'B17_2_q6', 'B17_2_q24', 'B17_2_q25'], dtype='object')\n",
      "Index(['HHID', 'B10_q1', 'B10_q12_q22'], dtype='object')\n",
      "Index(['HHID', 'B7_q1', 'B7_q7', 'B7_q13'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)\n",
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "929892dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming, creating, dropping columns \n",
    "\n",
    "df1  = df1.rename(columns={\n",
    "    'B5_q16': 'MCE'\n",
    "})\n",
    "\n",
    "df2  = df2.rename(columns={\n",
    "    'B14_15_16_q1': 'Fin_Asset_Serial',\n",
    "    'B14_15_16_q3': 'Fin_Asset_val1',\n",
    "    'B14_15_16_q6': 'Fin_Asset_val2'\n",
    "})\n",
    "\n",
    "df3  = df3.rename(columns={\n",
    "    'B14_15_16_q1': 'Share_Asset_serial',\n",
    "    'B14_15_16_q3': 'Share_Asset_val1',\n",
    "    'B14_15_16_q6': 'Share_Asset_val2'\n",
    "})\n",
    "\n",
    "df4  = df4.rename(columns={\n",
    "    'B17_2_q1': 'Liability_serial',\n",
    "    'B17_2_q6': 'Credit_Agency',\n",
    "    'B17_2_q24': 'Liability_val1',\n",
    "    'B17_2_q25': 'Liability_val2'\n",
    "})\n",
    "\n",
    "\n",
    "df5  = df5.rename(columns={\n",
    "    'B10_q1': 'Building_serial',\n",
    "    'B10_q12_q22': 'Building_value'\n",
    "})\n",
    "\n",
    "\n",
    "df6  = df6.rename(columns={\n",
    "    'B7_q1': 'Land_serial',\n",
    "    'B7_q7': 'Land_val1',\n",
    "    'B7_q13': 'Land_val2'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "52ee7321",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_stata(\"df1_1992.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_1992.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_1992.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_1992.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_1992.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_1992.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "f0259078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'State', 'MCE'], dtype='object')\n",
      "Index(['HHID', 'Fin_Asset_Serial', 'Fin_Asset_val1', 'Fin_Asset_val2'], dtype='object')\n",
      "Index(['HHID', 'Share_Asset_serial', 'Share_Asset_val1', 'Share_Asset_val2'], dtype='object')\n",
      "Index(['HHID', 'Liability_serial', 'Credit_Agency', 'Liability_val1',\n",
      "       'Liability_val2'],\n",
      "      dtype='object')\n",
      "Index(['HHID', 'Building_serial', 'Building_value'], dtype='object')\n",
      "Index(['HHID', 'Land_serial', 'Land_val1', 'Land_val2'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)\n",
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "248c5029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Land_serial\n",
       "99    2897\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 768,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6['Land_serial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "1d1cce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2\n",
    "#df2 = df2[df2['Fin_Asset_Serial'].isin([1,2,3,4,5,6,7,8,9,10,11,12,13,14])]\n",
    "\n",
    "# df3\n",
    "#df3 = df3[df3['Share_Asset_serial'] == 8] \n",
    "\n",
    "# df4\n",
    "#df4 = df4[df4['Credit_Agency'].isin([1,2,3,4,5,6])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "8dec0381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fin_Asset_Serial\n",
       "14    2750\n",
       "5      795\n",
       "9      674\n",
       "10     657\n",
       "4      336\n",
       "1      255\n",
       "3      236\n",
       "6      132\n",
       "2      118\n",
       "12      93\n",
       "13      58\n",
       "11       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Fin_Asset_Serial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "3bdcf467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Share_Asset_serial\n",
       "8    2897\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 771,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3['Share_Asset_serial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "id": "08eebbfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit_Agency\n",
       "2.0    2064\n",
       "3.0     910\n",
       "1.0     316\n",
       "5.0     152\n",
       "6.0     110\n",
       "4.0      51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 772,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['Credit_Agency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "id": "e8fcddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Column  Missing_Values  Zero_Values\n",
      "0   HHID               0            0\n",
      "1  State               0            0\n",
      "2    MCE               0            0\n",
      "Unique HHIDs: 2897\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in df1\n",
    "\n",
    "cols = ['HHID', 'State', 'MCE']\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Missing_Values': [df1[col].isna().sum() for col in cols],\n",
    "    'Zero_Values': [(df1[col] == 0).sum() for col in cols]\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(df1['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "id": "86f82688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Fin_Asset_Serial', 'Fin_Asset_val1', 'Fin_Asset_val2'], dtype='object')"
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "id": "5086cef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\3678178701.py:28: FutureWarning: The behavior of array concatenation with empty entries is deprecated. In a future version, this will no longer exclude empty items when determining the result dtype. To retain the old behavior, exclude the empty entries before the concat operation.\n",
      "  df6['Land_value'] = df6['Land_val1'].combine_first(df6['Land_val2'])\n"
     ]
    }
   ],
   "source": [
    "# df2\n",
    "df2['Fin_Asset_Value'] = np.where(\n",
    "    df2['Fin_Asset_Serial'] == 14,\n",
    "    df2['Fin_Asset_val1'],\n",
    "    df2['Fin_Asset_val1'].combine_first(df2['Fin_Asset_val2'])\n",
    ")\n",
    "df2['Fin_Asset_Value'] = pd.Series(df2['Fin_Asset_Value']).fillna(0)\n",
    "\n",
    "# df3 (Share Asset value)\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_val1'].combine_first(df3['Share_Asset_val2'])\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_Value'].fillna(0)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "\n",
    "# Mapping dictionary using float keys (since your column is float64)\n",
    "Share_Asset_Map = {\n",
    "    8.0: 'Shares and Debentures'\n",
    "}\n",
    "\n",
    "# df4 (Loans)\n",
    "df4['Liability_value'] = df4['Liability_val1'].combine_first(df4['Liability_val2'])\n",
    "df4['Liability_value'] = df4['Liability_value'].fillna(0)\n",
    "\n",
    "# df5 (Buildings value)\n",
    "df5['Building_value'] = df5['Building_value'].fillna(0)\n",
    "\n",
    "# df6 (Land value)\n",
    "\n",
    "df6['Land_value'] = df6['Land_val1'].combine_first(df6['Land_val2'])\n",
    "df6['Land_value'] = df6['Land_value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "id": "260a308f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit_Agency\n",
       "2.0    2064\n",
       "3.0     910\n",
       "1.0     316\n",
       "5.0     152\n",
       "6.0     110\n",
       "4.0      51\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['Credit_Agency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "id": "60db9108",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Building_value\n",
       "50000      97\n",
       "40000      89\n",
       "100000     85\n",
       "30000      78\n",
       "20000      77\n",
       "           ..\n",
       "16600       1\n",
       "30700       1\n",
       "134670      1\n",
       "197650      1\n",
       "5000000     1\n",
       "Name: count, Length: 605, dtype: int64"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5['Building_value'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "4c9f2c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fin_Asset_Serial\n",
       "14    2750\n",
       "5      795\n",
       "9      674\n",
       "10     657\n",
       "4      336\n",
       "1      255\n",
       "3      236\n",
       "6      132\n",
       "2      118\n",
       "12      93\n",
       "13      58\n",
       "11       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 778,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Fin_Asset_Serial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "1be93d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin_Asset_Serial\n",
      "14    2750\n",
      "5      795\n",
      "9      674\n",
      "10     657\n",
      "4      336\n",
      "1      255\n",
      "3      236\n",
      "6      132\n",
      "2      118\n",
      "12      93\n",
      "13      58\n",
      "11       8\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HHID  Fin_Asset_Serial  Fin_Asset_val1  Fin_Asset_val2  Fin_Asset_Value\n",
      "0  3104731                14           380.0             NaN            380.0\n",
      "1  3104731               999             NaN             NaN            380.0\n",
      "2  3107711                14           150.0             NaN            150.0\n",
      "3  3107711               999             NaN             NaN            150.0\n",
      "4  3107712                14           120.0             NaN            120.0\n",
      "2886\n"
     ]
    }
   ],
   "source": [
    "print(df2['Fin_Asset_Serial'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Fin_Asset_Value\n",
    "summary_rows = df2.groupby('HHID', as_index=False)['Fin_Asset_Value'].sum()\n",
    "summary_rows['Fin_Asset_Serial'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df2.columns:\n",
    "    if col not in ['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value']:\n",
    "        summary_rows[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df2_mod = pd.concat([df2, summary_rows], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df2_mod = df2_mod.sort_values(by=['HHID', 'Fin_Asset_Serial']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "print(df2_mod.head())\n",
    "print(df2_mod['HHID'].nunique())\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts(dropna=False)\n",
    "\n",
    "# Modify serials and labels, if any\n",
    "\n",
    "# df2_mod\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([14]), 'Fin_Asset_Serial'] = 31\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([4,5,6]), 'Fin_Asset_Serial'] = 32\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([9]), 'Fin_Asset_Serial'] = 34\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([10,11]), 'Fin_Asset_Serial'] = 35\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([1,2,3,12,13]), 'Fin_Asset_Serial'] = 36\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([999]), 'Fin_Asset_Serial'] = 37\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "# Fin Asset Mapping\n",
    "\n",
    "Fin_Asset_map = {\n",
    "\n",
    "    31.0: 'Currency',\n",
    "    32.0: 'Deposits',\n",
    "    34.0: 'Life Insurance fund',\n",
    "    35.0: 'Provident and Pension fund',\n",
    "    36.0: 'Claims on Government',\n",
    "    37.0: 'Total financial assets (other thanshares and related instruments)'\n",
    "}\n",
    "\n",
    "df2_mod = df2_mod.groupby(['HHID', 'Fin_Asset_Serial'], as_index=False).agg({\n",
    "    'Fin_Asset_Value': 'sum'  \n",
    "})\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts()\n",
    "\n",
    "df2 = df2_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "id": "3309242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Liability_serial', 'Credit_Agency', 'Liability_val1',\n",
       "       'Liability_val2', 'Liability_value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 780,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "215dbac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit_Agency\n",
      "2.0    2064\n",
      "3.0     910\n",
      "1.0     316\n",
      "5.0     152\n",
      "6.0     110\n",
      "4.0      51\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HHID  Liability_serial  Credit_Agency  Liability_val1  Liability_val2  \\\n",
      "0  3104731               1.0            2.0             NaN             NaN   \n",
      "1  3104731               NaN          999.0             NaN             NaN   \n",
      "2  3107711               1.0            3.0             NaN             NaN   \n",
      "3  3107711               NaN          999.0             NaN             NaN   \n",
      "4  3107712               1.0            3.0             NaN             NaN   \n",
      "\n",
      "   Liability_value  \n",
      "0              0.0  \n",
      "1              0.0  \n",
      "2              0.0  \n",
      "3              0.0  \n",
      "4              0.0  \n",
      "2487\n"
     ]
    }
   ],
   "source": [
    "print(df4['Credit_Agency'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Liability_value\n",
    "summary_rows_liab = df4.groupby('HHID', as_index=False)['Liability_value'].sum()\n",
    "summary_rows_liab['Credit_Agency'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df4.columns:\n",
    "    if col not in ['HHID', 'Credit_Agency', 'Liability_value']:\n",
    "        summary_rows_liab[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df4_mod = pd.concat([df4, summary_rows_liab], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df4_mod = df4_mod.sort_values(by=['HHID', 'Credit_Agency']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df4_mod.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "print(df4_mod.head())\n",
    "print(df4_mod['HHID'].nunique())\n",
    "\n",
    "\n",
    "# df4_mod\n",
    "df4_mod.loc[df4_mod['Credit_Agency'].isin([2,3]), 'Credit_Agency'] = 91\n",
    "df4_mod.loc[df4_mod['Credit_Agency'].isin([1,4,5,6]), 'Credit_Agency'] = 92\n",
    "\n",
    "df4_mod = df4_mod.reset_index(drop=True)\n",
    "\n",
    "# Liability Map\n",
    "\n",
    "Credit_Agency_Map = {\n",
    "\n",
    "    91.0: 'Bank advances',\n",
    "    92.0: 'Non-banking loans and advances',\n",
    "    999.0: 'Total Institutional Liabilities'\n",
    "}\n",
    "\n",
    "df4_mod = df4_mod.groupby(['HHID', 'Credit_Agency'], as_index=False).agg({\n",
    "    'Liability_value': 'sum'  \n",
    "})\n",
    "\n",
    "df4_mod = df4_mod.reset_index(drop=True)\n",
    "\n",
    "df4_mod.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "df4_mod['Credit_Agency'].value_counts()\n",
    "\n",
    "df4 = df4_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "b800753d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'State', 'MCE'], dtype='object')\n",
      "Index(['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value'], dtype='object')\n",
      "Index(['HHID', 'Share_Asset_serial', 'Share_Asset_val1', 'Share_Asset_val2',\n",
      "       'Share_Asset_Value'],\n",
      "      dtype='object')\n",
      "Index(['HHID', 'Credit_Agency', 'Liability_value'], dtype='object')\n",
      "Index(['HHID', 'Building_serial', 'Building_value'], dtype='object')\n",
      "Index(['HHID', 'Land_serial', 'Land_val1', 'Land_val2', 'Land_value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)\n",
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "a1aad28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "\n",
    "df3.drop(columns=['Share_Asset_val1', 'Share_Asset_val2'], inplace=True)\n",
    "df6.drop(columns=['Land_val1', 'Land_val2'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "id": "fad6d4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'State', 'MCE'], dtype='object')\n",
      "Index(['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value'], dtype='object')\n",
      "Index(['HHID', 'Share_Asset_serial', 'Share_Asset_Value'], dtype='object')\n",
      "Index(['HHID', 'Credit_Agency', 'Liability_value'], dtype='object')\n",
      "Index(['HHID', 'Building_serial', 'Building_value'], dtype='object')\n",
      "Index(['HHID', 'Land_serial', 'Land_value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df1.columns)\n",
    "print(df2.columns)\n",
    "print(df3.columns)\n",
    "print(df4.columns)\n",
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "id": "bc7c238e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_stata(\"df1_1992.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_1992.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_1992.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_1992.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_1992.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_1992.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "id": "fef55831",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['df1'] = df1\n",
    "dataframes['df2'] = df2\n",
    "dataframes['df3'] = df3\n",
    "dataframes['df4'] = df4\n",
    "dataframes['df5'] = df5\n",
    "dataframes['df6'] = df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "id": "80bf1a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 2897\n",
      "Unique HHIDs in df2: 2886\n",
      "Unique HHIDs in df3: 2897\n",
      "Unique HHIDs in df4: 2487\n",
      "Unique HHIDs in df5: 2897\n",
      "Unique HHIDs in df6: 2897\n",
      "Common HHIDs across: 2479\n",
      "df1: 2479 rows after filtering\n",
      "df2: 7507 rows after filtering\n",
      "df3: 2479 rows after filtering\n",
      "df4: 5155 rows after filtering\n",
      "df5: 2479 rows after filtering\n",
      "df6: 2479 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 7):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs: intersection of first five & union of df6, df7\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 7)\n",
    "])\n",
    "\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 7):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6 = [dataframes[f'df{i}'] for i in range(1, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "id": "06663061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['HHID', 'Building_serial', 'Building_value'], dtype='object')\n",
      "Index(['HHID', 'Land_serial', 'Land_value'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df5.columns)\n",
    "print(df6.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "2f1fed04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs: 2479\n",
      "Number of duplicates: 0\n",
      "Unique HHIDs with Fin_Asset_Serial = 37: 2479\n",
      "Unique HHIDs with Share_Asset_serial = 5: 2479\n",
      "Unique HHIDs with Credit_Agency = 999: 2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\1237505174.py:17: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\1237505174.py:26: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged1.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Merging all datasets\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6]\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='HHID', how='inner'), dfs)\n",
    "\n",
    "merged_df.to_stata(\"merged.dta\", write_index=False)\n",
    "\n",
    "unique_hhids = set(merged_df['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "# Real Estate Value\n",
    "\n",
    "merged_df['Real Estate'] = merged_df['Building_value'] + merged_df['Land_value']\n",
    "\n",
    "merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1 = merged_df.copy()\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "merged_df1.drop(['Building_value', 'Land_value'\n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "merged_df1.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "# Check for duplicates\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "dup_check = merged_df1.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Fin_Asset_Serial'] == 37, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Fin_Asset_Serial = 37: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Share_Asset_serial'] == 8, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Share_Asset_serial = 5: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Credit_Agency'] == 999, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Credit_Agency = 999: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "id": "c025b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Column  Missing_Values  Zero_Values\n",
      "0                HHID               0            0\n",
      "1               State               0            0\n",
      "2                 MCE               0            0\n",
      "3    Fin_Asset_Serial               0            0\n",
      "4  Share_Asset_serial               0            0\n",
      "5       Credit_Agency               0            0\n",
      "6   Share_Asset_Value               0           23\n",
      "7     Fin_Asset_Value               0           28\n",
      "8     Liability_value               0          325\n",
      "9         Real Estate               0            0\n",
      "Unique HHIDs: 2479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\3725251504.py:20: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "cols = ['HHID', 'State', 'MCE', 'Fin_Asset_Serial', 'Share_Asset_serial',\n",
    "        'Credit_Agency', 'Share_Asset_Value', 'Fin_Asset_Value',\n",
    "        'Liability_value', 'Real Estate']\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Missing_Values': [merged_df1[col].isna().sum() for col in cols],\n",
    "    'Zero_Values': [(merged_df1[col] == 0).sum() for col in cols]\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(merged_df1['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "id": "8085de02",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3 = merged_df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "id": "822d01fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column\n",
    "merged_df3['Fin_Asset'] = merged_df3['Fin_Asset_Serial'].map(Fin_Asset_map)\n",
    "merged_df3['Share_Asset'] = merged_df3['Share_Asset_serial'].map(Share_Asset_Map)\n",
    "merged_df3['Liability'] = merged_df3['Credit_Agency'].map(Credit_Agency_Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "d4518a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs: 2479\n"
     ]
    }
   ],
   "source": [
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(merged_df3['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "id": "2ace01d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs present in all three groups: 2479\n"
     ]
    }
   ],
   "source": [
    "# HHIDs satisfying each condition\n",
    "hhid_fin = set(merged_df3.loc[merged_df3['Fin_Asset_Serial'] == 37, 'HHID'])\n",
    "hhid_share = set(merged_df3.loc[merged_df3['Share_Asset_serial'] == 8, 'HHID'])\n",
    "hhid_credit = set(merged_df3.loc[merged_df3['Credit_Agency'] == 999, 'HHID'])\n",
    "\n",
    "# Intersection: HHIDs present in all three\n",
    "common_hhids = hhid_fin & hhid_share & hhid_credit\n",
    "\n",
    "# Keep only those rows\n",
    "filtered_df = merged_df3[merged_df3['HHID'].isin(common_hhids)].copy()\n",
    "\n",
    "# Print how many unique HHIDs remain\n",
    "print(f\"Unique HHIDs present in all three groups: {len(common_hhids)}\")\n",
    "\n",
    "#filtered_df.drop(['Fin_Asset', 'Share_Asset', 'Liability'], axis=1, inplace=True)\n",
    "\n",
    "#filtered_df.to_stata(\"final(1).dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "59b54375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 32, 34, 35, 36, 37]\n",
      "[8]\n",
      "[91.0, 92.0, 999.0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks if grouping worked correctly\n",
    "\n",
    "print(sorted(filtered_df['Fin_Asset_Serial'].unique()))\n",
    "#print(filtered_df['Fin_Asset_Serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Share_Asset_serial'].unique()))\n",
    "#print(filtered_df['Share_Asset_serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Credit_Agency'].unique()))\n",
    "#print(filtered_df['Credit_Agency'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "0fc91bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped1 = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "888374b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       HHID  Gross wealth\n",
      "0   3104731       72580.0\n",
      "4   3107711        8650.0\n",
      "8   3107712        5370.0\n",
      "12  3109751      104679.0\n",
      "18  3110751      409361.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\2515997013.py:31: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  df_grouped1.to_stata(\"final(25).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter the relevant rows satisfying your condition\n",
    "wealth_rows = df_grouped1[\n",
    "    (df_grouped1['Credit_Agency'] == 999) &\n",
    "    (df_grouped1['Fin_Asset_Serial'] == 37) &\n",
    "    (df_grouped1['Share_Asset_serial'] == 8)\n",
    "].copy()\n",
    "\n",
    "# Step 2: Compute wealth for these rows\n",
    "wealth_rows['Gross wealth'] = (\n",
    "    wealth_rows['Fin_Asset_Value'] +\n",
    "    wealth_rows['Share_Asset_Value'] +\n",
    "    wealth_rows['Real Estate']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Financial Assets'] = (\n",
    "    wealth_rows['Fin_Asset_Value']+\n",
    "    wealth_rows['Share_Asset_Value']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Institutional liabilities'] = wealth_rows['Liability_value']\n",
    "\n",
    "# Step 3: Keep only HHID and wealth columns\n",
    "wealth_per_hhid = wealth_rows[['HHID', 'Gross wealth', 'Total Financial Assets', 'Total Institutional liabilities']]\n",
    "\n",
    "# Step 4: Merge wealth back to the full dataset based on HHID\n",
    "df_grouped1 = df_grouped1.merge(wealth_per_hhid, on='HHID', how='left')\n",
    "\n",
    "# Step 5: View result\n",
    "print(df_grouped1[['HHID', 'Gross wealth']].drop_duplicates().head())\n",
    "\n",
    "df_grouped1.to_stata(\"final(25).dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "167ad480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2 =  df_grouped1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "37e4f3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize columns with NaN\n",
    "df_grouped2['Asset_serial'] = np.nan\n",
    "df_grouped2['Asset_Value'] = np.nan\n",
    "df_grouped2['Asset_Name'] = np.nan\n",
    "\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "\n",
    "df_grouped2['HHID'] = df_grouped2['HHID'].astype(str)\n",
    "\n",
    "#df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "id": "f0356807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      HHID  Credit_Agency  Fin_Asset_Serial  Share_Asset_serial  Asset_serial  \\\n",
      "0  3104731           91.0                31                   8            31   \n",
      "1  3104731           91.0                31                   8           208   \n",
      "2  3104731           91.0                31                   8           300   \n",
      "3  3104731           91.0                31                   8          1000   \n",
      "4  3104731           91.0                31                   8          1500   \n",
      "\n",
      "   Asset_Value  State     MCE  Fin_Asset_Value  Share_Asset_Value  ...  \\\n",
      "0        380.0      4  1400.0            380.0             1200.0  ...   \n",
      "1       1200.0      4  1400.0            380.0             1200.0  ...   \n",
      "2      71000.0      4  1400.0            380.0             1200.0  ...   \n",
      "3      72580.0      4  1400.0            380.0             1200.0  ...   \n",
      "4       1580.0      4  1400.0            380.0             1200.0  ...   \n",
      "\n",
      "   Building_serial  Land_serial  Real Estate  Fin_Asset  \\\n",
      "0               11           99        71000   Currency   \n",
      "1               11           99        71000   Currency   \n",
      "2               11           99        71000   Currency   \n",
      "3               11           99        71000   Currency   \n",
      "4               11           99        71000   Currency   \n",
      "\n",
      "             Share_Asset      Liability Gross wealth  Total Financial Assets  \\\n",
      "0  Shares and Debentures  Bank advances      72580.0                  1580.0   \n",
      "1  Shares and Debentures  Bank advances      72580.0                  1580.0   \n",
      "2  Shares and Debentures  Bank advances      72580.0                  1580.0   \n",
      "3  Shares and Debentures  Bank advances      72580.0                  1580.0   \n",
      "4  Shares and Debentures  Bank advances      72580.0                  1580.0   \n",
      "\n",
      "   Total Institutional liabilities              Asset_Name  \n",
      "0                              0.0                Currency  \n",
      "1                              0.0   Shares and Debentures  \n",
      "2                              0.0             Real Estate  \n",
      "3                              0.0            Gross wealth  \n",
      "4                              0.0  Total Financial Assets  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\347409114.py:69: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  asset_df.to_stata(\"final(28).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store new rows\n",
    "rows = []\n",
    "\n",
    "# Loop over each unique group\n",
    "for _, group in df_grouped2.groupby(group_cols):\n",
    "\n",
    "    # Get first row for copying other columns\n",
    "    first_row = group.iloc[0].to_dict()\n",
    "\n",
    "    # 1. Financial Asset Row\n",
    "    if not np.isnan(first_row['Fin_Asset_Value']):\n",
    "        fa_row = first_row.copy()\n",
    "        fa_row['Asset_serial'] = first_row['Fin_Asset_Serial']\n",
    "        fa_row['Asset_Value'] = first_row['Fin_Asset_Value']\n",
    "        fa_row['Asset_Name'] = first_row['Fin_Asset']\n",
    "        rows.append(fa_row)\n",
    "\n",
    "    # 2. Share Asset Row\n",
    "    if not np.isnan(first_row['Share_Asset_Value']):\n",
    "        sa_row = first_row.copy()\n",
    "        sa_row['Asset_serial'] = 200 + first_row['Share_Asset_serial']  # As you specified\n",
    "        sa_row['Asset_Value'] = first_row['Share_Asset_Value']\n",
    "        sa_row['Asset_Name'] = first_row['Share_Asset']\n",
    "        rows.append(sa_row)\n",
    "\n",
    "    # 3. Real Estate Row\n",
    "    if not np.isnan(first_row['Real Estate']):\n",
    "        re_row = first_row.copy()\n",
    "        re_row['Asset_serial'] = 300\n",
    "        re_row['Asset_Value'] = first_row['Real Estate']\n",
    "        re_row['Asset_Name'] = 'Real Estate'\n",
    "        rows.append(re_row)\n",
    "\n",
    "    # 4. Gross wealth Row\n",
    "    if not np.isnan(first_row['Gross wealth']):\n",
    "        w_row = first_row.copy()\n",
    "        w_row['Asset_serial'] = 1000\n",
    "        w_row['Asset_Value'] = first_row['Gross wealth']\n",
    "        w_row['Asset_Name'] = 'Gross wealth'\n",
    "        rows.append(w_row)\n",
    "\n",
    "    # 6. Total Financial Assets Row\n",
    "    if not np.isnan(first_row['Total Financial Assets']):\n",
    "        tfa_row = first_row.copy()\n",
    "        tfa_row['Asset_serial'] = 1500\n",
    "        tfa_row['Asset_Value'] = first_row['Total Financial Assets']\n",
    "        tfa_row['Asset_Name'] = 'Total Financial Assets'\n",
    "        rows.append(tfa_row)   \n",
    "\n",
    "# Convert to DataFrame\n",
    "asset_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: Arrange columns nicely\n",
    "cols_order = group_cols + ['Asset_serial', 'Asset_Value'] + [col for col in df_grouped2.columns if col not in group_cols + ['Asset_serial', 'Asset_Value']]\n",
    "asset_df = asset_df[cols_order]\n",
    "\n",
    "# Check result\n",
    "print(asset_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Liabilities\n",
    "\n",
    "asset_df = asset_df.rename(columns={\n",
    "    'Credit_Agency': 'Liability_serial',\n",
    "    'Liability': 'Liability_name'\n",
    "})\n",
    "\n",
    "asset_df.to_stata(\"final(28).dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "id": "b6be6426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subsets: 15856\n",
      "Number of duplicates: 63424\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "group_cols = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "group_cols1 = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial', 'Asset_serial']\n",
    "\n",
    "# Count unique combinations\n",
    "num_subsets = asset_df[group_cols].drop_duplicates().shape[0]\n",
    "\n",
    "print(f\"Number of unique subsets: {num_subsets}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols1)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "id": "54fd8881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HHID                                object\n",
       "Liability_serial                   float64\n",
       "Fin_Asset_Serial                     int64\n",
       "Share_Asset_serial                   int64\n",
       "Asset_serial                         int64\n",
       "Asset_Value                        float64\n",
       "State                                int64\n",
       "MCE                                float64\n",
       "Fin_Asset_Value                    float64\n",
       "Share_Asset_Value                  float64\n",
       "Liability_value                    float64\n",
       "Building_serial                      int64\n",
       "Land_serial                          int64\n",
       "Real Estate                          int64\n",
       "Fin_Asset                           object\n",
       "Share_Asset                         object\n",
       "Liability_name                      object\n",
       "Gross wealth                       float64\n",
       "Total Financial Assets             float64\n",
       "Total Institutional liabilities    float64\n",
       "Asset_Name                          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "208f8544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset_serial    0\n",
      "Asset_Value     0\n",
      "Asset_Name      0\n",
      "dtype: int64\n",
      "\n",
      "Total rows with missing values: 0\n",
      "Empty DataFrame\n",
      "Columns: [HHID, Liability_serial, Fin_Asset_Serial, Share_Asset_serial, Asset_serial, Asset_Value, State, MCE, Fin_Asset_Value, Share_Asset_Value, Liability_value, Building_serial, Land_serial, Real Estate, Fin_Asset, Share_Asset, Liability_name, Gross wealth, Total Financial Assets, Total Institutional liabilities, Asset_Name]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "# Check how many missing in each column\n",
    "print(asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().sum())\n",
    "\n",
    "# Optionally, view rows with missing in any of them\n",
    "missing_rows = asset_df[\n",
    "    asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().any(axis=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal rows with missing values: {len(missing_rows)}\")\n",
    "print(missing_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "id": "4ec97c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = asset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "5c08187c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial',\n",
       "       'Asset_serial', 'Asset_Value', 'State', 'MCE', 'Fin_Asset_Value',\n",
       "       'Share_Asset_Value', 'Liability_value', 'Building_serial',\n",
       "       'Land_serial', 'Real Estate', 'Fin_Asset', 'Share_Asset',\n",
       "       'Liability_name', 'Gross wealth', 'Total Financial Assets',\n",
       "       'Total Institutional liabilities', 'Asset_Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "id": "563c735c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\3300125772.py:9: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(1992).dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\3300125772.py:17: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(1992).dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\3300125772.py:24: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df1.to_stata(\"final(1992)_1.dta\", write_index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2479"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.drop(['Fin_Asset_Serial', \n",
    "               'Share_Asset_serial', \n",
    "               'Fin_Asset_Value',\n",
    "                'Share_Asset_Value', \n",
    "                'Fin_Asset',\n",
    "                'Share_Asset',               \n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "final_df.to_stata(\"final(1992).dta\", write_index=False)\n",
    "\n",
    "final_df = final_df[['HHID', 'State', 'MCE','Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value',\n",
    "                    'Real Estate', 'Total Financial Assets', 'Total Institutional liabilities', \n",
    "                    ]].copy()\n",
    "\n",
    "final_df.to_stata(\"final(1992).dta\", write_index=False)\n",
    "\n",
    "final_df1 = final_df[['HHID', 'State', 'MCE', 'Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value'\n",
    "                    ]].copy()\n",
    "\n",
    "final_df1.to_stata(\"final(1992)_1.dta\", write_index=False)\n",
    "\n",
    "final_df1['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "id": "c98e5ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 32, 34, 35, 36, 37, 208, 300, 1000, 1500]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(final_df1['Asset_serial'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "f8adf39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\2175703513.py:3: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df2.to_stata(\"final(1992)_2.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "group_cols1 = ['HHID', 'Liability_serial', 'Asset_serial']\n",
    "final_df2 = final_df1.drop_duplicates(subset=group_cols1, keep='first').reset_index(drop=True)\n",
    "final_df2.to_stata(\"final(1992)_2.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "8ae35789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All HHIDs are unique within each Asset_serial category.\n",
      " All HHIDs are unique within each Liability_serial category.\n"
     ]
    }
   ],
   "source": [
    "# Check for repetition\n",
    "\n",
    "# Check for uniqueness \n",
    "\n",
    "duplicates = final_df2.duplicated(subset=['Asset_serial', 'HHID', 'Liability_serial'], keep=False)\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Asset_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Asset_serial', 'HHID']].sort_values(['Asset_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Asset_serial category.\")\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Liability_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Liability_serial', 'HHID']].sort_values(['Liability_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Liability_serial category.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "7a96f66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2479"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093d4020",
   "metadata": {},
   "source": [
    "# Add State names corresponding to their codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "441e65df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_39844\\932734623.py:45: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df3.to_stata('1992_final.dta', write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# final_df2 (Merge D&D, D&N)\n",
    "final_df2.loc[final_df2['State'].isin([27,33]), 'State'] = 27\n",
    "final_df2['State'].value_counts()\n",
    "\n",
    "# State Map\n",
    "\n",
    "State_map = {\n",
    "    2.0: 'Andhra Pradesh',\n",
    "    3.0: 'Assam',\n",
    "    4.0: 'Bihar',\n",
    "    5.0: 'Gujarat',\n",
    "    6.0: 'Haryana',\n",
    "    7.0: 'Himachal Pradesh',\n",
    "    8.0: 'Jammu & Kashmir',\n",
    "    9.0: 'Karnataka',\n",
    "    10.0: 'Kerala',\n",
    "    11.0: 'Madhya Pradesh',\n",
    "    12.0: 'Maharashtra',\n",
    "    13.0: 'Manipur',\n",
    "    14.0: 'Meghalaya',\n",
    "    15.0: 'Nagaland',\n",
    "    16.0: 'Odisha',\n",
    "    17.0: 'Punjab',\n",
    "    18.0: 'Rajasthan',\n",
    "    19.0: 'Sikkim',\n",
    "    20.0: 'Tamilnadu',\n",
    "    21.0: 'Tripura',\n",
    "    22.0: 'Uttar Pradesh',\n",
    "    23.0: 'West Bengal',\n",
    "    24.0: 'Andaman & Nicobar',\n",
    "    25.0: 'Arunachal Pradesh',\n",
    "    26.0: 'Chandigarh',\n",
    "    27.0: 'Daman and Diu and Dadra and Nagar Haveli',\n",
    "    28.0: 'Delhi',\n",
    "    29.0: 'Goa',\n",
    "    30.0: 'Lakshadweep',\n",
    "    31.0: 'Mizoram',\n",
    "    32.0: 'Puducherry'\n",
    "}\n",
    "\n",
    "final_df3 = final_df2.copy()\n",
    "\n",
    "final_df3['State_name'] = final_df3['State'].map(State_map)\n",
    "\n",
    "final_df3.to_stata('1992_final.dta', write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
