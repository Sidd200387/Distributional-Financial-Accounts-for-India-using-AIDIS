{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "f89fcd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "os.chdir(r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\2003\")\n",
    "os.getcwd()\n",
    "\n",
    "# Filepaths for the datasets\n",
    "filepaths = {\n",
    "    'df1': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS 2003 All Datasets\\Visit 1 & 2 Combined_Block 3_Household characteristics.dta\",\n",
    "    'df2': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS 2003 All Datasets\\Visit 1 & 2 Combined_Block 13_financial assets other than shares & debentures owned by the household.dta\",\n",
    "    'df3': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS 2003 All Datasets\\Visit 1 & 2 Combined_Block 12_shares & debentures owned by the household in co operative societies & companies.dta\",\n",
    "    'df4': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS2~1\\VIB918~1.DTA\",\n",
    "    'df5': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS2~1\\VI4642~1.DTA\",\n",
    "    'df6': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS 2003 All Datasets\\Visit 1 & 2 Combined_Block 5_land owned by the household.dta\",\n",
    "    'df7': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS 2003 All Datasets\\Visit 1 & 2 Combined_Block 6 part 1_buildings and other constructions owned by the household.dta\",\n",
    "    'df8': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2003 nss 59th round\\AIDIS 2003 All Datasets\\Visit 1 & 2 Combined_Block 6 part 2_buildings and other constructions owned by the household.dta\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "eba8509d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 139041\n",
      "Unique HHIDs in df2: 124906\n",
      "Unique HHIDs in df3: 9661\n",
      "Unique HHIDs in df4: 144714\n",
      "Unique HHIDs in df5: 144716\n",
      "Unique HHIDs in df6: 118622\n",
      "Unique HHIDs in df7: 118674\n",
      "Common HHIDs across: 6342\n",
      "df1: 6342 rows after filtering\n",
      "df2: 21266 rows after filtering\n",
      "df3: 13027 rows after filtering\n",
      "df4: 10549 rows after filtering\n",
      "df5: 10549 rows after filtering\n",
      "df6: 21428 rows after filtering\n",
      "df7: 15972 rows after filtering\n",
      "df8: 15884 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Load dataframes\n",
    "dataframes = {name: pd.read_stata(path) for name, path in filepaths.items()}\n",
    "\n",
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs across the first five datasets\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 9)\n",
    "]) #& set.union(*[\n",
    "    #set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(6, 8)\n",
    "#])\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 9):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6, df7, df8 = [dataframes[f'df{i}'] for i in range(1, 9)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "d0609a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_stata(\"df1_2003.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_2003.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_2003.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_2003.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_2003.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_2003.dta\", write_index=False)\n",
    "df7.to_stata(\"df7_2003.dta\", write_index=False)\n",
    "df8.to_stata(\"df8_2003.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "8198a048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'CentreCodeRndShift', 'Vill_Blk_Slno', 'Round', 'Schedule',\n",
       "       'Sample', 'Sector', 'State_Region', 'State', 'District', 'Stratum',\n",
       "       'SubRound', 'SubSample', 'FOD_SubRegion', 'HG_SubBlkNo',\n",
       "       'Second_Stratum', 'Visit_no', 'Hhold_no', 'Level', 'B3_q1', 'B3_q2',\n",
       "       'B3_q3', 'B3_q4', 'HH_Type', 'B3_q5', 'B3_q6', 'B3_q7', 'B3_q8',\n",
       "       'B3_q9', 'B3_q10', 'B3_q11', 'B3_q12', 'B3_q13', 'B3_q14', 'NSS', 'NSC',\n",
       "       'MLT', 'Weight'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "58917b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeping relevant variables and merging\n",
    "\n",
    "# df1\n",
    "df1 = df1[['HHID', 'State', 'B3_q13', 'Weight']].copy()        # MCE\n",
    "\n",
    "# df2\n",
    "df2 = df2[['HHID', 'B13_q1', 'B13_q3','B13_q4', 'B13_q7', 'B13_q8']].copy()     # Fin Assets\n",
    "df2 = df2[df2['B13_q1'].isin([1,2,3,4,5,8,9,10,11])]\n",
    "\n",
    "# df3\n",
    "df3 = df3[['HHID', 'B12_q1', 'B12_q3', 'B12_q6']].copy()      # Shares\n",
    "df3 = df3[df3['B12_q1'] == 8] \n",
    "\n",
    "# df4\n",
    "df4 = df4[['HHID', 'B15_2_q1', 'B15_2_q6']].copy()         # Loan\n",
    "df4 = df4[df4['B15_2_q6'].isin([1,2,3,4,5,6,7,8])]\n",
    "\n",
    "\n",
    "# df5\n",
    "df5 = df5[['HHID', 'B15_2_q1', 'B15_2_q23', 'B15_2_q24']].copy()     # Loan\n",
    "\n",
    "\n",
    "# df6\n",
    "df6 = df6[['HHID', 'B5_q1', 'B5_q5', 'B5_q11']].copy()                  # Land\n",
    "df6 = df6[df6['B5_q1'] == 99]  \n",
    "\n",
    "\n",
    "\n",
    "# df7\n",
    "df7 = df7[['HHID', 'B6_q1', 'B6_q4']].copy()                      # Buildings\n",
    "df7 = df7[df7['B6_q1'] == 11]  \n",
    "\n",
    "# df8\n",
    "df8 = df8[['HHID', 'B6_q1', 'B6_q14']].copy()                        # Buildings\n",
    "df8 = df8[df8['B6_q1'] == 11]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "b27799ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Column  Missing_Values  Zero_Values\n",
      "0    HHID               0            0\n",
      "1   State               0            0\n",
      "2  B3_q13               0            0\n",
      "3  Weight               0            0\n",
      "Unique HHIDs: 6342\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values in df1\n",
    "\n",
    "cols = ['HHID', 'State', 'B3_q13', 'Weight']\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Missing_Values': [df1[col].isna().sum() for col in cols],\n",
    "    'Zero_Values': [(df1[col] == 0).sum() for col in cols]\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(df1['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "e9085e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'B6_q1', 'B6_q4'], dtype='object')"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df7.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "3188f30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming, creating, dropping columns \n",
    "\n",
    "df1  = df1.rename(columns={\n",
    "    'B3_q13': 'MCE'\n",
    "})\n",
    "\n",
    "df2  = df2.rename(columns={\n",
    "    'B13_q1': 'Fin_Asset_Serial',\n",
    "    'B13_q3': 'Fin_Asset_val1_sign',\n",
    "    'B13_q4': 'Fin_Asset_val1_absvalue',\n",
    "    'B13_q7': 'Fin_Asset_val2_sign',\n",
    "    'B13_q8': 'Fin_Asset_val2_absvalue'\n",
    "})\n",
    "\n",
    "df3  = df3.rename(columns={\n",
    "    'B12_q1': 'Share_Asset_serial',\n",
    "    'B12_q3': 'Share_Asset_val1',\n",
    "    'B12_q6': 'Share_Asset_val2'\n",
    "})\n",
    "\n",
    "df4  = df4.rename(columns={\n",
    "    'B15_2_q1': 'Liability_serial',\n",
    "    'B15_2_q6': 'Credit_Agency'\n",
    "})\n",
    "\n",
    "df5  = df5.rename(columns={\n",
    "    'B15_2_q1': 'Liability_serial',\n",
    "    'B15_2_q23': 'Liability_val1',\n",
    "    'B15_2_q24': 'Liability_val2'\n",
    "})\n",
    "\n",
    "df6  = df6.rename(columns={\n",
    "    'B5_q1': 'land_serial',\n",
    "    'B5_q5': 'land_val1',\n",
    "    'B5_q11': 'land_val2'\n",
    "})\n",
    "\n",
    "df7  = df7.rename(columns={\n",
    "    'B6_q1': 'Building_serial',\n",
    "    'B6_q4': 'Building_val1',\n",
    "})\n",
    "\n",
    "df8  = df8.rename(columns={\n",
    "    'B6_q1': 'Building_serial',\n",
    "    'B6_q14': 'Building_val2',\n",
    "})\n",
    "\n",
    "# Creating\n",
    "#df1['MCE'] = df1['MCE']/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ac0ffc41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Liability_serial', 'Credit_Agency'], dtype='object')"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "e83306a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Liability_serial', 'Liability_val1', 'Liability_val2'], dtype='object')"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "06f13898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 (Financial Asset value)\n",
    "\n",
    "# Alternatively, using `.where()` for better Pandas performance:\n",
    "df2['Fin_Asset_val1'] = df2['Fin_Asset_val1_absvalue'].where(df2['Fin_Asset_val1_sign'] != 1, -df2['Fin_Asset_val1_absvalue'])\n",
    "df2['Fin_Asset_val2'] = df2['Fin_Asset_val2_absvalue'].where(df2['Fin_Asset_val2_sign'] != 1, -df2['Fin_Asset_val2_absvalue'])\n",
    "\n",
    "\n",
    "df2['Fin_Asset_Value'] = np.where(\n",
    "    df2['Fin_Asset_Serial'] == 11,\n",
    "    df2['Fin_Asset_val1'],\n",
    "    df2['Fin_Asset_val1'].combine_first(df2['Fin_Asset_val2'])\n",
    ")\n",
    "df2['Fin_Asset_Value'] = pd.Series(df2['Fin_Asset_Value']).fillna(0)\n",
    "\n",
    "\n",
    "# df3 (Share Asset value)\n",
    "\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_val1'].combine_first(df3['Share_Asset_val2'])\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_Value'].fillna(0)\n",
    "df3 = df3.reset_index(drop=True)\n",
    "\n",
    "# Share_Asset_Map\n",
    "\n",
    "# Mapping dictionary using float keys (since your column is float64)\n",
    "Share_Asset_Map = {\n",
    "    8.0: 'Shares and Debentures'\n",
    "}\n",
    "\n",
    "# df6 (Land value)\n",
    "\n",
    "df6['Land_value'] = df6['land_val1'].combine_first(df6['land_val2'])\n",
    "df6['Land_value'] = df6['Land_value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "5fe57048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Fin_Asset_Serial\n",
       "11    6088\n",
       "8     1842\n",
       "4     1545\n",
       "10    1211\n",
       "3     1068\n",
       "5      515\n",
       "2      500\n",
       "1      324\n",
       "9       23\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2['Fin_Asset_Serial'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "447da4fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin_Asset_Serial\n",
      "11    6088\n",
      "8     1842\n",
      "4     1545\n",
      "10    1211\n",
      "3     1068\n",
      "5      515\n",
      "2      500\n",
      "1      324\n",
      "9       23\n",
      "Name: count, dtype: int64\n",
      "         HHID  Fin_Asset_Serial  Fin_Asset_val1_sign  Fin_Asset_val1_absvalue  \\\n",
      "0  1100011102                11                  NaN                    400.0   \n",
      "1  1100011102               999                  NaN                      NaN   \n",
      "2  1100115102                 4                  NaN                    500.0   \n",
      "3  1100115102                 8                  NaN                  36000.0   \n",
      "4  1100115102                10                  NaN                  48000.0   \n",
      "\n",
      "   Fin_Asset_val2_sign  Fin_Asset_val2_absvalue  Fin_Asset_val1  \\\n",
      "0                  NaN                      NaN           400.0   \n",
      "1                  NaN                      NaN             NaN   \n",
      "2                  NaN                    500.0           500.0   \n",
      "3                  NaN                  34800.0         36000.0   \n",
      "4                  NaN                  44000.0         48000.0   \n",
      "\n",
      "   Fin_Asset_val2  Fin_Asset_Value  \n",
      "0             NaN            400.0  \n",
      "1             NaN            400.0  \n",
      "2           500.0            500.0  \n",
      "3         34800.0          36000.0  \n",
      "4         44000.0          48000.0  \n",
      "6316\n"
     ]
    }
   ],
   "source": [
    "print(df2['Fin_Asset_Serial'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Fin_Asset_Value\n",
    "summary_rows = df2.groupby('HHID', as_index=False)['Fin_Asset_Value'].sum()\n",
    "summary_rows['Fin_Asset_Serial'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df2.columns:\n",
    "    if col not in ['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value']:\n",
    "        summary_rows[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df2_mod = pd.concat([df2, summary_rows], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df2_mod = df2_mod.sort_values(by=['HHID', 'Fin_Asset_Serial']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "print(df2_mod.head())\n",
    "print(df2_mod['HHID'].nunique())\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts(dropna=False)\n",
    "\n",
    "# Modify serials and labels, if any\n",
    "\n",
    "# df2_mod\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([11]), 'Fin_Asset_Serial'] = 31\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([3,4,5]), 'Fin_Asset_Serial'] = 32\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([8]), 'Fin_Asset_Serial'] = 34\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([9,10]), 'Fin_Asset_Serial'] = 35\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([1,2]), 'Fin_Asset_Serial'] = 36\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([999]), 'Fin_Asset_Serial'] = 37\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "# Fin Asset Mapping\n",
    "\n",
    "Fin_Asset_map = {\n",
    "\n",
    "    31.0: 'Currency',\n",
    "    32.0: 'Deposits',\n",
    "    34.0: 'Life Insurance fund',\n",
    "    35.0: 'Provident and Pension fund',\n",
    "    36.0: 'Claims on Government',\n",
    "    37.0: 'Total financial assets (other thanshares and related instruments)'\n",
    "}\n",
    "\n",
    "df2_mod = df2_mod.groupby(['HHID', 'Fin_Asset_Serial'], as_index=False).agg({\n",
    "    'Fin_Asset_Value': 'sum'  \n",
    "})\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts()\n",
    "\n",
    "df2 = df2_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "30df9848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>Liability_serial</th>\n",
       "      <th>Credit_Agency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2409315101</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2403015101</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2490715102</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2499025101</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2497013101</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          HHID  Liability_serial  Credit_Agency\n",
       "3   2409315101                 1            2.0\n",
       "5   2403015101                 1            2.0\n",
       "9   2490715102                 1            3.0\n",
       "10  2499025101                 1            3.0\n",
       "12  2497013101                 1            6.0"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7ae0e47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HHID  Liability_serial  Credit_Agency  Liability_val1  \\\n",
      "0  1100115102               3.0            2.0         65000.0   \n",
      "1  1100115102               2.0            3.0         80400.0   \n",
      "2  1100115102               4.0            3.0             NaN   \n",
      "3  1100115102               5.0            3.0             NaN   \n",
      "4  1100115102               1.0            5.0          6600.0   \n",
      "\n",
      "   Liability_val2  Liability_value  \n",
      "0         69000.0          65000.0  \n",
      "1             NaN          80400.0  \n",
      "2         20900.0          20900.0  \n",
      "3         22350.0          22350.0  \n",
      "4          9000.0           6600.0  \n",
      "5435\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Credit_Agency\n",
       "999.0    5435\n",
       "91.0     5040\n",
       "92.0      683\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df4 and df5\n",
    "\n",
    "# Merging both\n",
    "\n",
    "# df4 and df5\n",
    "mergedd_loan = pd.merge(df4, df5, on=['HHID', 'Liability_serial'], how='inner')\n",
    "\n",
    "# Step 1: Compute Liability_value\n",
    "mergedd_loan['Liability_value'] = mergedd_loan['Liability_val1'].combine_first(mergedd_loan['Liability_val2'])\n",
    "mergedd_loan['Liability_value'] = mergedd_loan['Liability_value'].fillna(0)\n",
    "\n",
    "# Step 2: Group by HHID and sum Liability_value\n",
    "summary_rows_liab = mergedd_loan.groupby('HHID', as_index=False)['Liability_value'].sum()\n",
    "summary_rows_liab['Credit_Agency'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in mergedd_loan.columns:\n",
    "    if col not in ['HHID', 'Credit_Agency', 'Liability_value']:\n",
    "        summary_rows_liab[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "merged_loan = pd.concat([mergedd_loan, summary_rows_liab], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "merged_loan = merged_loan.sort_values(by=['HHID', 'Credit_Agency']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "merged_loan.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "print(merged_loan.head())\n",
    "print(merged_loan['HHID'].nunique())\n",
    "\n",
    "# merged_loan\n",
    "merged_loan.loc[merged_loan['Credit_Agency'].isin([2,3]), 'Credit_Agency'] = 91\n",
    "merged_loan.loc[merged_loan['Credit_Agency'].isin([1,4,5,6,7,8]), 'Credit_Agency'] = 92\n",
    "\n",
    "merged_loan = merged_loan.reset_index(drop=True)\n",
    "\n",
    "# Liability Map\n",
    "\n",
    "Credit_Agency_Map = {\n",
    "\n",
    "    91.0: 'Bank advances',\n",
    "    92.0: 'Non-banking loans and advances',\n",
    "    999.0: 'Total Institutional Liabilities'\n",
    "}\n",
    "\n",
    "merged_loan = merged_loan.groupby(['HHID', 'Credit_Agency'], as_index=False).agg({\n",
    "    'Liability_value': 'sum'  \n",
    "})\n",
    "\n",
    "merged_loan = merged_loan.reset_index(drop=True)\n",
    "\n",
    "merged_loan.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "merged_loan['Credit_Agency'].value_counts()\n",
    "\n",
    "#df4 = merged_loan.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "b08f801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df7 and df8\n",
    "\n",
    "# Outer merge df7 and df8 on 'HHID', 'Building_serial'\n",
    "\n",
    "merged_building = pd.merge(df7, df8, on=['HHID', 'Building_serial'], how='outer')\n",
    "\n",
    "merged_building['Building_value'] = merged_building['Building_val1'].combine_first(merged_building['Building_val2'])\n",
    "merged_building['Building_value'] = merged_building['Building_value'].fillna(0)\n",
    "\n",
    "merged_building.to_stata(\"mergedforu2.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f44fcc50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "\n",
    "merged_building.drop(['Building_serial', 'Building_val1', 'Building_val2'], axis=1, inplace=True)\n",
    "#merged_loan.drop(['Liability_serial', 'Liability_val1','Liability_val2'], axis=1, inplace=True)\n",
    "#df2.drop(['Fin_Asset_val1_sign',\n",
    "#        'Fin_Asset_val1_absvalue', 'Fin_Asset_val2_sign',\n",
    "#        'Fin_Asset_val2_absvalue', 'Fin_Asset_val1', 'Fin_Asset_val2'], axis=1, inplace=True)\n",
    "df3.drop(['Share_Asset_val1', 'Share_Asset_val2'], axis=1, inplace=True)\n",
    "df6.drop(['land_val1', 'land_val2', 'land_serial'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Viewing before merging\n",
    "\n",
    "df1.to_stata(\"df1_2003.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_2003.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_2003.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_2003.dta\", write_index=False)\n",
    "merged_building.to_stata(\"merged_building_2003.dta\", write_index=False)\n",
    "merged_loan.to_stata(\"merged_loan_2003.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "4f10e7c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value'], dtype='object')"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "911d0182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groupby\n",
    "\n",
    "df2 = df2.groupby(['HHID', 'Fin_Asset_Serial'], as_index=False).agg({\n",
    "    'Fin_Asset_Value': 'sum'  \n",
    "})\n",
    "\n",
    "df3 = df3.groupby(['HHID', 'Share_Asset_serial'], as_index=False).agg({\n",
    "    'Share_Asset_Value': 'sum'  \n",
    "})\n",
    "\n",
    "merged_loan = merged_loan.groupby(['HHID', 'Credit_Agency'], as_index=False).agg({\n",
    "    'Liability_value': 'sum'  \n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "7c80c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['df1'] = df1\n",
    "dataframes['df2'] = df2\n",
    "dataframes['df3'] = df3\n",
    "dataframes['df4'] = merged_loan\n",
    "dataframes['df5'] = merged_building\n",
    "dataframes['df6'] = df6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "89a5fa98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 6342\n",
      "Unique HHIDs in df2: 6316\n",
      "Unique HHIDs in df3: 6338\n",
      "Unique HHIDs in df4: 5435\n",
      "Unique HHIDs in df5: 6340\n",
      "Unique HHIDs in df6: 6341\n",
      "Common HHIDs across: 5406\n",
      "df1: 5406 rows after filtering\n",
      "df2: 16295 rows after filtering\n",
      "df3: 5406 rows after filtering\n",
      "df4: 11096 rows after filtering\n",
      "df5: 5406 rows after filtering\n",
      "df6: 5406 rows after filtering\n",
      "df7: 13579 rows after filtering\n",
      "df8: 13503 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 7):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs: intersection of first five & union of df6, df7\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 7)\n",
    "])\n",
    "\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 7):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6 = [dataframes[f'df{i}'] for i in range(1, 7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e5ae6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs: 5406\n",
      "Number of duplicates: 0\n",
      "Unique HHIDs with Fin_Asset_Serial = 37: 5406\n",
      "Unique HHIDs with Share_Asset_serial = 8: 5406\n",
      "Unique HHIDs with Credit_Agency = 999: 5406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\2437617522.py:21: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\2437617522.py:30: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged1.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Merging all datasets\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6]\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='HHID', how='outer'), dfs)\n",
    "\n",
    "merged_df.to_stata(\"merged.dta\", write_index=False)\n",
    "\n",
    "unique_hhids = set(merged_df['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "\n",
    "# Land value\n",
    "#merged_df['Land_value'] = merged_df[['Urban_land_value', 'Rural_land_value']].sum(axis=1, min_count=1).fillna(0)\n",
    "\n",
    "# Real Estate Value\n",
    "\n",
    "merged_df['Real Estate'] = merged_df['Building_value'] + merged_df['Land_value']\n",
    "\n",
    "merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1 = merged_df.copy()\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "merged_df1.drop(['Building_value','Land_value'\n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "merged_df1.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "# Check for duplicates\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "dup_check = merged_df1.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Fin_Asset_Serial'] == 37, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Fin_Asset_Serial = 37: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Share_Asset_serial'] == 8, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Share_Asset_serial = 8: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Credit_Agency'] == 999, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Credit_Agency = 999: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "d11e5d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Column  Missing_Values  Zero_Values\n",
      "0                HHID               0            0\n",
      "1               State               0            0\n",
      "2              Weight               0            0\n",
      "3    Fin_Asset_Serial               0            0\n",
      "4  Share_Asset_serial               0            0\n",
      "5       Credit_Agency               0            0\n",
      "6   Share_Asset_Value               0           14\n",
      "7     Fin_Asset_Value               0           19\n",
      "8     Liability_value               0          329\n",
      "9         Real Estate               0            0\n",
      "Unique HHIDs: 5406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\3797293583.py:20: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "cols = ['HHID', 'State','Weight', 'Fin_Asset_Serial', 'Share_Asset_serial',\n",
    "        'Credit_Agency', 'Share_Asset_Value', 'Fin_Asset_Value',\n",
    "        'Liability_value', 'Real Estate']\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Missing_Values': [merged_df1[col].isna().sum() for col in cols],\n",
    "    'Zero_Values': [(merged_df1[col] == 0).sum() for col in cols]\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(merged_df1['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "cbf1f9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\2063279316.py:24: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  filtered_df.to_stata(\"final(1)_13.dta\", write_index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs present in all three groups: 5406\n"
     ]
    }
   ],
   "source": [
    "merged_df3 = merged_df1.copy()\n",
    "\n",
    "# Create the new column\n",
    "merged_df3['Fin_Asset'] = merged_df3['Fin_Asset_Serial'].map(Fin_Asset_map)\n",
    "merged_df3['Share_Asset'] = merged_df3['Share_Asset_serial'].map(Share_Asset_Map)\n",
    "merged_df3['Liability'] = merged_df3['Credit_Agency'].map(Credit_Agency_Map)\n",
    "\n",
    "# HHIDs satisfying each condition\n",
    "hhid_fin = set(merged_df3.loc[merged_df3['Fin_Asset_Serial'] == 37, 'HHID'])\n",
    "hhid_share = set(merged_df3.loc[merged_df3['Share_Asset_serial'] == 8, 'HHID'])\n",
    "hhid_credit = set(merged_df3.loc[merged_df3['Credit_Agency'] == 999, 'HHID'])\n",
    "\n",
    "# Intersection: HHIDs present in all three\n",
    "common_hhids = hhid_fin & hhid_share & hhid_credit\n",
    "\n",
    "# Keep only those rows\n",
    "filtered_df = merged_df3[merged_df3['HHID'].isin(common_hhids)].copy()\n",
    "\n",
    "# Print how many unique HHIDs remain\n",
    "print(f\"Unique HHIDs present in all three groups: {len(common_hhids)}\")\n",
    "\n",
    "#filtered_df.drop(['Fin_Asset', 'Share_Asset', 'Liability'], axis=1, inplace=True)\n",
    "\n",
    "filtered_df.to_stata(\"final(1)_13.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "618a77a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 32, 34, 35, 36, 37]\n",
      "[8]\n",
      "[91.0, 92.0, 999.0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks if grouping worked correctly\n",
    "\n",
    "print(sorted(filtered_df['Fin_Asset_Serial'].unique()))\n",
    "#print(filtered_df['Fin_Asset_Serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Share_Asset_serial'].unique()))\n",
    "#print(filtered_df['Share_Asset_serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Credit_Agency'].unique()))\n",
    "#print(filtered_df['Credit_Agency'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "e7448075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          HHID  Gross wealth\n",
      "0   1100115102      554900.0\n",
      "15  1100316101      262341.0\n",
      "21  1100321101      196542.0\n",
      "31  1100411102      180566.0\n",
      "37  1100412101      430280.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\3773941852.py:33: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  df_grouped1.to_stata(\"final(25).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "df_grouped1 = filtered_df.copy()\n",
    "\n",
    "# Step 1: Filter the relevant rows satisfying your condition\n",
    "wealth_rows = df_grouped1[\n",
    "    (df_grouped1['Credit_Agency'] == 999) &\n",
    "    (df_grouped1['Fin_Asset_Serial'] == 37) &\n",
    "    (df_grouped1['Share_Asset_serial'] == 8)\n",
    "].copy()\n",
    "\n",
    "# Step 2: Compute wealth for these rows\n",
    "wealth_rows['Gross wealth'] = (\n",
    "    wealth_rows['Fin_Asset_Value'] +\n",
    "    wealth_rows['Share_Asset_Value'] +\n",
    "    wealth_rows['Real Estate']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Financial Assets'] = (\n",
    "    wealth_rows['Fin_Asset_Value']+\n",
    "    wealth_rows['Share_Asset_Value']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Institutional liabilities'] = wealth_rows['Liability_value']\n",
    "\n",
    "# Step 3: Keep only HHID and wealth columns\n",
    "wealth_per_hhid = wealth_rows[['HHID', 'Gross wealth', 'Total Financial Assets', 'Total Institutional liabilities']]\n",
    "\n",
    "# Step 4: Merge wealth back to the full dataset based on HHID\n",
    "df_grouped1 = df_grouped1.merge(wealth_per_hhid, on='HHID', how='left')\n",
    "\n",
    "# Step 5: View result\n",
    "print(df_grouped1[['HHID', 'Gross wealth']].drop_duplicates().head())\n",
    "\n",
    "df_grouped1.to_stata(\"final(25).dta\", write_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "6aff3cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'Weight', 'Fin_Asset_Serial', 'Fin_Asset_Value',\n",
       "       'Share_Asset_serial', 'Share_Asset_Value', 'Credit_Agency',\n",
       "       'Liability_value', 'Real Estate', 'Fin_Asset', 'Share_Asset',\n",
       "       'Liability', 'Gross wealth', 'Total Financial Assets',\n",
       "       'Total Institutional liabilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c21816b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\1759378772.py:17: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HHID  Credit_Agency  Fin_Asset_Serial  Share_Asset_serial  \\\n",
      "0  1100115102           91.0                31                   8   \n",
      "1  1100115102           91.0                31                   8   \n",
      "2  1100115102           91.0                31                   8   \n",
      "3  1100115102           91.0                31                   8   \n",
      "4  1100115102           91.0                31                   8   \n",
      "\n",
      "   Asset_serial  Asset_Value  State     MCE   Weight  Fin_Asset_Value  \\\n",
      "0            31        400.0     33  4500.0  3.85125            400.0   \n",
      "1           208      20000.0     33  4500.0  3.85125            400.0   \n",
      "2           300     450000.0     33  4500.0  3.85125            400.0   \n",
      "3          1000     554900.0     33  4500.0  3.85125            400.0   \n",
      "4          1500     104900.0     33  4500.0  3.85125            400.0   \n",
      "\n",
      "   Share_Asset_Value  Liability_value  Real Estate Fin_Asset  \\\n",
      "0            20000.0         188650.0     450000.0  Currency   \n",
      "1            20000.0         188650.0     450000.0  Currency   \n",
      "2            20000.0         188650.0     450000.0  Currency   \n",
      "3            20000.0         188650.0     450000.0  Currency   \n",
      "4            20000.0         188650.0     450000.0  Currency   \n",
      "\n",
      "             Share_Asset      Liability  Gross wealth  Total Financial Assets  \\\n",
      "0  Shares and Debentures  Bank advances      554900.0                104900.0   \n",
      "1  Shares and Debentures  Bank advances      554900.0                104900.0   \n",
      "2  Shares and Debentures  Bank advances      554900.0                104900.0   \n",
      "3  Shares and Debentures  Bank advances      554900.0                104900.0   \n",
      "4  Shares and Debentures  Bank advances      554900.0                104900.0   \n",
      "\n",
      "   Total Institutional liabilities              Asset_Name  \n",
      "0                         195250.0                Currency  \n",
      "1                         195250.0   Shares and Debentures  \n",
      "2                         195250.0             Real Estate  \n",
      "3                         195250.0            Gross wealth  \n",
      "4                         195250.0  Total Financial Assets  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\1759378772.py:88: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  asset_df.to_stata(\"final(28).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "df_grouped2 =  df_grouped1.copy()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize columns with NaN\n",
    "df_grouped2['Asset_serial'] = np.nan\n",
    "df_grouped2['Asset_Value'] = np.nan\n",
    "df_grouped2['Asset_Name'] = np.nan\n",
    "\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "\n",
    "df_grouped2['HHID'] = df_grouped2['HHID'].astype(str)\n",
    "\n",
    "#df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n",
    "\n",
    "\n",
    "df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n",
    "\n",
    "\n",
    "# Empty list to store new rows\n",
    "rows = []\n",
    "\n",
    "# Loop over each unique group\n",
    "for _, group in df_grouped2.groupby(group_cols):\n",
    "\n",
    "    # Get first row for copying other columns\n",
    "    first_row = group.iloc[0].to_dict()\n",
    "\n",
    "    # 1. Financial Asset Row\n",
    "    if not np.isnan(first_row['Fin_Asset_Value']):\n",
    "        fa_row = first_row.copy()\n",
    "        fa_row['Asset_serial'] = first_row['Fin_Asset_Serial']\n",
    "        fa_row['Asset_Value'] = first_row['Fin_Asset_Value']\n",
    "        fa_row['Asset_Name'] = first_row['Fin_Asset']\n",
    "        rows.append(fa_row)\n",
    "\n",
    "    # 2. Share Asset Row\n",
    "    if not np.isnan(first_row['Share_Asset_Value']):\n",
    "        sa_row = first_row.copy()\n",
    "        sa_row['Asset_serial'] = 200 + first_row['Share_Asset_serial']  # As you specified\n",
    "        sa_row['Asset_Value'] = first_row['Share_Asset_Value']\n",
    "        sa_row['Asset_Name'] = first_row['Share_Asset']\n",
    "        rows.append(sa_row)\n",
    "\n",
    "    # 3. Real Estate Row\n",
    "    if not np.isnan(first_row['Real Estate']):\n",
    "        re_row = first_row.copy()\n",
    "        re_row['Asset_serial'] = 300\n",
    "        re_row['Asset_Value'] = first_row['Real Estate']\n",
    "        re_row['Asset_Name'] = 'Real Estate'\n",
    "        rows.append(re_row)\n",
    "\n",
    "    # 4. Gross wealth Row\n",
    "    if not np.isnan(first_row['Gross wealth']):\n",
    "        w_row = first_row.copy()\n",
    "        w_row['Asset_serial'] = 1000\n",
    "        w_row['Asset_Value'] = first_row['Gross wealth']\n",
    "        w_row['Asset_Name'] = 'Gross wealth'\n",
    "        rows.append(w_row)\n",
    "\n",
    "    # 6. Total Financial Assets Row\n",
    "    if not np.isnan(first_row['Total Financial Assets']):\n",
    "        tfa_row = first_row.copy()\n",
    "        tfa_row['Asset_serial'] = 1500\n",
    "        tfa_row['Asset_Value'] = first_row['Total Financial Assets']\n",
    "        tfa_row['Asset_Name'] = 'Total Financial Assets'\n",
    "        rows.append(tfa_row)   \n",
    "\n",
    "# Convert to DataFrame\n",
    "asset_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: Arrange columns nicely\n",
    "cols_order = group_cols + ['Asset_serial', 'Asset_Value'] + [col for col in df_grouped2.columns if col not in group_cols + ['Asset_serial', 'Asset_Value']]\n",
    "asset_df = asset_df[cols_order]\n",
    "\n",
    "# Check result\n",
    "print(asset_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Liabilities\n",
    "\n",
    "asset_df = asset_df.rename(columns={\n",
    "    'Credit_Agency': 'Liability_serial',\n",
    "    'Liability': 'Liability_name'\n",
    "})\n",
    "\n",
    "asset_df.to_stata(\"final(28).dta\", write_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "22efcfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subsets: 33670\n",
      "Number of duplicates: 134680\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "group_cols = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "group_cols1 = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial', 'Asset_serial']\n",
    "\n",
    "# Count unique combinations\n",
    "num_subsets = asset_df[group_cols].drop_duplicates().shape[0]\n",
    "\n",
    "print(f\"Number of unique subsets: {num_subsets}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols1)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "69c3cc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset_serial    0\n",
      "Asset_Value     0\n",
      "Asset_Name      0\n",
      "dtype: int64\n",
      "\n",
      "Total rows with missing values: 0\n",
      "Empty DataFrame\n",
      "Columns: [HHID, Liability_serial, Fin_Asset_Serial, Share_Asset_serial, Asset_serial, Asset_Value, State, MCE, Weight, Fin_Asset_Value, Share_Asset_Value, Liability_value, Real Estate, Fin_Asset, Share_Asset, Liability_name, Gross wealth, Total Financial Assets, Total Institutional liabilities, Asset_Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "# Check how many missing in each column\n",
    "print(asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().sum())\n",
    "\n",
    "# Optionally, view rows with missing in any of them\n",
    "missing_rows = asset_df[\n",
    "    asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().any(axis=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal rows with missing values: {len(missing_rows)}\")\n",
    "print(missing_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "5f5a73d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\3734631850.py:11: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\3734631850.py:19: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\3734631850.py:28: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df1.to_stata(\"final(2013)_1.dta\", write_index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5406"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = asset_df.copy()\n",
    "\n",
    "final_df.drop(['Fin_Asset_Serial', \n",
    "               'Share_Asset_serial', \n",
    "               'Fin_Asset_Value',\n",
    "                'Share_Asset_Value', \n",
    "                'Fin_Asset',\n",
    "                'Share_Asset',               \n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
    "\n",
    "final_df = final_df[['HHID', 'State' ,'MCE', 'Weight','Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value',\n",
    "                    'Real Estate', 'Total Financial Assets', 'Total Institutional liabilities', \n",
    "                    ]].copy()\n",
    "\n",
    "final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
    "\n",
    "final_df['HHID'].nunique()\n",
    "\n",
    "final_df1 = final_df[['HHID', 'State', 'MCE', 'Weight', 'Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value'\n",
    "                    ]].copy()\n",
    "\n",
    "final_df1.to_stata(\"final(2013)_1.dta\", write_index=False)\n",
    "final_df1['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9f1b225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All HHIDs are unique within each Asset_serial category.\n",
      " All HHIDs are unique within each Liability_serial category.\n"
     ]
    }
   ],
   "source": [
    "group_cols1 = ['HHID', 'Liability_serial', 'Asset_serial']\n",
    "\n",
    "final_df2 = final_df1.drop_duplicates(subset=group_cols1, keep='first').reset_index(drop=True)\n",
    "\n",
    "# Check for repetition\n",
    "\n",
    "# Check for uniqueness \n",
    "\n",
    "duplicates = final_df2.duplicated(subset=['Asset_serial', 'HHID', 'Liability_serial'], keep=False)\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Asset_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Asset_serial', 'HHID']].sort_values(['Asset_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Asset_serial category.\")\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Liability_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Liability_serial', 'HHID']].sort_values(['Liability_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Liability_serial category.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "16bd22df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5406"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d2dc89",
   "metadata": {},
   "source": [
    "# Add State names corresponding to their codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "082fc234",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_1412\\1591137529.py:48: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df3.to_stata('2003_final.dta', write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# final_df2 (Merge D&D, D&N)\n",
    "final_df2.loc[final_df2['State'].isin([25,26]), 'State'] = 25\n",
    "final_df2['State'].value_counts()\n",
    "\n",
    "# State Map\n",
    "\n",
    "State_map = {\n",
    "    1.0: 'Jammu and Kashmir',\n",
    "    2.0: 'Himachal Pradesh',\n",
    "    3.0: 'Punjab',\n",
    "    4.0: 'Chandigarh',\n",
    "    5.0: 'Uttarakhand',\n",
    "    6.0: 'Haryana',\n",
    "    7.0: 'Delhi',\n",
    "    8.0: 'Rajasthan',\n",
    "    9.0: 'Uttar Pradesh',\n",
    "    10.0: 'Bihar',\n",
    "    11.0: 'Sikkim',\n",
    "    12.0: 'Arunachal Pradesh',\n",
    "    13.0: 'Nagaland',\n",
    "    14.0: 'Manipur',\n",
    "    15.0: 'Mizoram',\n",
    "    16.0: 'Tripura',\n",
    "    17.0: 'Meghalaya',\n",
    "    18.0: 'Assam',\n",
    "    19.0: 'West Bengal',\n",
    "    20.0: 'Jharkhand',\n",
    "    21.0: 'Odisha',\n",
    "    22.0: 'Chhattishgarh',\n",
    "    23.0: 'Madhya Pradesh',\n",
    "    24.0: 'Gujarat',\n",
    "    25.0: 'Daman and Diu and Dadra and Nagar Haveli',\n",
    "    27.0: 'Maharashtra',\n",
    "    28.0: 'Andhra Pradesh',\n",
    "    29.0: 'Karnataka',\n",
    "    30.0: 'Goa',\n",
    "    31.0: 'Lakshadweep',\n",
    "    32.0: 'Kerala',\n",
    "    33.0: 'Tamilnadu',\n",
    "    34.0: 'Puducherry',\n",
    "    35.0: 'Andaman & Nicobar'\n",
    "}\n",
    "\n",
    "final_df3 = final_df2.copy()\n",
    "\n",
    "final_df3['State_name'] = final_df3['State'].map(State_map)\n",
    "\n",
    "final_df3.to_stata('2003_final.dta', write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
