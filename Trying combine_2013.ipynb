{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "8163f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "os.chdir(r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\2013\")\n",
    "os.getcwd()\n",
    "\n",
    "# Filepaths for the datasets\n",
    "filepaths = {\n",
    "    'df1': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 3_Household Characteristics.dta\",\n",
    "    'df2': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 12_Financial assets other than shares and debentures owned.dta\",\n",
    "    'df3': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 11_ Shares and debentures owned in co-operative societies & companies.dta\",\n",
    "    'df4': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 14_particulars of cash loans payable to institutional, non-institutional agencies.dta\",\n",
    "    'df5': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 6_Buildings and other constructions owned by the household as on 30.06.2012.dta\",\n",
    "    'df6': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 5pt2_Details of land owned by the household as on 30.06.12.dta\",\n",
    "    'df7': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2013 nss 70th round\\AIDIS 2013 Visit 1&2 dataset\\Visit 1_Block 5pt1_Details of land owned by the household as on 30.06.12.dta\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "3e22fc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 110800\n",
      "Unique HHIDs in df2: 101412\n",
      "Unique HHIDs in df3: 3836\n",
      "Unique HHIDs in df4: 73720\n",
      "Unique HHIDs in df5: 95488\n",
      "Unique HHIDs in df6: 34217\n",
      "Unique HHIDs in df7: 67747\n",
      "Common HHIDs across: 3000\n",
      "df1: 3000 rows after filtering\n",
      "df2: 13313 rows after filtering\n",
      "df3: 6085 rows after filtering\n",
      "df4: 9166 rows after filtering\n",
      "df5: 8636 rows after filtering\n",
      "df6: 2764 rows after filtering\n",
      "df7: 8251 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Load dataframes\n",
    "dataframes = {name: pd.read_stata(path) for name, path in filepaths.items()}\n",
    "\n",
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs: intersection of first five & union of df6, df7\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 6)\n",
    "]) & set.union(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(6, 8)\n",
    "])\n",
    "\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6, df7 = [dataframes[f'df{i}'] for i in range(1, 8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "dc58977d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'CentreCodeRndShift', 'Vill_Blk_Slno', 'Round', 'Schedule',\n",
       "       'Sample', 'Sector', 'State_Region', 'State', 'District_id',\n",
       "       'State_District', 'Stratum', 'SubStratumNo', 'SubRound', 'SubSample',\n",
       "       'FODSubRegion', 'HG_SubBlkNo', 'Second_Stratum', 'Hhold_no', 'Visit_no',\n",
       "       'Level', 'b3q1', 'HH_type', 'b3q3', 'b3q4', 'b3q5', 'b3q6', 'b3q7',\n",
       "       'b3q8', 'b3q9', 'b3q10', 'b3q11', 'b3q12', 'b3q13', 'b3q14', 'NSS',\n",
       "       'NSC', 'MLT', 'Weight_SS', 'Weight_SC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3ca0ff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1: Keep relevant columns\n",
    "df1 = df1[['HHID', 'State', 'Weight_SC']].copy()\n",
    "\n",
    "# df2: Keep all rows for HHIDs that have at least one row with b12_q1 == 11\n",
    "df2 = df2[['HHID', 'b12_q1', 'b12_q3']].copy()\n",
    "#hhids_with_11 = df2.loc[df2['b12_q1'] == 11, 'HHID'].unique()\n",
    "#df2 = df2[df2['HHID'].isin(hhids_with_11)].copy()\n",
    "df2 = df2[df2['b12_q1'].isin([1,2,3,4,5,6,7])] \n",
    "\n",
    "# df3: Keep relevant columns\n",
    "df3 = df3[['HHID', 'b11_q1', 'b11_q3', 'b11_q6']].copy()\n",
    "df3 = df3[df3['b11_q1'] == 5] \n",
    "\n",
    "# df4: Keep relevant columns\n",
    "df4 = df4[['HHID', 'b14_q1', 'b14_q6', 'b14_q16', 'b14_q17']].copy()\n",
    "#hhids_with_99 = df4.loc[df4['b14_q1'] == 99, 'HHID'].unique()\n",
    "#df4 = df4[df4['HHID'].isin(hhids_with_99)].copy()\n",
    "df4 = df4[df4['b14_q6'].isin([1,2,3,4,5,6,7,11])]\n",
    "\n",
    "\n",
    "# df5: Keep only rows where b6_q3 == 11\n",
    "df5 = df5[['HHID', 'b6_q3', 'b6_q6']].copy()\n",
    "df5 = df5[df5['b6_q3'] == 11].copy()\n",
    "\n",
    "# df6: Keep only rows where b5_2_1 == 99\n",
    "df6 = df6[['HHID', 'b5_2_1', 'b5_2_6']].copy()\n",
    "df6 = df6[df6['b5_2_1'] == 99].copy()\n",
    "\n",
    "# df7: Keep only rows where b5_1_1 == 99\n",
    "df7 = df7[['HHID', 'b5_1_1', 'b5_1_6']].copy()\n",
    "df7 = df7[df7['b5_1_1'] == 99].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "8ac0f7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming, creating, dropping columns \n",
    "\n",
    "df1 = df1.rename(columns={\n",
    "    'Weight_SC': 'Weight'\n",
    "})\n",
    "\n",
    "df2  = df2.rename(columns={\n",
    "    'b12_q1': 'Fin_Asset_Serial',\n",
    "    'b12_q3': 'Fin_Asset_Value'\n",
    "})\n",
    "\n",
    "df3  = df3.rename(columns={\n",
    "    'b11_q1': 'Share_Asset_serial',\n",
    "    'b11_q3': 'Share_Asset_val1',\n",
    "    'b11_q6': 'Share_Asset_val2'\n",
    "})\n",
    "\n",
    "df4  = df4.rename(columns={\n",
    "    'b14_q1': 'Liability_serial',\n",
    "    'b14_q6': 'Credit_Agency',\n",
    "    'b14_q16': 'Liability_val1',\n",
    "    'b14_q17': 'Liability_val2'\n",
    "})\n",
    "\n",
    "df5  = df5.rename(columns={\n",
    "    'b6_q3': 'Building_serial',\n",
    "    'b6_q6': 'Building_value'\n",
    "})\n",
    "\n",
    "df6  = df6.rename(columns={\n",
    "    'b5_2_1': 'Urban_land_serial',\n",
    "    'b5_2_6': 'Urban_land_value'\n",
    "})\n",
    "\n",
    "df7  = df7.rename(columns={\n",
    "    'b5_1_1': 'Rural_land_serial',\n",
    "    'b5_1_6': 'Rural_land_value',\n",
    "})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "7fbff0b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value'], dtype='object')"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "dfebe040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 (Financial Asset value)\n",
    "df2['Fin_Asset_Value'] = df2['Fin_Asset_Value'].fillna(0)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df3 (Share Asset value)\n",
    "\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_val1'].combine_first(df3['Share_Asset_val2'])\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_Value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "92a4e9d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>Share_Asset_serial</th>\n",
       "      <th>Share_Asset_val1</th>\n",
       "      <th>Share_Asset_val2</th>\n",
       "      <th>Share_Asset_Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>247332101</td>\n",
       "      <td>5</td>\n",
       "      <td>700000.0</td>\n",
       "      <td>700000.0</td>\n",
       "      <td>700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224701102</td>\n",
       "      <td>5</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "      <td>160000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>224811101</td>\n",
       "      <td>5</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "      <td>17000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>221171103</td>\n",
       "      <td>5</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>100000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>221531101</td>\n",
       "      <td>5</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        HHID  Share_Asset_serial  Share_Asset_val1  Share_Asset_val2  \\\n",
       "1  247332101                   5          700000.0          700000.0   \n",
       "2  224701102                   5          160000.0          160000.0   \n",
       "4  224811101                   5           17000.0           17000.0   \n",
       "7  221171103                   5          100000.0          100000.0   \n",
       "8  221531101                   5             400.0             400.0   \n",
       "\n",
       "   Share_Asset_Value  \n",
       "1           700000.0  \n",
       "2           160000.0  \n",
       "4            17000.0  \n",
       "7           100000.0  \n",
       "8              400.0  "
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "b7d39f86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Credit_Agency\n",
       "2.0     2673\n",
       "3.0     1091\n",
       "11.0     107\n",
       "7.0       88\n",
       "1.0       68\n",
       "6.0       56\n",
       "4.0       33\n",
       "5.0        7\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4['Credit_Agency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "24289958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df4 (Liability value)\n",
    "\n",
    "df4['Liability_value'] = df4['Liability_val1'].combine_first(df4['Liability_val2'])\n",
    "df4['Liability_value'] = df4['Liability_value'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "024a8bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df5 (Buildings value)\n",
    "df5['Building_value'] = df5['Building_value'].fillna(0)\n",
    "\n",
    "\n",
    "# df6 (Urban land value)\n",
    "\n",
    "df6['Urban_land_value'] = df6['Urban_land_value'].fillna(0)\n",
    "\n",
    "\n",
    "# df7 (Rural land value)\n",
    "\n",
    "df7['Rural_land_value'] = df7['Rural_land_value'].fillna(0)\n",
    "\n",
    "\n",
    "# Dropping unnecessary columns\n",
    "\n",
    "# df2.columns\n",
    "\n",
    "df3.drop(['Share_Asset_val1', 'Share_Asset_val2'], axis=1, inplace=True)\n",
    "df4.drop(['Liability_serial', 'Liability_val1', 'Liability_val2'], axis=1, inplace=True)\n",
    "df5.drop(['Building_serial'], axis=1, inplace=True)\n",
    "df6.drop(['Urban_land_serial'], axis=1, inplace=True)\n",
    "df7.drop(['Rural_land_serial'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "397a576f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin_Asset_Serial\n",
      "2    2211\n",
      "7    1242\n",
      "1     629\n",
      "4     490\n",
      "6     466\n",
      "3      94\n",
      "5      35\n",
      "Name: count, dtype: int64\n",
      "        HHID  Fin_Asset_Serial  Fin_Asset_Value\n",
      "0  100131102                 2           3237.0\n",
      "1  100131102                 7           7910.0\n",
      "2  100131102               999          11147.0\n",
      "3  100131105                 2          28600.0\n",
      "4  100131105                 7          26536.0\n",
      "2653\n"
     ]
    }
   ],
   "source": [
    "print(df2['Fin_Asset_Serial'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Fin_Asset_Value\n",
    "summary_rows = df2.groupby('HHID', as_index=False)['Fin_Asset_Value'].sum()\n",
    "summary_rows['Fin_Asset_Serial'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df2.columns:\n",
    "    if col not in ['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value']:\n",
    "        summary_rows[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df2_mod = pd.concat([df2, summary_rows], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df2_mod = df2_mod.sort_values(by=['HHID', 'Fin_Asset_Serial']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "print(df2_mod.head())\n",
    "print(df2_mod['HHID'].nunique())\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts(dropna=False)\n",
    "\n",
    "# Modify serials and labels, if any\n",
    "\n",
    "# df2_mod\n",
    "#df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([1]), 'Fin_Asset_Serial'] = 31\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([2,3,4]), 'Fin_Asset_Serial'] = 32\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([7]), 'Fin_Asset_Serial'] = 34\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([5,6]), 'Fin_Asset_Serial'] = 35\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([1]), 'Fin_Asset_Serial'] = 36\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([999]), 'Fin_Asset_Serial'] = 37\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "# Fin Asset Mapping\n",
    "\n",
    "Fin_Asset_map = {\n",
    "\n",
    "    31.0: 'Currency',\n",
    "    32.0: 'Deposits',\n",
    "    34.0: 'Life Insurance fund',\n",
    "    35.0: 'Provident and Pension fund',\n",
    "    36.0: 'Claims on Government',\n",
    "    37.0: 'Total financial assets (other thanshares and related instruments)'\n",
    "}\n",
    "\n",
    "df2_mod = df2_mod.groupby(['HHID', 'Fin_Asset_Serial'], as_index=False).agg({\n",
    "    'Fin_Asset_Value': 'sum'  \n",
    "})\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts()\n",
    "\n",
    "df2 = df2_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "f84490bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Share_Asset_serial'].value_counts()\n",
    "\n",
    "df3 = df3.reset_index(drop=True)\n",
    "\n",
    "# Share_Asset_Map\n",
    "\n",
    "# Mapping dictionary using float keys (since your column is float64)\n",
    "Share_Asset_Map = {\n",
    "    5.0: 'Shares and Debentures'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "15e84f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>Credit_Agency</th>\n",
       "      <th>Liability_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>247332101</td>\n",
       "      <td>6.0</td>\n",
       "      <td>684760.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>224701102</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>224811101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>44000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>221171103</td>\n",
       "      <td>3.0</td>\n",
       "      <td>32100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>221531101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>86339.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         HHID  Credit_Agency  Liability_value\n",
       "5   247332101            6.0         684760.0\n",
       "6   224701102            3.0          50000.0\n",
       "9   224811101            3.0          44000.0\n",
       "11  221171103            3.0          32100.0\n",
       "14  221531101            3.0          86339.0"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df4.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "b0aa8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit_Agency\n",
      "2.0     2673\n",
      "3.0     1091\n",
      "11.0     107\n",
      "7.0       88\n",
      "1.0       68\n",
      "6.0       56\n",
      "4.0       33\n",
      "5.0        7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df4['Credit_Agency'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "9f1be362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit_Agency\n",
      "2.0     2673\n",
      "3.0     1091\n",
      "11.0     107\n",
      "7.0       88\n",
      "1.0       68\n",
      "6.0       56\n",
      "4.0       33\n",
      "5.0        7\n",
      "Name: count, dtype: int64\n",
      "        HHID  Credit_Agency  Liability_value\n",
      "0  100131102            3.0          50580.0\n",
      "1  100131102            3.0         145200.0\n",
      "2  100131102          999.0         195780.0\n",
      "3  100131105            3.0          32855.0\n",
      "4  100131105          999.0          32855.0\n",
      "2596\n"
     ]
    }
   ],
   "source": [
    "print(df4['Credit_Agency'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Liability_value\n",
    "summary_rows_liab = df4.groupby('HHID', as_index=False)['Liability_value'].sum()\n",
    "summary_rows_liab['Credit_Agency'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df4.columns:\n",
    "    if col not in ['HHID', 'Credit_Agency', 'Liability_value']:\n",
    "        summary_rows_liab[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df4_mod = pd.concat([df4, summary_rows_liab], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df4_mod = df4_mod.sort_values(by=['HHID', 'Credit_Agency']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df4_mod.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "print(df4_mod.head())\n",
    "print(df4_mod['HHID'].nunique())\n",
    "\n",
    "\n",
    "# df4_mod\n",
    "df4_mod.loc[df4_mod['Credit_Agency'].isin([2,3]), 'Credit_Agency'] = 91\n",
    "df4_mod.loc[df4_mod['Credit_Agency'].isin([1,4,5,6,7,11]), 'Credit_Agency'] = 92\n",
    "\n",
    "df4_mod = df4_mod.reset_index(drop=True)\n",
    "\n",
    "# Liability Map\n",
    "\n",
    "Credit_Agency_Map = {\n",
    "\n",
    "    91.0: 'Bank advances',\n",
    "    92.0: 'Non-banking loans and advances',\n",
    "    999.0: 'Total Institutional Liabilities'\n",
    "}\n",
    "\n",
    "df4_mod = df4_mod.groupby(['HHID', 'Credit_Agency'], as_index=False).agg({\n",
    "    'Liability_value': 'sum'  \n",
    "})\n",
    "\n",
    "df4_mod = df4_mod.reset_index(drop=True)\n",
    "\n",
    "df4_mod.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "df4_mod['Credit_Agency'].value_counts()\n",
    "\n",
    "df4 = df4_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "89f2b56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Check for uniqueness of subsets\n",
    "is_unique = not df4.duplicated(subset=['HHID', 'Credit_Agency']).any()\n",
    "print(is_unique)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "63bd00e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['df1'] = df1\n",
    "dataframes['df2'] = df2\n",
    "dataframes['df3'] = df3\n",
    "dataframes['df4'] = df4\n",
    "dataframes['df5'] = df5\n",
    "dataframes['df6'] = df6\n",
    "dataframes['df7'] = df7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "171ae89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 3000\n",
      "Unique HHIDs in df2: 2653\n",
      "Unique HHIDs in df3: 2991\n",
      "Unique HHIDs in df4: 2596\n",
      "Unique HHIDs in df5: 3000\n",
      "Unique HHIDs in df6: 1107\n",
      "Unique HHIDs in df7: 2166\n",
      "Common HHIDs across: 2286\n",
      "df1: 2286 rows after filtering\n",
      "df2: 6405 rows after filtering\n",
      "df3: 2286 rows after filtering\n",
      "df4: 4713 rows after filtering\n",
      "df5: 2286 rows after filtering\n",
      "df6: 935 rows after filtering\n",
      "df7: 1588 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs: intersection of first five & union of df6, df7\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 6)\n",
    "]) & set.union(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(6, 8)\n",
    "])\n",
    "\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6, df7 = [dataframes[f'df{i}'] for i in range(1, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "140600df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs: 2286\n",
      "Number of duplicates: 0\n",
      "Unique HHIDs with Fin_Asset_Serial = 37: 2286\n",
      "Unique HHIDs with Share_Asset_serial = 5: 2286\n",
      "Unique HHIDs with Credit_Agency = 999: 2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1925433272.py:21: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1925433272.py:31: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged1.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Merging all datasets\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='HHID', how='outer'), dfs)\n",
    "\n",
    "merged_df.to_stata(\"merged.dta\", write_index=False)\n",
    "\n",
    "unique_hhids = set(merged_df['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "\n",
    "# Land value\n",
    "merged_df['Land_value'] = merged_df[['Urban_land_value', 'Rural_land_value']].sum(axis=1, min_count=1).fillna(0)\n",
    "\n",
    "# Real Estate Value\n",
    "\n",
    "merged_df['Real Estate'] = merged_df['Building_value'] + merged_df['Land_value']\n",
    "\n",
    "merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1 = merged_df.copy()\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "merged_df1.drop(['Building_value', 'Urban_land_value',\n",
    "                'Rural_land_value', 'Land_value'\n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "merged_df1.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "# Check for duplicates\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "dup_check = merged_df1.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Fin_Asset_Serial'] == 37, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Fin_Asset_Serial = 37: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Share_Asset_serial'] == 5, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Share_Asset_serial = 5: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Credit_Agency'] == 999, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Credit_Agency = 999: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "aa2883dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Column  Missing_Values  Zero_Values\n",
      "0                HHID               0            0\n",
      "1               State               0            0\n",
      "2              Weight               0            0\n",
      "3    Fin_Asset_Serial               0            0\n",
      "4  Share_Asset_serial               0            0\n",
      "5       Credit_Agency               0            0\n",
      "6   Share_Asset_Value               0           10\n",
      "7     Fin_Asset_Value               0            2\n",
      "8     Liability_value               0           62\n",
      "9         Real Estate               0            0\n",
      "Unique HHIDs: 2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\3797293583.py:20: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "cols = ['HHID', 'State','Weight', 'Fin_Asset_Serial', 'Share_Asset_serial',\n",
    "        'Credit_Agency', 'Share_Asset_Value', 'Fin_Asset_Value',\n",
    "        'Liability_value', 'Real Estate']\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Missing_Values': [merged_df1[col].isna().sum() for col in cols],\n",
    "    'Zero_Values': [(merged_df1[col] == 0).sum() for col in cols]\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(merged_df1['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "c62ce34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs present in all three groups: 2286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1483304361.py:24: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  filtered_df.to_stata(\"final(1)_13.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "merged_df3 = merged_df1.copy()\n",
    "\n",
    "# Create the new column\n",
    "merged_df3['Fin_Asset'] = merged_df3['Fin_Asset_Serial'].map(Fin_Asset_map)\n",
    "merged_df3['Share_Asset'] = merged_df3['Share_Asset_serial'].map(Share_Asset_Map)\n",
    "merged_df3['Liability'] = merged_df3['Credit_Agency'].map(Credit_Agency_Map)\n",
    "\n",
    "# HHIDs satisfying each condition\n",
    "hhid_fin = set(merged_df3.loc[merged_df3['Fin_Asset_Serial'] == 37, 'HHID'])\n",
    "hhid_share = set(merged_df3.loc[merged_df3['Share_Asset_serial'] == 5, 'HHID'])\n",
    "hhid_credit = set(merged_df3.loc[merged_df3['Credit_Agency'] == 999, 'HHID'])\n",
    "\n",
    "# Intersection: HHIDs present in all three\n",
    "common_hhids = hhid_fin & hhid_share & hhid_credit\n",
    "\n",
    "# Keep only those rows\n",
    "filtered_df = merged_df3[merged_df3['HHID'].isin(common_hhids)].copy()\n",
    "\n",
    "# Print how many unique HHIDs remain\n",
    "print(f\"Unique HHIDs present in all three groups: {len(common_hhids)}\")\n",
    "\n",
    "#filtered_df.drop(['Fin_Asset', 'Share_Asset', 'Liability'], axis=1, inplace=True)\n",
    "\n",
    "filtered_df.to_stata(\"final(1)_13.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e8fa301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 34, 35, 36, 37]\n",
      "[5]\n",
      "[91.0, 92.0, 999.0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks if grouping worked correctly\n",
    "\n",
    "print(sorted(filtered_df['Fin_Asset_Serial'].unique()))\n",
    "#print(filtered_df['Fin_Asset_Serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Share_Asset_serial'].unique()))\n",
    "#print(filtered_df['Share_Asset_serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Credit_Agency'].unique()))\n",
    "#print(filtered_df['Credit_Agency'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "b4057270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HHID  Gross wealth\n",
      "0   100131102      324547.0\n",
      "6   100131105      357936.0\n",
      "12  100131106      447196.0\n",
      "18  100141105     2103565.0\n",
      "28  100141301      600911.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\2434637912.py:33: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  df_grouped1.to_stata(\"final(25).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "df_grouped1 = filtered_df.copy()\n",
    "\n",
    "# Step 1: Filter the relevant rows satisfying your condition\n",
    "wealth_rows = df_grouped1[\n",
    "    (df_grouped1['Credit_Agency'] == 999) &\n",
    "    (df_grouped1['Fin_Asset_Serial'] == 37) &\n",
    "    (df_grouped1['Share_Asset_serial'] == 5)\n",
    "].copy()\n",
    "\n",
    "# Step 2: Compute wealth for these rows\n",
    "wealth_rows['Gross wealth'] = (\n",
    "    wealth_rows['Fin_Asset_Value'] +\n",
    "    wealth_rows['Share_Asset_Value'] +\n",
    "    wealth_rows['Real Estate']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Financial Assets'] = (\n",
    "    wealth_rows['Fin_Asset_Value']+\n",
    "    wealth_rows['Share_Asset_Value']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Institutional liabilities'] = wealth_rows['Liability_value']\n",
    "\n",
    "# Step 3: Keep only HHID and wealth columns\n",
    "wealth_per_hhid = wealth_rows[['HHID', 'Gross wealth', 'Total Financial Assets', 'Total Institutional liabilities']]\n",
    "\n",
    "# Step 4: Merge wealth back to the full dataset based on HHID\n",
    "df_grouped1 = df_grouped1.merge(wealth_per_hhid, on='HHID', how='left')\n",
    "\n",
    "# Step 5: View result\n",
    "print(df_grouped1[['HHID', 'Gross wealth']].drop_duplicates().head())\n",
    "\n",
    "df_grouped1.to_stata(\"final(25).dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "5385f47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'Weight', 'Fin_Asset_Serial', 'Fin_Asset_Value',\n",
       "       'Share_Asset_serial', 'Share_Asset_Value', 'Credit_Agency',\n",
       "       'Liability_value', 'Real Estate', 'Fin_Asset', 'Share_Asset',\n",
       "       'Liability', 'Gross wealth', 'Total Financial Assets',\n",
       "       'Total Institutional liabilities'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "38a94d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1759378772.py:17: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HHID  Credit_Agency  Fin_Asset_Serial  Share_Asset_serial  \\\n",
      "0  100131102           91.0                32                   5   \n",
      "1  100131102           91.0                32                   5   \n",
      "2  100131102           91.0                32                   5   \n",
      "3  100131102           91.0                32                   5   \n",
      "4  100131102           91.0                32                   5   \n",
      "\n",
      "   Asset_serial  Asset_Value  State  Weight  Fin_Asset_Value  \\\n",
      "0            32       3237.0     33  3187.5           3237.0   \n",
      "1           205        600.0     33  3187.5           3237.0   \n",
      "2           300     312800.0     33  3187.5           3237.0   \n",
      "3          1000     324547.0     33  3187.5           3237.0   \n",
      "4          1500      11747.0     33  3187.5           3237.0   \n",
      "\n",
      "   Share_Asset_Value  Liability_value  Real Estate Fin_Asset  \\\n",
      "0              600.0         195780.0     312800.0  Deposits   \n",
      "1              600.0         195780.0     312800.0  Deposits   \n",
      "2              600.0         195780.0     312800.0  Deposits   \n",
      "3              600.0         195780.0     312800.0  Deposits   \n",
      "4              600.0         195780.0     312800.0  Deposits   \n",
      "\n",
      "             Share_Asset      Liability  Gross wealth  Total Financial Assets  \\\n",
      "0  Shares and Debentures  Bank advances      324547.0                 11747.0   \n",
      "1  Shares and Debentures  Bank advances      324547.0                 11747.0   \n",
      "2  Shares and Debentures  Bank advances      324547.0                 11747.0   \n",
      "3  Shares and Debentures  Bank advances      324547.0                 11747.0   \n",
      "4  Shares and Debentures  Bank advances      324547.0                 11747.0   \n",
      "\n",
      "   Total Institutional liabilities              Asset_Name  \n",
      "0                         195780.0                Deposits  \n",
      "1                         195780.0   Shares and Debentures  \n",
      "2                         195780.0             Real Estate  \n",
      "3                         195780.0            Gross wealth  \n",
      "4                         195780.0  Total Financial Assets  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1759378772.py:88: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  asset_df.to_stata(\"final(28).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "df_grouped2 =  df_grouped1.copy()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Initialize columns with NaN\n",
    "df_grouped2['Asset_serial'] = np.nan\n",
    "df_grouped2['Asset_Value'] = np.nan\n",
    "df_grouped2['Asset_Name'] = np.nan\n",
    "\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "\n",
    "df_grouped2['HHID'] = df_grouped2['HHID'].astype(str)\n",
    "\n",
    "#df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n",
    "\n",
    "\n",
    "df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n",
    "\n",
    "\n",
    "# Empty list to store new rows\n",
    "rows = []\n",
    "\n",
    "# Loop over each unique group\n",
    "for _, group in df_grouped2.groupby(group_cols):\n",
    "\n",
    "    # Get first row for copying other columns\n",
    "    first_row = group.iloc[0].to_dict()\n",
    "\n",
    "    # 1. Financial Asset Row\n",
    "    if not np.isnan(first_row['Fin_Asset_Value']):\n",
    "        fa_row = first_row.copy()\n",
    "        fa_row['Asset_serial'] = first_row['Fin_Asset_Serial']\n",
    "        fa_row['Asset_Value'] = first_row['Fin_Asset_Value']\n",
    "        fa_row['Asset_Name'] = first_row['Fin_Asset']\n",
    "        rows.append(fa_row)\n",
    "\n",
    "    # 2. Share Asset Row\n",
    "    if not np.isnan(first_row['Share_Asset_Value']):\n",
    "        sa_row = first_row.copy()\n",
    "        sa_row['Asset_serial'] = 200 + first_row['Share_Asset_serial']  # As you specified\n",
    "        sa_row['Asset_Value'] = first_row['Share_Asset_Value']\n",
    "        sa_row['Asset_Name'] = first_row['Share_Asset']\n",
    "        rows.append(sa_row)\n",
    "\n",
    "    # 3. Real Estate Row\n",
    "    if not np.isnan(first_row['Real Estate']):\n",
    "        re_row = first_row.copy()\n",
    "        re_row['Asset_serial'] = 300\n",
    "        re_row['Asset_Value'] = first_row['Real Estate']\n",
    "        re_row['Asset_Name'] = 'Real Estate'\n",
    "        rows.append(re_row)\n",
    "\n",
    "    # 4. Gross wealth Row\n",
    "    if not np.isnan(first_row['Gross wealth']):\n",
    "        w_row = first_row.copy()\n",
    "        w_row['Asset_serial'] = 1000\n",
    "        w_row['Asset_Value'] = first_row['Gross wealth']\n",
    "        w_row['Asset_Name'] = 'Gross wealth'\n",
    "        rows.append(w_row)\n",
    "\n",
    "    # 6. Total Financial Assets Row\n",
    "    if not np.isnan(first_row['Total Financial Assets']):\n",
    "        tfa_row = first_row.copy()\n",
    "        tfa_row['Asset_serial'] = 1500\n",
    "        tfa_row['Asset_Value'] = first_row['Total Financial Assets']\n",
    "        tfa_row['Asset_Name'] = 'Total Financial Assets'\n",
    "        rows.append(tfa_row)   \n",
    "\n",
    "# Convert to DataFrame\n",
    "asset_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: Arrange columns nicely\n",
    "cols_order = group_cols + ['Asset_serial', 'Asset_Value'] + [col for col in df_grouped2.columns if col not in group_cols + ['Asset_serial', 'Asset_Value']]\n",
    "asset_df = asset_df[cols_order]\n",
    "\n",
    "# Check result\n",
    "print(asset_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Liabilities\n",
    "\n",
    "asset_df = asset_df.rename(columns={\n",
    "    'Credit_Agency': 'Liability_serial',\n",
    "    'Liability': 'Liability_name'\n",
    "})\n",
    "\n",
    "asset_df.to_stata(\"final(28).dta\", write_index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "9a2f3c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subsets: 13257\n",
      "Number of duplicates: 53028\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "group_cols = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "group_cols1 = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial', 'Asset_serial']\n",
    "\n",
    "# Count unique combinations\n",
    "num_subsets = asset_df[group_cols].drop_duplicates().shape[0]\n",
    "\n",
    "print(f\"Number of unique subsets: {num_subsets}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols1)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "318a96f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset_serial    0\n",
      "Asset_Value     0\n",
      "Asset_Name      0\n",
      "dtype: int64\n",
      "\n",
      "Total rows with missing values: 0\n",
      "Empty DataFrame\n",
      "Columns: [HHID, Liability_serial, Fin_Asset_Serial, Share_Asset_serial, Asset_serial, Asset_Value, State, Weight, Fin_Asset_Value, Share_Asset_Value, Liability_value, Real Estate, Fin_Asset, Share_Asset, Liability_name, Gross wealth, Total Financial Assets, Total Institutional liabilities, Asset_Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "# Check how many missing in each column\n",
    "print(asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().sum())\n",
    "\n",
    "# Optionally, view rows with missing in any of them\n",
    "missing_rows = asset_df[\n",
    "    asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().any(axis=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal rows with missing values: {len(missing_rows)}\")\n",
    "print(missing_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "a98ad296",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\3884663063.py:11: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\3884663063.py:19: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\3884663063.py:28: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df1.to_stata(\"final(2013)_1.dta\", write_index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2286"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = asset_df.copy()\n",
    "\n",
    "final_df.drop(['Fin_Asset_Serial', \n",
    "               'Share_Asset_serial', \n",
    "               'Fin_Asset_Value',\n",
    "                'Share_Asset_Value', \n",
    "                'Fin_Asset',\n",
    "                'Share_Asset',               \n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
    "\n",
    "final_df = final_df[['HHID', 'State' , 'Weight','Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value',\n",
    "                    'Real Estate', 'Total Financial Assets', 'Total Institutional liabilities', \n",
    "                    ]].copy()\n",
    "\n",
    "final_df.to_stata(\"final(2013).dta\", write_index=False)\n",
    "\n",
    "final_df['HHID'].nunique()\n",
    "\n",
    "final_df1 = final_df[['HHID', 'State', 'Weight', 'Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value'\n",
    "                    ]].copy()\n",
    "\n",
    "final_df1.to_stata(\"final(2013)_1.dta\", write_index=False)\n",
    "final_df1['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b89b6a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 34, 35, 36, 37, 205, 300, 1000, 1500]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(final_df1['Asset_serial'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "ca44a65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All HHIDs are unique within each Asset_serial category.\n",
      " All HHIDs are unique within each Liability_serial category.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1595470577.py:5: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df2.to_stata(\"final(2013)_2.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "group_cols1 = ['HHID', 'Liability_serial', 'Asset_serial']\n",
    "\n",
    "final_df2 = final_df1.drop_duplicates(subset=group_cols1, keep='first').reset_index(drop=True)\n",
    "\n",
    "final_df2.to_stata(\"final(2013)_2.dta\", write_index=False)\n",
    "\n",
    "# Check for repetition\n",
    "\n",
    "# Check for uniqueness \n",
    "\n",
    "duplicates = final_df2.duplicated(subset=['Asset_serial', 'HHID', 'Liability_serial'], keep=False)\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Asset_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Asset_serial', 'HHID']].sort_values(['Asset_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Asset_serial category.\")\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Liability_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Liability_serial', 'HHID']].sort_values(['Liability_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Liability_serial category.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "494ec03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2286"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3875d7b",
   "metadata": {},
   "source": [
    "# Add State names corresponding to their codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "0659ac81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_33136\\1197275976.py:49: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df3.to_stata('2013_final.dta', write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# final_df2 (Merge D&D, D&N)\n",
    "final_df2.loc[final_df2['State'].isin([25,26]), 'State'] = 25\n",
    "final_df2['State'].value_counts()\n",
    "\n",
    "# State Map\n",
    "\n",
    "State_map = {\n",
    "    1.0: 'Jammu and Kashmir',\n",
    "    2.0: 'Himachal Pradesh',\n",
    "    3.0: 'Punjab',\n",
    "    4.0: 'Chandigarh',\n",
    "    5.0: 'Uttarakhand',\n",
    "    6.0: 'Haryana',\n",
    "    7.0: 'Delhi',\n",
    "    8.0: 'Rajasthan',\n",
    "    9.0: 'Uttar Pradesh',\n",
    "    10.0: 'Bihar',\n",
    "    11.0: 'Sikkim',\n",
    "    12.0: 'Arunachal Pradesh',\n",
    "    13.0: 'Nagaland',\n",
    "    14.0: 'Manipur',\n",
    "    15.0: 'Mizoram',\n",
    "    16.0: 'Tripura',\n",
    "    17.0: 'Meghalaya',\n",
    "    18.0: 'Assam',\n",
    "    19.0: 'West Bengal',\n",
    "    20.0: 'Jharkhand',\n",
    "    21.0: 'Odisha',\n",
    "    22.0: 'Chhattishgarh',\n",
    "    23.0: 'Madhya Pradesh',\n",
    "    24.0: 'Gujarat',\n",
    "    25.0: 'Daman and Diu and Dadra and Nagar Haveli',\n",
    "    27.0: 'Maharashtra',\n",
    "    28.0: 'Andhra Pradesh',\n",
    "    29.0: 'Karnataka',\n",
    "    30.0: 'Goa',\n",
    "    31.0: 'Lakshadweep',\n",
    "    32.0: 'Kerala',\n",
    "    33.0: 'Tamilnadu',\n",
    "    34.0: 'Puducherry',\n",
    "    35.0: 'Andaman & Nicobar',\n",
    "    36.0: 'Telengana'\n",
    "}\n",
    "\n",
    "final_df3 = final_df2.copy()\n",
    "\n",
    "final_df3['State_name'] = final_df3['State'].map(State_map)\n",
    "\n",
    "final_df3.to_stata('2013_final.dta', write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bab64fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39750f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174d2da2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
