{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3853768b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "os.chdir(r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\")\n",
    "os.getcwd()\n",
    "\n",
    "fp_1992 = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\1992\\1992_final.dta\"\n",
    "fp_2003 = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\2003\\2003_final.dta\"\n",
    "fp_2013 = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\2013\\2013_final.dta\"\n",
    "fp_2019 = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\2019\\2019_final.dta\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a180203c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "d02c4aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reserve files\n",
    "\n",
    "#r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\1992_final.dta\"\n",
    "\n",
    "#r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\2003_final.dta\"\n",
    "\n",
    "#r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\2013_final.dta\"\n",
    "\n",
    "#r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\2019_final.dta\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "721555d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992 = pd.read_stata(fp_1992)\n",
    "df_2003 = pd.read_stata(fp_2003)\n",
    "df_2013 = pd.read_stata(fp_2013)\n",
    "df_2019 = pd.read_stata(fp_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40c05bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'Weight', 'Gross_wealth', 'Liability_serial',\n",
       "       'Liability_name', 'Liability_value', 'Asset_serial', 'Asset_Name',\n",
       "       'Asset_Value', 'State_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2003.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d8ab57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    78054.000000\n",
       "mean        14.697743\n",
       "std         18.268026\n",
       "min          0.026350\n",
       "25%          2.961100\n",
       "50%          7.985750\n",
       "75%         19.252500\n",
       "max        255.870200\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2003['Weight'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e734cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    5406.000000\n",
       "mean       14.957802\n",
       "std        18.146613\n",
       "min         0.026350\n",
       "25%         3.032412\n",
       "50%         8.220575\n",
       "75%        19.809563\n",
       "max       255.870200\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2003_hh = df_2003[['HHID', 'Weight']].copy()\n",
    "df_2003_hh = df_2003_hh.drop_duplicates().reset_index()\n",
    "print(df_2003_hh['Weight'].describe())\n",
    "\n",
    "df_2013_hh = df_2013[['HHID', 'Weight']].copy()\n",
    "df_2013_hh = df_2013_hh.drop_duplicates().reset_index()\n",
    "print(df_2013_hh['Weight'].describe())\n",
    "\n",
    "df_2019_hh = df_2019[['HHID', 'Weight']].copy()\n",
    "df_2019_hh = df_2019_hh.drop_duplicates().reset_index()\n",
    "print(df_2019_hh['Weight'].describe()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4705ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2019_hh = df_2019[['HHID', 'Weight']].copy()\n",
    "df_2019_hh = df_2019_hh.drop_duplicates().reset_index()\n",
    "df_2019_hh['Weight'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4eadde34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     2286.000000\n",
       "mean      1854.393865\n",
       "std       2793.314884\n",
       "min          4.165000\n",
       "25%        467.931250\n",
       "50%       1038.665000\n",
       "75%       2078.541250\n",
       "max      45360.000000\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2013_hh = df_2013[['HHID', 'Weight']].copy()\n",
    "df_2013_hh = df_2013_hh.drop_duplicates().reset_index()\n",
    "df_2013_hh['Weight'].describe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6bd8090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    32109.000000\n",
       "mean      1860.794501\n",
       "std       2750.815591\n",
       "min          4.165000\n",
       "25%        468.835000\n",
       "50%       1049.465000\n",
       "75%       2088.665000\n",
       "max      45360.000000\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2013['Weight'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd00529",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    16398.000000\n",
       "mean      1023.158252\n",
       "std       1581.456348\n",
       "min          6.250000\n",
       "25%        152.250000\n",
       "50%        450.000000\n",
       "75%       1300.000000\n",
       "max      23437.750000\n",
       "Name: Weight, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019['Weight'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "559fb9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_1 = df_1992.copy()\n",
    "df_2003_1 = df_2003.copy()\n",
    "df_2013_1 = df_2013.copy()\n",
    "df_2019_1 = df_2019.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "45d2ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_2 = df_1992.copy()\n",
    "df_2003_2 = df_2003.copy()\n",
    "df_2013_2 = df_2013.copy()\n",
    "df_2019_2 = df_2019.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "f9c7fecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_3 = pd.read_stata(fp_1992)\n",
    "df_2003_3 = pd.read_stata(fp_2003)\n",
    "df_2013_3 = pd.read_stata(fp_2013)\n",
    "df_2019_3 = pd.read_stata(fp_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "d6c41e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_4 = pd.read_stata(fp_1992)\n",
    "df_2003_4 = pd.read_stata(fp_2003)\n",
    "df_2013_4 = pd.read_stata(fp_2013)\n",
    "df_2019_4 = pd.read_stata(fp_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "c9463c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_5 = pd.read_stata(fp_1992)\n",
    "df_2003_5 = pd.read_stata(fp_2003)\n",
    "df_2013_5 = pd.read_stata(fp_2013)\n",
    "df_2019_5 = pd.read_stata(fp_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "8a84ebfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_11 = pd.read_stata(fp_1992)\n",
    "df_2003_11 = pd.read_stata(fp_2003)\n",
    "df_2013_11 = pd.read_stata(fp_2013)\n",
    "df_2019_11 = pd.read_stata(fp_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "6c04ba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1992_12 = pd.read_stata(fp_1992)\n",
    "df_2003_12 = pd.read_stata(fp_2003)\n",
    "df_2013_12 = pd.read_stata(fp_2013)\n",
    "df_2019_12 = pd.read_stata(fp_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "608cf99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>State</th>\n",
       "      <th>MCE</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Gross_wealth</th>\n",
       "      <th>Liability_serial</th>\n",
       "      <th>Liability_name</th>\n",
       "      <th>Liability_value</th>\n",
       "      <th>Asset_serial</th>\n",
       "      <th>Asset_Name</th>\n",
       "      <th>Asset_Value</th>\n",
       "      <th>State_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501171011</td>\n",
       "      <td>24</td>\n",
       "      <td>3200</td>\n",
       "      <td>95.815</td>\n",
       "      <td>1523500.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>Bank advances</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>31</td>\n",
       "      <td>Currency</td>\n",
       "      <td>3000.0</td>\n",
       "      <td>Gujarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>501171011</td>\n",
       "      <td>24</td>\n",
       "      <td>3200</td>\n",
       "      <td>95.815</td>\n",
       "      <td>1523500.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>Bank advances</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>205</td>\n",
       "      <td>Shares and Debentures</td>\n",
       "      <td>500.0</td>\n",
       "      <td>Gujarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>501171011</td>\n",
       "      <td>24</td>\n",
       "      <td>3200</td>\n",
       "      <td>95.815</td>\n",
       "      <td>1523500.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>Bank advances</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>300</td>\n",
       "      <td>Real Estate</td>\n",
       "      <td>1500000.0</td>\n",
       "      <td>Gujarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>501171011</td>\n",
       "      <td>24</td>\n",
       "      <td>3200</td>\n",
       "      <td>95.815</td>\n",
       "      <td>1523500.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>Bank advances</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1000</td>\n",
       "      <td>Gross wealth</td>\n",
       "      <td>1523500.0</td>\n",
       "      <td>Gujarat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>501171011</td>\n",
       "      <td>24</td>\n",
       "      <td>3200</td>\n",
       "      <td>95.815</td>\n",
       "      <td>1523500.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>Bank advances</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1500</td>\n",
       "      <td>Total Financial Assets</td>\n",
       "      <td>23500.0</td>\n",
       "      <td>Gujarat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        HHID  State   MCE  Weight  Gross_wealth  Liability_serial  \\\n",
       "0  501171011     24  3200  95.815     1523500.0              91.0   \n",
       "1  501171011     24  3200  95.815     1523500.0              91.0   \n",
       "2  501171011     24  3200  95.815     1523500.0              91.0   \n",
       "3  501171011     24  3200  95.815     1523500.0              91.0   \n",
       "4  501171011     24  3200  95.815     1523500.0              91.0   \n",
       "\n",
       "  Liability_name  Liability_value  Asset_serial              Asset_Name  \\\n",
       "0  Bank advances          10000.0            31                Currency   \n",
       "1  Bank advances          10000.0           205   Shares and Debentures   \n",
       "2  Bank advances          10000.0           300             Real Estate   \n",
       "3  Bank advances          10000.0          1000            Gross wealth   \n",
       "4  Bank advances          10000.0          1500  Total Financial Assets   \n",
       "\n",
       "   Asset_Value State_name  \n",
       "0       3000.0    Gujarat  \n",
       "1        500.0    Gujarat  \n",
       "2    1500000.0    Gujarat  \n",
       "3    1523500.0    Gujarat  \n",
       "4      23500.0    Gujarat  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6bd45cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: Statewise_Concentration_1992.xlsx\n",
      "Saved: Statewise_Concentration_2003.xlsx\n",
      "Saved: Statewise_Concentration_2013.xlsx\n",
      "Saved: Statewise_Concentration_2019.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------------------\n",
    "# 1. Function to Compute Concentration Table (Handles weighted & unweighted)\n",
    "# -----------------------------------------\n",
    "def compute_concentration(df, value_col, name_col):\n",
    "    df = df.copy()\n",
    "\n",
    "    # If Weight column exists, compute weighted values; else set equal to value_col\n",
    "    if 'Weight' in df.columns:\n",
    "        df['Weighted_Value'] = df[value_col] * df['Weight']\n",
    "    else:\n",
    "        df['Weighted_Value'] = df[value_col]  # treat as unweighted\n",
    "\n",
    "    # Group by category & state\n",
    "    grouped = df.groupby([name_col, 'State_name'], as_index=False).agg(\n",
    "        Non_Weighted_Total=(value_col, 'sum'),\n",
    "        Weighted_Total=('Weighted_Value', 'sum')\n",
    "    )\n",
    "\n",
    "    # National totals\n",
    "    national_totals = grouped.groupby(name_col).agg(\n",
    "        Total_Non_Weighted=('Non_Weighted_Total', 'sum'),\n",
    "        Total_Weighted=('Weighted_Total', 'sum')\n",
    "    ).reset_index()\n",
    "\n",
    "    # Merge\n",
    "    merged = grouped.merge(national_totals, on=name_col)\n",
    "\n",
    "    # Percentages\n",
    "    merged['Non_Weighted_Percentage'] = (merged['Non_Weighted_Total'] / merged['Total_Non_Weighted']) * 100\n",
    "    merged['Weighted_Percentage'] = (merged['Weighted_Total'] / merged['Total_Weighted']) * 100\n",
    "\n",
    "    # If Weight column didn't exist, drop Weighted_Percentage\n",
    "    if 'Weight' not in df.columns:\n",
    "        merged['Weighted_Percentage'] = None  # or just drop this column\n",
    "\n",
    "    final = merged[[name_col, 'State_name', 'Non_Weighted_Percentage', 'Weighted_Percentage']]\n",
    "    return final\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 2. Function to Process Each Year\n",
    "# -----------------------------------------\n",
    "def process_year(df, year):\n",
    "    assets_df = df.dropna(subset=['Asset_serial', 'Asset_Value']).copy()\n",
    "    liab_df = df.dropna(subset=['Liability_serial', 'Liability_value']).copy()\n",
    "\n",
    "    asset_results = compute_concentration(assets_df, 'Asset_Value', 'Asset_Name')\n",
    "    liability_results = compute_concentration(liab_df, 'Liability_value', 'Liability_name')\n",
    "\n",
    "    with pd.ExcelWriter(f\"Statewise_Concentration_{year}.xlsx\") as writer:\n",
    "        asset_results.to_excel(writer, sheet_name='Assets', index=False)\n",
    "        liability_results.to_excel(writer, sheet_name='Liabilities', index=False)\n",
    "\n",
    "    print(f\"Saved: Statewise_Concentration_{year}.xlsx\")\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------\n",
    "# 3. Apply for All Years\n",
    "# -----------------------------------------\n",
    "process_year(df_1992_1, 1992)\n",
    "process_year(df_2003_1, 2003)\n",
    "process_year(df_2013_1, 2013)\n",
    "process_year(df_2019_1, 2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d07cf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions1_1992.xlsx\n",
      "Exported Household_Compositions1_2003.xlsx\n",
      "Exported Household_Compositions1_2013.xlsx\n",
      "Exported Household_Compositions1_2019.xlsx\n",
      "✅ All files exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    # Collapse to HHID level\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    # --- Wealth quartiles ---\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(\n",
    "        hhid_df[wealth_col],\n",
    "        [-np.inf, *cuts_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(\n",
    "            hhid_df[wealth_col],\n",
    "            [-np.inf, *cuts_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_weighted = None\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    # --- MCE quartiles (if present) ---\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(\n",
    "            hhid_df[mce_col],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(\n",
    "                hhid_df[mce_col],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted'] + \n",
    "                   ([ 'MCEGroup','MCEGroup_weighted'] if mce_col and mce_col in df.columns else [])], \\\n",
    "           cuts_unweighted, cuts_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col):\n",
    "    # Pre-aggregate per HHID per category\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Aggregate by group & category\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(\n",
    "            TotalValue=(value_col, 'sum'),\n",
    "            HouseholdCount=('HHID','nunique')\n",
    "        ).reset_index()\n",
    "    )\n",
    "\n",
    "    # Compute category totals\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Weighted values\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    # Assign HHID-level quartiles\n",
    "    hhid_groups, w_cuts, w_cuts_wt, mce_cuts, mce_cuts_wt = assign_hhid_quartiles(df, \n",
    "        wealth_col='Gross_wealth', \n",
    "        mce_col='MCE', \n",
    "        weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Composition tables\n",
    "    wealth_assets = compute_composition(df, 'Gross_wealthGroup', 'Asset_Value_weighted', 'Asset_Name')\n",
    "    wealth_assets_wt = compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Value_weighted', 'Asset_Name') if has_weight else None\n",
    "    mce_assets = compute_composition(df, 'MCEGroup', 'Asset_Value_weighted', 'Asset_Name') if 'MCEGroup' in df.columns else None\n",
    "    mce_assets_wt = compute_composition(df, 'MCEGroup_weighted', 'Asset_Value_weighted', 'Asset_Name') if has_weight and 'MCEGroup_weighted' in df.columns else None\n",
    "\n",
    "    wealth_liabilities = compute_composition(df, 'Gross_wealthGroup', 'Liability_Value_weighted', 'Liability_name')\n",
    "    wealth_liabilities_wt = compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_Value_weighted', 'Liability_name') if has_weight else None\n",
    "    mce_liabilities = compute_composition(df, 'MCEGroup', 'Liability_Value_weighted', 'Liability_name') if 'MCEGroup' in df.columns else None\n",
    "    mce_liabilities_wt = compute_composition(df, 'MCEGroup_weighted', 'Liability_Value_weighted', 'Liability_name') if has_weight and 'MCEGroup_weighted' in df.columns else None\n",
    "\n",
    "    # Quartile cutoff table\n",
    "    cutoff_data = {\n",
    "        'Percentile': ['25th','50th','75th'],\n",
    "        'Wealth_Unweighted': w_cuts.values,\n",
    "        'Wealth_Weighted': w_cuts_wt if w_cuts_wt is not None else [None,None,None]\n",
    "    }\n",
    "    if mce_cuts is not None:\n",
    "        cutoff_data['MCE_Unweighted'] = mce_cuts.values\n",
    "        cutoff_data['MCE_Weighted'] = mce_cuts_wt if mce_cuts_wt is not None else [None,None,None]\n",
    "    cutoffs = pd.DataFrame(cutoff_data)\n",
    "\n",
    "    # Export to Excel\n",
    "    with pd.ExcelWriter(f'Household_Compositions1_{year}.xlsx') as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        if wealth_assets_wt is not None: wealth_assets_wt.to_excel(writer, sheet_name='Assets_WealthG_weighted', index=False)\n",
    "        if mce_assets is not None: mce_assets.to_excel(writer, sheet_name='Assets_MCEG', index=False)\n",
    "        if mce_assets_wt is not None: mce_assets_wt.to_excel(writer, sheet_name='Assets_MCEG_weighted', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if wealth_liabilities_wt is not None: wealth_liabilities_wt.to_excel(writer, sheet_name='Liabilities_WealthG_weighted', index=False)\n",
    "        if mce_liabilities is not None: mce_liabilities.to_excel(writer, sheet_name='Liabilities_MCEG', index=False)\n",
    "        if mce_liabilities_wt is not None: mce_liabilities_wt.to_excel(writer, sheet_name='Liabilities_MCEG_weighted', index=False)\n",
    "        cutoffs.to_excel(writer, sheet_name='Quartile_Cutoffs', index=False)\n",
    "    print(f\"Exported Household_Compositions1_{year}.xlsx\")\n",
    "\n",
    "# -------------------- Run for All Years --------------------\n",
    "datasets = [(df_1992_1, 1992), (df_2003_1, 2003), (df_2013_1, 2013), (df_2019_1, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year)\n",
    "print(\"✅ All files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f683169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions1_1992.xlsx\n",
      "Exported Household_Compositions1_2003.xlsx\n",
      "Exported Household_Compositions1_2013.xlsx\n",
      "Exported Household_Compositions1_2019.xlsx\n",
      "✅ All files exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    # --- Wealth quartiles ---\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(\n",
    "        hhid_df[wealth_col],\n",
    "        [-np.inf, *cuts_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(\n",
    "            hhid_df[wealth_col],\n",
    "            [-np.inf, *cuts_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_weighted = None\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    # --- MCE quartiles ---\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(\n",
    "            hhid_df[mce_col],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(\n",
    "                hhid_df[mce_col],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted'] + \n",
    "                   ([ 'MCEGroup','MCEGroup_weighted'] if mce_col and mce_col in df.columns else [])], \\\n",
    "           cuts_unweighted, cuts_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation (Corrected: Apply weight once per HH) --------------------\n",
    "def compute_composition(df, group_col, category_col):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Collapse to HHID-category level first (sum per household)\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False).agg(\n",
    "        HH_Category_Value=('Asset_Value', 'sum'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID', 'size')\n",
    "    )\n",
    "    # Apply weight at household level\n",
    "    if has_weight:\n",
    "        hh_cat['HH_Category_Value_weighted'] = hh_cat['HH_Category_Value'] * hh_cat['Weight']\n",
    "        value_col_to_use = 'HH_Category_Value_weighted'\n",
    "    else:\n",
    "        value_col_to_use = 'HH_Category_Value'\n",
    "\n",
    "    # Merge quartile groups\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Aggregate by category & quartile\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(\n",
    "            TotalValue=(value_col_to_use, 'sum'),\n",
    "            HouseholdCount=('HHID','nunique')\n",
    "        ).reset_index()\n",
    "    )\n",
    "    # Compute category totals\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col_to_use].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "    return merged\n",
    "\n",
    "# -------------------- Household-Level Summary --------------------\n",
    "def compute_household_totals(df):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    agg_dict = {\n",
    "        'Total_Assets': ('Asset_Value','sum'),\n",
    "        'Total_Liabilities': ('Liability_value','sum')\n",
    "    }\n",
    "    if has_weight:\n",
    "        agg_dict['Weight'] = ('Weight','first')\n",
    "    \n",
    "    hh_agg = df.groupby('HHID', as_index=False).agg(**agg_dict)\n",
    "    hh_agg['Net_Wealth'] = hh_agg['Total_Assets'] - hh_agg['Total_Liabilities']\n",
    "\n",
    "    if has_weight:\n",
    "        hh_agg['Total_Assets_wt'] = hh_agg['Total_Assets'] * hh_agg['Weight']\n",
    "        hh_agg['Total_Liabilities_wt'] = hh_agg['Total_Liabilities'] * hh_agg['Weight']\n",
    "        hh_agg['Net_Wealth_wt'] = hh_agg['Net_Wealth'] * hh_agg['Weight']\n",
    "        summary = {\n",
    "            'Households': hh_agg['HHID'].nunique(),\n",
    "            'Weights_Applied': True,\n",
    "            'Total_Assets_Unweighted': hh_agg['Total_Assets'].sum(),\n",
    "            'Total_Assets_Weighted': hh_agg['Total_Assets_wt'].sum(),\n",
    "            'Total_Liabilities_Unweighted': hh_agg['Total_Liabilities'].sum(),\n",
    "            'Total_Liabilities_Weighted': hh_agg['Total_Liabilities_wt'].sum(),\n",
    "            'Net_Wealth_Unweighted': hh_agg['Net_Wealth'].sum(),\n",
    "            'Net_Wealth_Weighted': hh_agg['Net_Wealth_wt'].sum()\n",
    "        }\n",
    "    else:\n",
    "        summary = {\n",
    "            'Households': hh_agg['HHID'].nunique(),\n",
    "            'Weights_Applied': False,\n",
    "            'Total_Assets_Unweighted': hh_agg['Total_Assets'].sum(),\n",
    "            'Total_Assets_Weighted': None,\n",
    "            'Total_Liabilities_Unweighted': hh_agg['Total_Liabilities'].sum(),\n",
    "            'Total_Liabilities_Weighted': None,\n",
    "            'Net_Wealth_Unweighted': hh_agg['Net_Wealth'].sum(),\n",
    "            'Net_Wealth_Weighted': None\n",
    "        }\n",
    "    return pd.DataFrame([summary])\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Quartile assignment\n",
    "    hhid_groups, w_cuts, w_cuts_wt, mce_cuts, mce_cuts_wt = assign_hhid_quartiles(df, \n",
    "        wealth_col='Gross_wealth', \n",
    "        mce_col='MCE', \n",
    "        weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Compositions (wealth-based assets & liabilities)\n",
    "    wealth_assets = compute_composition(df, 'Gross_wealthGroup', 'Asset_Name')\n",
    "    wealth_assets_wt = compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Name') if has_weight else None\n",
    "    wealth_liabilities = compute_composition(df, 'Gross_wealthGroup', 'Liability_name')\n",
    "    wealth_liabilities_wt = compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_name') if has_weight else None\n",
    "\n",
    "    # Quartile cutoffs\n",
    "    cutoff_data = {\n",
    "        'Percentile': ['25th','50th','75th'],\n",
    "        'Wealth_Unweighted': w_cuts.values,\n",
    "        'Wealth_Weighted': w_cuts_wt if w_cuts_wt is not None else [None,None,None]\n",
    "    }\n",
    "    cutoffs = pd.DataFrame(cutoff_data)\n",
    "\n",
    "    # Household-level totals\n",
    "    summary = compute_household_totals(df)\n",
    "\n",
    "    # Export to Excel\n",
    "    with pd.ExcelWriter(f'Household_Compositions1_{year}.xlsx') as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        if wealth_assets_wt is not None: wealth_assets_wt.to_excel(writer, sheet_name='Assets_WealthG_weighted', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if wealth_liabilities_wt is not None: wealth_liabilities_wt.to_excel(writer, sheet_name='Liabilities_WealthG_weighted', index=False)\n",
    "        cutoffs.to_excel(writer, sheet_name='Quartile_Cutoffs', index=False)\n",
    "        summary.to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
    "    print(f\"Exported Household_Compositions1_{year}.xlsx\")\n",
    "\n",
    "# -------------------- Run for All Years --------------------\n",
    "datasets = [(df_1992_1, 1992), (df_2003_1, 2003), (df_2013_1, 2013), (df_2019_1, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year)\n",
    "print(\"✅ All files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ebda1c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Unweighted_Compositions_1992.xlsx\n",
      "Exported Household_Unweighted_Compositions_2003.xlsx\n",
      "Exported Household_Unweighted_Compositions_2013.xlsx\n",
      "Exported Household_Unweighted_Compositions_2019.xlsx\n",
      "✅ All unweighted files exported.\n"
     ]
    }
   ],
   "source": [
    "df_1992 = pd.read_stata(fp_1992)\n",
    "df_2003 = pd.read_stata(fp_2003)\n",
    "df_2013 = pd.read_stata(fp_2013)\n",
    "df_2019 = pd.read_stata(fp_2019)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -------------------- Assign Quartile Groups --------------------\n",
    "def assign_unweighted_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE'):\n",
    "    # Collapse to HHID level\n",
    "    group_dict = {wealth_col: ('Gross_wealth', 'sum')}\n",
    "    if mce_col in df.columns:\n",
    "        group_dict[mce_col] = ('MCE', 'sum')\n",
    "    hh = df.groupby('HHID', as_index=False).agg(**group_dict)\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_wealth = hh['Gross_wealth'].quantile([0.25, 0.5, 0.75])\n",
    "    hh['WealthGroup'] = pd.cut(\n",
    "        hh['Gross_wealth'],\n",
    "        [-np.inf, *cuts_wealth, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    # MCE quartiles (only if MCE exists)\n",
    "    if 'MCE' in hh.columns:\n",
    "        cuts_mce = hh['MCE'].quantile([0.25, 0.5, 0.75])\n",
    "        hh['MCEGroup'] = pd.cut(\n",
    "            hh['MCE'],\n",
    "            [-np.inf, *cuts_mce, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_mce = None\n",
    "        hh['MCEGroup'] = None\n",
    "\n",
    "    return hh[['HHID','WealthGroup','MCEGroup']], cuts_wealth, cuts_mce\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_unweighted_composition(df, group_col, category_col, value_col):\n",
    "    # Collapse to household level per category\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Aggregate by group & category\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(\n",
    "            TotalValue=(value_col, 'sum'),\n",
    "            HouseholdCount=('HHID','nunique')\n",
    "        ).reset_index()\n",
    "    )\n",
    "    # Compute category totals\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_unweighted(df, year):\n",
    "    # Assign quartiles\n",
    "    hhid_groups, wealth_cuts, mce_cuts = assign_unweighted_quartiles(df)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Wealth-based composition\n",
    "    wealth_assets = compute_unweighted_composition(df, 'WealthGroup', 'Asset_Name', 'Asset_Value')\n",
    "    wealth_liabilities = compute_unweighted_composition(df, 'WealthGroup', 'Liability_name', 'Liability_value')\n",
    "\n",
    "    # MCE-based composition (only if present)\n",
    "    if 'MCEGroup' in hhid_groups.columns and hhid_groups['MCEGroup'].notnull().any():\n",
    "        mce_assets = compute_unweighted_composition(df, 'MCEGroup', 'Asset_Name', 'Asset_Value')\n",
    "        mce_liabilities = compute_unweighted_composition(df, 'MCEGroup', 'Liability_name', 'Liability_value')\n",
    "    else:\n",
    "        mce_assets, mce_liabilities = None, None\n",
    "\n",
    "    # Quartile cutoffs\n",
    "    cutoffs = pd.DataFrame({\n",
    "        'Percentile': ['25th','50th','75th'],\n",
    "        'Wealth_Cutoffs': wealth_cuts.values,\n",
    "        'MCE_Cutoffs': mce_cuts.values if mce_cuts is not None else [None, None, None]\n",
    "    })\n",
    "\n",
    "    # Export\n",
    "    with pd.ExcelWriter(f'Household_Unweighted_Compositions_{year}.xlsx') as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if mce_assets is not None: mce_assets.to_excel(writer, sheet_name='Assets_MCEG', index=False)\n",
    "        if mce_liabilities is not None: mce_liabilities.to_excel(writer, sheet_name='Liabilities_MCEG', index=False)\n",
    "        cutoffs.to_excel(writer, sheet_name='Quartile_Cutoffs', index=False)\n",
    "\n",
    "    print(f\"Exported Household_Unweighted_Compositions_{year}.xlsx\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "datasets = [(df_1992, 1992), (df_2003, 2003), (df_2013, 2013), (df_2019, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_unweighted(df, year)\n",
    "print(\"✅ All unweighted files exported.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7048331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions_1992.xlsx\n",
      "Exported Household_Compositions_2003.xlsx\n",
      "Exported Household_Compositions_2013.xlsx\n",
      "Exported Household_Compositions_2019.xlsx\n",
      "✅ All files exported.\n"
     ]
    }
   ],
   "source": [
    "df_1992 = pd.read_stata(fp_1992)\n",
    "df_2003 = pd.read_stata(fp_2003)\n",
    "df_2013 = pd.read_stata(fp_2013)\n",
    "df_2019 = pd.read_stata(fp_2019)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups --------------------\n",
    "def assign_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col=None):\n",
    "    # Collapse to HHID level\n",
    "    group_dict = {wealth_col: ('Gross_wealth', 'sum')}\n",
    "    if mce_col in df.columns:\n",
    "        group_dict[mce_col] = ('MCE', 'sum')\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        group_dict[weight_col] = ('Weight', 'first')\n",
    "\n",
    "    hh = df.groupby('HHID', as_index=False).agg(**group_dict)\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_wealth_unweighted = hh['Gross_wealth'].quantile([0.25, 0.5, 0.75])\n",
    "    hh['WealthGroup'] = pd.cut(\n",
    "        hh['Gross_wealth'],\n",
    "        [-np.inf, *cuts_wealth_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    if weight_col and weight_col in hh.columns:\n",
    "        cuts_wealth_weighted = weighted_percentile(hh['Gross_wealth'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "        hh['WealthGroup_weighted'] = pd.cut(\n",
    "            hh['Gross_wealth'],\n",
    "            [-np.inf, *cuts_wealth_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_wealth_weighted = None\n",
    "        hh['WealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (only if MCE exists)\n",
    "    if 'MCE' in hh.columns:\n",
    "        cuts_mce_unweighted = hh['MCE'].quantile([0.25, 0.5, 0.75])\n",
    "        hh['MCEGroup'] = pd.cut(\n",
    "            hh['MCE'],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in hh.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hh['MCE'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "            hh['MCEGroup_weighted'] = pd.cut(\n",
    "                hh['MCE'],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hh['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "        hh['MCEGroup'] = None\n",
    "        hh['MCEGroup_weighted'] = None\n",
    "\n",
    "    return hh[['HHID','WealthGroup','WealthGroup_weighted','MCEGroup','MCEGroup_weighted']], \\\n",
    "           cuts_wealth_unweighted, cuts_wealth_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, category_col, value_col, use_weight=False):\n",
    "    # Collapse to HHID-category level\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False).agg(\n",
    "        HH_Category_Value=(value_col, 'sum'),\n",
    "        Weight=('Weight', 'first') if 'Weight' in df.columns else ('HHID', 'size')\n",
    "    )\n",
    "    if use_weight and 'Weight' in hh_cat.columns:\n",
    "        hh_cat['HH_Category_Value'] = hh_cat['HH_Category_Value'] * hh_cat['Weight']\n",
    "\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Group by category & quartile\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(\n",
    "            TotalValue=('HH_Category_Value', 'sum'),\n",
    "            HouseholdCount=('HHID','nunique')\n",
    "        ).reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)['HH_Category_Value'].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Assign quartiles\n",
    "    hhid_groups, w_cuts, w_cuts_wt, mce_cuts, mce_cuts_wt = assign_quartiles(df, weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Wealth-based composition\n",
    "    wealth_assets = compute_composition(df, 'WealthGroup', 'Asset_Name', 'Asset_Value', use_weight=False)\n",
    "    wealth_assets_wt = compute_composition(df, 'WealthGroup_weighted', 'Asset_Name', 'Asset_Value', use_weight=True) if has_weight else None\n",
    "    wealth_liabilities = compute_composition(df, 'WealthGroup', 'Liability_name', 'Liability_value', use_weight=False)\n",
    "    wealth_liabilities_wt = compute_composition(df, 'WealthGroup_weighted', 'Liability_name', 'Liability_value', use_weight=True) if has_weight else None\n",
    "\n",
    "    # MCE-based composition (only if MCE exists)\n",
    "    if 'MCEGroup' in hhid_groups.columns and hhid_groups['MCEGroup'].notnull().any():\n",
    "        mce_assets = compute_composition(df, 'MCEGroup', 'Asset_Name', 'Asset_Value', use_weight=False)\n",
    "        mce_assets_wt = compute_composition(df, 'MCEGroup_weighted', 'Asset_Name', 'Asset_Value', use_weight=True) if has_weight else None\n",
    "        mce_liabilities = compute_composition(df, 'MCEGroup', 'Liability_name', 'Liability_value', use_weight=False)\n",
    "        mce_liabilities_wt = compute_composition(df, 'MCEGroup_weighted', 'Liability_name', 'Liability_value', use_weight=True) if has_weight else None\n",
    "    else:\n",
    "        mce_assets = mce_assets_wt = mce_liabilities = mce_liabilities_wt = None\n",
    "\n",
    "    # Quartile cutoffs\n",
    "    cutoffs = pd.DataFrame({\n",
    "        'Percentile': ['25th','50th','75th'],\n",
    "        'Wealth_Unweighted': w_cuts.values,\n",
    "        'Wealth_Weighted': w_cuts_wt if w_cuts_wt is not None else [None,None,None],\n",
    "        'MCE_Unweighted': mce_cuts.values if mce_cuts is not None else [None,None,None],\n",
    "        'MCE_Weighted': mce_cuts_wt if mce_cuts_wt is not None else [None,None,None]\n",
    "    })\n",
    "\n",
    "    # Export to Excel\n",
    "    with pd.ExcelWriter(f'Household_Compositions22_{year}.xlsx') as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        if wealth_assets_wt is not None: wealth_assets_wt.to_excel(writer, sheet_name='Assets_WealthG_weighted', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if wealth_liabilities_wt is not None: wealth_liabilities_wt.to_excel(writer, sheet_name='Liabilities_WealthG_weighted', index=False)\n",
    "        if mce_assets is not None: mce_assets.to_excel(writer, sheet_name='Assets_MCEG', index=False)\n",
    "        if mce_assets_wt is not None: mce_assets_wt.to_excel(writer, sheet_name='Assets_MCEG_weighted', index=False)\n",
    "        if mce_liabilities is not None: mce_liabilities.to_excel(writer, sheet_name='Liabilities_MCEG', index=False)\n",
    "        if mce_liabilities_wt is not None: mce_liabilities_wt.to_excel(writer, sheet_name='Liabilities_MCEG_weighted', index=False)\n",
    "        cutoffs.to_excel(writer, sheet_name='Quartile_Cutoffs', index=False)\n",
    "    print(f\"Exported Household_Compositions_{year}.xlsx\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "datasets = [(df_1992, 1992), (df_2003, 2003), (df_2013, 2013), (df_2019, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year)\n",
    "print(\"✅ All files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4777f7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:222: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  totals_summary.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:223: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  breakdown.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Components_Breakdown', index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions_1992.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:222: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  totals_summary.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:223: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  breakdown.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Components_Breakdown', index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions_2003.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:222: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  totals_summary.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:223: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  breakdown.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Components_Breakdown', index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions_2013.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:222: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  totals_summary.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_65048\\3079377234.py:223: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  breakdown.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Components_Breakdown', index=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported Household_Compositions_2019.xlsx\n",
      "✅ All files exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_1992 = pd.read_stata(fp_1992)\n",
    "df_2003 = pd.read_stata(fp_2003)\n",
    "df_2013 = pd.read_stata(fp_2013)\n",
    "df_2019 = pd.read_stata(fp_2019)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups --------------------\n",
    "def assign_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col=None):\n",
    "    # Collapse to HHID level\n",
    "    group_dict = {wealth_col: ('Gross_wealth', 'first')}\n",
    "    if mce_col in df.columns:\n",
    "        group_dict[mce_col] = ('MCE', 'first')\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        group_dict[weight_col] = ('Weight', 'first')\n",
    "\n",
    "    hh = df.groupby('HHID', as_index=False).agg(**group_dict)\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_wealth_unweighted = hh['Gross_wealth'].quantile([0.25, 0.5, 0.75])\n",
    "    hh['WealthGroup'] = pd.cut(\n",
    "        hh['Gross_wealth'],\n",
    "        [-np.inf, *cuts_wealth_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    if weight_col and weight_col in hh.columns:\n",
    "        cuts_wealth_weighted = weighted_percentile(hh['Gross_wealth'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "        hh['WealthGroup_weighted'] = pd.cut(\n",
    "            hh['Gross_wealth'],\n",
    "            [-np.inf, *cuts_wealth_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_wealth_weighted = None\n",
    "        hh['WealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (only if MCE exists)\n",
    "    if 'MCE' in hh.columns:\n",
    "        cuts_mce_unweighted = hh['MCE'].quantile([0.25, 0.5, 0.75])\n",
    "        hh['MCEGroup'] = pd.cut(\n",
    "            hh['MCE'],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in hh.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hh['MCE'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "            hh['MCEGroup_weighted'] = pd.cut(\n",
    "                hh['MCE'],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hh['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "        hh['MCEGroup'] = None\n",
    "        hh['MCEGroup_weighted'] = None\n",
    "\n",
    "    return hh[['HHID','WealthGroup','WealthGroup_weighted','MCEGroup','MCEGroup_weighted']], \\\n",
    "           cuts_wealth_unweighted, cuts_wealth_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, category_col, value_col, use_weight=False):\n",
    "    # Collapse to HHID-category level\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False).agg(\n",
    "        HH_Category_Value=(value_col, 'first'),\n",
    "        Weight=('Weight', 'first') if 'Weight' in df.columns else ('HHID', 'size')\n",
    "    )\n",
    "    if use_weight and 'Weight' in hh_cat.columns:\n",
    "        hh_cat['HH_Category_Value'] = hh_cat['HH_Category_Value'] * hh_cat['Weight']\n",
    "\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Group by category & quartile\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(\n",
    "            TotalValue=('HH_Category_Value', 'sum'),\n",
    "            HouseholdCount=('HHID','nunique')\n",
    "        ).reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)['HH_Category_Value'].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "    return merged\n",
    "\n",
    "# -------------------- Totals Summary --------------------\n",
    "def compute_totals(df):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # --- Collapse to HHID + component first (household-level) ---\n",
    "    assets_cat = df.groupby(['HHID','Asset_Name'], as_index=False).agg(\n",
    "        Unweighted_Value=('Asset_Value', 'first'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID','size')\n",
    "    )\n",
    "    liabilities_cat = df.groupby(['HHID','Liability_name'], as_index=False).agg(\n",
    "        Unweighted_Value=('Liability_value', 'first'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID','size')\n",
    "    )\n",
    "\n",
    "    # --- Compute weighted at household level ---\n",
    "    if has_weight:\n",
    "        assets_cat['Weighted_Value'] = assets_cat['Unweighted_Value'] * assets_cat['Weight']\n",
    "        liabilities_cat['Weighted_Value'] = liabilities_cat['Unweighted_Value'] * liabilities_cat['Weight']\n",
    "    else:\n",
    "        assets_cat['Weighted_Value'] = None\n",
    "        liabilities_cat['Weighted_Value'] = None\n",
    "\n",
    "    # --- Collapse to component level ---\n",
    "    assets_comp = assets_cat.groupby('Asset_Name', as_index=False).agg(\n",
    "        Unweighted_Total=('Unweighted_Value','sum'),\n",
    "        Weighted_Total=('Weighted_Value','sum')\n",
    "    )\n",
    "    liabilities_comp = liabilities_cat.groupby('Liability_name', as_index=False).agg(\n",
    "        Unweighted_Total=('Unweighted_Value','sum'),\n",
    "        Weighted_Total=('Weighted_Value','sum')\n",
    "    )\n",
    "\n",
    "    assets_comp['Type'] = 'Asset'\n",
    "    liabilities_comp['Type'] = 'Liability'\n",
    "    components = pd.concat([\n",
    "        assets_comp.rename(columns={'Asset_Name':'Component'}),\n",
    "        liabilities_comp.rename(columns={'Liability_name':'Component'})\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # --- Grand totals at HHID level ---\n",
    "    hh_agg = df.groupby('HHID', as_index=False).agg(\n",
    "        Assets=('Asset_Value','sum'),\n",
    "        Liabilities=('Liability_value','sum'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID','size')\n",
    "    )\n",
    "    hh_agg['NetWealth'] = hh_agg['Assets'] - hh_agg['Liabilities']\n",
    "\n",
    "    if has_weight:\n",
    "        hh_agg['Assets_wt'] = hh_agg['Assets'] * hh_agg['Weight']\n",
    "        hh_agg['Liabilities_wt'] = hh_agg['Liabilities'] * hh_agg['Weight']\n",
    "        hh_agg['NetWealth_wt'] = hh_agg['NetWealth'] * hh_agg['Weight']\n",
    "        summary = pd.DataFrame([{\n",
    "            'Households': hh_agg['HHID'].nunique(),\n",
    "            'Total_Assets_Unweighted': hh_agg['Assets'].sum(),\n",
    "            'Total_Assets_Weighted': hh_agg['Assets_wt'].sum(),\n",
    "            'Total_Liabilities_Unweighted': hh_agg['Liabilities'].sum(),\n",
    "            'Total_Liabilities_Weighted': hh_agg['Liabilities_wt'].sum(),\n",
    "            'Net_Wealth_Unweighted': hh_agg['NetWealth'].sum(),\n",
    "            'Net_Wealth_Weighted': hh_agg['NetWealth_wt'].sum()\n",
    "        }])\n",
    "    else:\n",
    "        summary = pd.DataFrame([{\n",
    "            'Households': hh_agg['HHID'].nunique(),\n",
    "            'Total_Assets_Unweighted': hh_agg['Assets'].sum(),\n",
    "            'Total_Assets_Weighted': None,\n",
    "            'Total_Liabilities_Unweighted': hh_agg['Liabilities'].sum(),\n",
    "            'Total_Liabilities_Weighted': None,\n",
    "            'Net_Wealth_Unweighted': hh_agg['NetWealth'].sum(),\n",
    "            'Net_Wealth_Weighted': None\n",
    "        }])\n",
    "\n",
    "    return summary, components\n",
    "\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Assign quartiles\n",
    "    hhid_groups, w_cuts, w_cuts_wt, mce_cuts, mce_cuts_wt = assign_quartiles(df, weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Wealth-based composition\n",
    "    wealth_assets = compute_composition(df, 'WealthGroup', 'Asset_Name', 'Asset_Value', use_weight=False)\n",
    "    wealth_assets_wt = compute_composition(df, 'WealthGroup_weighted', 'Asset_Name', 'Asset_Value', use_weight=True) if has_weight else None\n",
    "    wealth_liabilities = compute_composition(df, 'WealthGroup', 'Liability_name', 'Liability_value', use_weight=False)\n",
    "    wealth_liabilities_wt = compute_composition(df, 'WealthGroup_weighted', 'Liability_name', 'Liability_value', use_weight=True) if has_weight else None\n",
    "\n",
    "    # MCE-based composition (only if MCE exists)\n",
    "    if 'MCEGroup' in hhid_groups.columns and hhid_groups['MCEGroup'].notnull().any():\n",
    "        mce_assets = compute_composition(df, 'MCEGroup', 'Asset_Name', 'Asset_Value', use_weight=False)\n",
    "        mce_assets_wt = compute_composition(df, 'MCEGroup_weighted', 'Asset_Name', 'Asset_Value', use_weight=True) if has_weight else None\n",
    "        mce_liabilities = compute_composition(df, 'MCEGroup', 'Liability_name', 'Liability_value', use_weight=False)\n",
    "        mce_liabilities_wt = compute_composition(df, 'MCEGroup_weighted', 'Liability_name', 'Liability_value', use_weight=True) if has_weight else None\n",
    "    else:\n",
    "        mce_assets = mce_assets_wt = mce_liabilities = mce_liabilities_wt = None\n",
    "\n",
    "    # Quartile cutoffs\n",
    "    cutoffs = pd.DataFrame({\n",
    "        'Percentile': ['25th','50th','75th'],\n",
    "        'Wealth_Unweighted': w_cuts.values,\n",
    "        'Wealth_Weighted': w_cuts_wt if w_cuts_wt is not None else [None,None,None],\n",
    "        'MCE_Unweighted': mce_cuts.values if mce_cuts is not None else [None,None,None],\n",
    "        'MCE_Weighted': mce_cuts_wt if mce_cuts_wt is not None else [None,None,None]\n",
    "    })\n",
    "\n",
    "    # In process_dataset():\n",
    "    totals_summary, breakdown = compute_totals(df)\n",
    "\n",
    "    # Export to Excel\n",
    "    with pd.ExcelWriter(f'Household_Compositions_{year}.xlsx') as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        if wealth_assets_wt is not None: wealth_assets_wt.to_excel(writer, sheet_name='Assets_WealthG_weighted', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if wealth_liabilities_wt is not None: wealth_liabilities_wt.to_excel(writer, sheet_name='Liabilities_WealthG_weighted', index=False)\n",
    "        if mce_assets is not None: mce_assets.to_excel(writer, sheet_name='Assets_MCEG', index=False)\n",
    "        if mce_assets_wt is not None: mce_assets_wt.to_excel(writer, sheet_name='Assets_MCEG_weighted', index=False)\n",
    "        if mce_liabilities is not None: mce_liabilities.to_excel(writer, sheet_name='Liabilities_MCEG', index=False)\n",
    "        if mce_liabilities_wt is not None: mce_liabilities_wt.to_excel(writer, sheet_name='Liabilities_MCEG_weighted', index=False)\n",
    "        cutoffs.to_excel(writer, sheet_name='Quartile_Cutoffs', index=False)\n",
    "        totals_summary.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
    "        breakdown.applymap(lambda x: f\"{x:.0f}\" if pd.notnull(x) and isinstance(x,(int,float)) else x).to_excel(writer, sheet_name='Components_Breakdown', index=False)\n",
    "    print(f\"Exported Household_Compositions_{year}.xlsx\")\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "datasets = [(df_1992, 1992), (df_2003, 2003), (df_2013, 2013), (df_2019, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year)\n",
    "print(\"✅ All files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a67a2449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for Asset_Name in WealthGroup: all sums to 100.\n",
      "Check for Liability_name in WealthGroup: all sums to 100.\n",
      "Check for Asset_Name in MCEGroup: all sums to 100.\n",
      "Check for Liability_name in MCEGroup: all sums to 100.\n",
      "Exported Household_Compositions_Files\\Household_Compositions111_1992.xlsx\n",
      "Check for Asset_Name in WealthGroup: all sums to 100.\n",
      "Check for Asset_Name in WealthGroup_weighted: all sums to 100.\n",
      "Check for Liability_name in WealthGroup: all sums to 100.\n",
      "Check for Liability_name in WealthGroup_weighted: all sums to 100.\n",
      "Check for Asset_Name in MCEGroup: all sums to 100.\n",
      "Check for Asset_Name in MCEGroup_weighted: all sums to 100.\n",
      "Check for Liability_name in MCEGroup: all sums to 100.\n",
      "Check for Liability_name in MCEGroup_weighted: all sums to 100.\n",
      "Exported Household_Compositions_Files\\Household_Compositions111_2003.xlsx\n",
      "Check for Asset_Name in WealthGroup: all sums to 100.\n",
      "Check for Asset_Name in WealthGroup_weighted: all sums to 100.\n",
      "Check for Liability_name in WealthGroup: all sums to 100.\n",
      "Check for Liability_name in WealthGroup_weighted: all sums to 100.\n",
      "Exported Household_Compositions_Files\\Household_Compositions111_2013.xlsx\n",
      "Check for Asset_Name in WealthGroup: all sums to 100.\n",
      "Check for Asset_Name in WealthGroup_weighted: all sums to 100.\n",
      "Check for Liability_name in WealthGroup: all sums to 100.\n",
      "Check for Liability_name in WealthGroup_weighted: all sums to 100.\n",
      "Check for Asset_Name in MCEGroup: all sums to 100.\n",
      "Check for Asset_Name in MCEGroup_weighted: all sums to 100.\n",
      "Check for Liability_name in MCEGroup: all sums to 100.\n",
      "Check for Liability_name in MCEGroup_weighted: all sums to 100.\n",
      "Exported Household_Compositions_Files\\Household_Compositions111_2019.xlsx\n",
      "✅ All files exported.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_folder = \"Household_Compositions_Files\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Create folder if it doesn't exist\n",
    "\n",
    "df_1992 = pd.read_stata(fp_1992)\n",
    "df_2003 = pd.read_stata(fp_2003)\n",
    "df_2013 = pd.read_stata(fp_2013)\n",
    "df_2019 = pd.read_stata(fp_2019)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups --------------------\n",
    "def assign_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col=None):\n",
    "    # Collapse to HHID level\n",
    "    group_dict = {wealth_col: ('Gross_wealth', 'first')}\n",
    "    if mce_col in df.columns:\n",
    "        group_dict[mce_col] = ('MCE', 'first')\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        group_dict[weight_col] = ('Weight', 'first')\n",
    "\n",
    "    hh = df.groupby('HHID', as_index=False).agg(**group_dict)\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_wealth_unweighted = hh['Gross_wealth'].quantile([0.25, 0.5, 0.75])\n",
    "    hh['WealthGroup'] = pd.cut(\n",
    "        hh['Gross_wealth'],\n",
    "        [-np.inf, *cuts_wealth_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "\n",
    "    if weight_col and weight_col in hh.columns:\n",
    "        cuts_wealth_weighted = weighted_percentile(hh['Gross_wealth'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "        hh['WealthGroup_weighted'] = pd.cut(\n",
    "            hh['Gross_wealth'],\n",
    "            [-np.inf, *cuts_wealth_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_wealth_weighted = None\n",
    "        hh['WealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (only if MCE exists)\n",
    "    if 'MCE' in hh.columns:\n",
    "        cuts_mce_unweighted = hh['MCE'].quantile([0.25, 0.5, 0.75])\n",
    "        hh['MCEGroup'] = pd.cut(\n",
    "            hh['MCE'],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in hh.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hh['MCE'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "            hh['MCEGroup_weighted'] = pd.cut(\n",
    "                hh['MCE'],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hh['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "        hh['MCEGroup'] = None\n",
    "        hh['MCEGroup_weighted'] = None\n",
    "\n",
    "    return hh[['HHID','WealthGroup','WealthGroup_weighted','MCEGroup','MCEGroup_weighted']], \\\n",
    "           cuts_wealth_unweighted, cuts_wealth_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, category_col, value_col, use_weight=False):\n",
    "    # Collapse to HHID-category level\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False).agg(\n",
    "        HH_Category_Value=(value_col, 'first'),\n",
    "        Weight=('Weight', 'first') if 'Weight' in df.columns else ('HHID', 'size')\n",
    "    )\n",
    "    if use_weight and 'Weight' in hh_cat.columns:\n",
    "        hh_cat['HH_Category_Value'] = hh_cat['HH_Category_Value'] * hh_cat['Weight']\n",
    "\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Group by category & quartile\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(\n",
    "            TotalValue=('HH_Category_Value', 'sum'),\n",
    "            HouseholdCount=('HHID','nunique')\n",
    "        ).reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)['HH_Category_Value'].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "\n",
    "    # Initial percentage calculation\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "\n",
    "    # --- Fix rounding so totals = 100 ---\n",
    "    def fix_percentages(df, category_col, group_col):\n",
    "        for cat, group in df.groupby(category_col):\n",
    "            diff = round(100 - group['PercentageShare'].sum(), 2)\n",
    "            if abs(diff) > 0.001:  # only adjust if there's a difference\n",
    "                if '75-100' in group[group_col].values:\n",
    "                    idx_last = group[group[group_col] == '75-100'].index\n",
    "                else:\n",
    "                    idx_last = group.index[-1]  # fallback: last group\n",
    "                df.loc[idx_last, 'PercentageShare'] += diff\n",
    "        return df\n",
    "\n",
    "    merged = fix_percentages(merged, category_col, group_col)\n",
    "\n",
    "    # --- Check if sums are now 100 ---\n",
    "    check = merged.groupby(category_col)['PercentageShare'].sum().reset_index()\n",
    "    check.columns = [category_col, 'Sum_Percent']\n",
    "    not_100 = check[np.abs(check['Sum_Percent'] - 100) > 0.01]\n",
    "    if not not_100.empty:\n",
    "        print(f\"⚠️ Warning: These {category_col} in {group_col} still don't sum to 100:\\n\", not_100)\n",
    "    else:\n",
    "        print(f\"Check for {category_col} in {group_col}: all sums to 100.\")\n",
    "\n",
    "    return merged\n",
    "\n",
    "\n",
    "# -------------------- Totals Summary --------------------\n",
    "def compute_totals(df):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # --- Collapse to HHID + component first (household-level) ---\n",
    "    assets_cat = df.groupby(['HHID','Asset_Name'], as_index=False).agg(\n",
    "        Unweighted_Value=('Asset_Value', 'first'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID','size')\n",
    "    )\n",
    "    liabilities_cat = df.groupby(['HHID','Liability_name'], as_index=False).agg(\n",
    "        Unweighted_Value=('Liability_value', 'first'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID','size')\n",
    "    )\n",
    "\n",
    "    # --- Compute weighted at household level ---\n",
    "    if has_weight:\n",
    "        assets_cat['Weighted_Value'] = assets_cat['Unweighted_Value'] * assets_cat['Weight']\n",
    "        liabilities_cat['Weighted_Value'] = liabilities_cat['Unweighted_Value'] * liabilities_cat['Weight']\n",
    "    else:\n",
    "        assets_cat['Weighted_Value'] = None\n",
    "        liabilities_cat['Weighted_Value'] = None\n",
    "\n",
    "    # --- Collapse to component level ---\n",
    "    assets_comp = assets_cat.groupby('Asset_Name', as_index=False).agg(\n",
    "        Unweighted_Total=('Unweighted_Value','sum'),\n",
    "        Weighted_Total=('Weighted_Value','sum')\n",
    "    )\n",
    "    liabilities_comp = liabilities_cat.groupby('Liability_name', as_index=False).agg(\n",
    "        Unweighted_Total=('Unweighted_Value','sum'),\n",
    "        Weighted_Total=('Weighted_Value','sum')\n",
    "    )\n",
    "\n",
    "    assets_comp['Type'] = 'Asset'\n",
    "    liabilities_comp['Type'] = 'Liability'\n",
    "    components = pd.concat([\n",
    "        assets_comp.rename(columns={'Asset_Name':'Component'}),\n",
    "        liabilities_comp.rename(columns={'Liability_name':'Component'})\n",
    "    ], ignore_index=True)\n",
    "\n",
    "    # --- Grand totals at HHID level ---\n",
    "    hh_agg = df.groupby('HHID', as_index=False).agg(\n",
    "        Assets=('Asset_Value','sum'),\n",
    "        Liabilities=('Liability_value','sum'),\n",
    "        Weight=('Weight','first') if has_weight else ('HHID','size')\n",
    "    )\n",
    "    hh_agg['NetWealth'] = hh_agg['Assets'] - hh_agg['Liabilities']\n",
    "\n",
    "    if has_weight:\n",
    "        hh_agg['Assets_wt'] = hh_agg['Assets'] * hh_agg['Weight']\n",
    "        hh_agg['Liabilities_wt'] = hh_agg['Liabilities'] * hh_agg['Weight']\n",
    "        hh_agg['NetWealth_wt'] = hh_agg['NetWealth'] * hh_agg['Weight']\n",
    "        summary = pd.DataFrame([{\n",
    "            'Households': hh_agg['HHID'].nunique(),\n",
    "            'Total_Assets_Unweighted': hh_agg['Assets'].sum(),\n",
    "            'Total_Assets_Weighted': hh_agg['Assets_wt'].sum(),\n",
    "            'Total_Liabilities_Unweighted': hh_agg['Liabilities'].sum(),\n",
    "            'Total_Liabilities_Weighted': hh_agg['Liabilities_wt'].sum(),\n",
    "            'Net_Wealth_Unweighted': hh_agg['NetWealth'].sum(),\n",
    "            'Net_Wealth_Weighted': hh_agg['NetWealth_wt'].sum()\n",
    "        }])\n",
    "    else:\n",
    "        summary = pd.DataFrame([{\n",
    "            'Households': hh_agg['HHID'].nunique(),\n",
    "            'Total_Assets_Unweighted': hh_agg['Assets'].sum(),\n",
    "            'Total_Assets_Weighted': None,\n",
    "            'Total_Liabilities_Unweighted': hh_agg['Liabilities'].sum(),\n",
    "            'Total_Liabilities_Weighted': None,\n",
    "            'Net_Wealth_Unweighted': hh_agg['NetWealth'].sum(),\n",
    "            'Net_Wealth_Weighted': None\n",
    "        }])\n",
    "\n",
    "    return summary, components\n",
    "\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Assign quartiles\n",
    "    hhid_groups, w_cuts, w_cuts_wt, mce_cuts, mce_cuts_wt = assign_quartiles(df, weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Wealth-based composition\n",
    "    wealth_assets = compute_composition(df, 'WealthGroup', 'Asset_Name', 'Asset_Value', use_weight=False)\n",
    "    wealth_assets_wt = compute_composition(df, 'WealthGroup_weighted', 'Asset_Name', 'Asset_Value', use_weight=True) if has_weight else None\n",
    "    wealth_liabilities = compute_composition(df, 'WealthGroup', 'Liability_name', 'Liability_value', use_weight=False)\n",
    "    wealth_liabilities_wt = compute_composition(df, 'WealthGroup_weighted', 'Liability_name', 'Liability_value', use_weight=True) if has_weight else None\n",
    "\n",
    "    # MCE-based composition (only if MCE exists)\n",
    "    if 'MCEGroup' in hhid_groups.columns and hhid_groups['MCEGroup'].notnull().any():\n",
    "        mce_assets = compute_composition(df, 'MCEGroup', 'Asset_Name', 'Asset_Value', use_weight=False)\n",
    "        mce_assets_wt = compute_composition(df, 'MCEGroup_weighted', 'Asset_Name', 'Asset_Value', use_weight=True) if has_weight else None\n",
    "        mce_liabilities = compute_composition(df, 'MCEGroup', 'Liability_name', 'Liability_value', use_weight=False)\n",
    "        mce_liabilities_wt = compute_composition(df, 'MCEGroup_weighted', 'Liability_name', 'Liability_value', use_weight=True) if has_weight else None\n",
    "    else:\n",
    "        mce_assets = mce_assets_wt = mce_liabilities = mce_liabilities_wt = None\n",
    "\n",
    "    # Quartile cutoffs\n",
    "    cutoffs = pd.DataFrame({\n",
    "        'Percentile': ['25th','50th','75th'],\n",
    "        'Wealth_Unweighted': w_cuts.values,\n",
    "        'Wealth_Weighted': w_cuts_wt if w_cuts_wt is not None else [None,None,None],\n",
    "        'MCE_Unweighted': mce_cuts.values if mce_cuts is not None else [None,None,None],\n",
    "        'MCE_Weighted': mce_cuts_wt if mce_cuts_wt is not None else [None,None,None]\n",
    "    })\n",
    "\n",
    "    # Totals and breakdown\n",
    "    totals_summary, breakdown = compute_totals(df)\n",
    "\n",
    "    # File path in the new folder\n",
    "    file_path = os.path.join(output_folder, f'Household_Compositions111_{year}.xlsx')\n",
    "\n",
    "    # Export to Excel\n",
    "    with pd.ExcelWriter(file_path) as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        if wealth_assets_wt is not None: wealth_assets_wt.to_excel(writer, sheet_name='Assets_WealthG_weighted', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if wealth_liabilities_wt is not None: wealth_liabilities_wt.to_excel(writer, sheet_name='Liabilities_WealthG_weighted', index=False)\n",
    "        if mce_assets is not None: mce_assets.to_excel(writer, sheet_name='Assets_MCEG', index=False)\n",
    "        if mce_assets_wt is not None: mce_assets_wt.to_excel(writer, sheet_name='Assets_MCEG_weighted', index=False)\n",
    "        if mce_liabilities is not None: mce_liabilities.to_excel(writer, sheet_name='Liabilities_MCEG', index=False)\n",
    "        if mce_liabilities_wt is not None: mce_liabilities_wt.to_excel(writer, sheet_name='Liabilities_MCEG_weighted', index=False)\n",
    "        cutoffs.to_excel(writer, sheet_name='Quartile_Cutoffs', index=False)\n",
    "\n",
    "        # Format numeric columns (avoid warnings)\n",
    "        for col in totals_summary.columns:\n",
    "            if totals_summary[col].dtype in [np.float64, np.int64]:\n",
    "                totals_summary[col] = totals_summary[col].map(lambda x: f\"{x:.0f}\" if pd.notnull(x) else x)\n",
    "        for col in breakdown.columns:\n",
    "            if breakdown[col].dtype in [np.float64, np.int64]:\n",
    "                breakdown[col] = breakdown[col].map(lambda x: f\"{x:.0f}\" if pd.notnull(x) else x)\n",
    "\n",
    "        totals_summary.to_excel(writer, sheet_name='Totals_Summary', index=False)\n",
    "        breakdown.to_excel(writer, sheet_name='Components_Breakdown', index=False)\n",
    "\n",
    "    print(f\"Exported {file_path}\")\n",
    "\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "datasets = [(df_1992, 1992), (df_2003, 2003), (df_2013, 2013), (df_2019, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year)\n",
    "print(\"✅ All files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb7ccc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Long format file exported: Household_Compositions_Files\\Household_Compositions_Long_Format.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "output_folder = \"Household_Compositions_Files\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "df_1992 = pd.read_stata(fp_1992)\n",
    "df_2003 = pd.read_stata(fp_2003)\n",
    "df_2013 = pd.read_stata(fp_2013)\n",
    "df_2019 = pd.read_stata(fp_2019)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups --------------------\n",
    "def assign_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col=None):\n",
    "    group_dict = {wealth_col: ('Gross_wealth', 'first')}\n",
    "    if mce_col in df.columns:\n",
    "        group_dict[mce_col] = ('MCE', 'first')\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        group_dict[weight_col] = ('Weight', 'first')\n",
    "\n",
    "    hh = df.groupby('HHID', as_index=False).agg(**group_dict)\n",
    "    cuts_wealth_unweighted = hh['Gross_wealth'].quantile([0.25, 0.5, 0.75])\n",
    "    hh['WealthGroup'] = pd.cut(hh['Gross_wealth'],\n",
    "        [-np.inf, *cuts_wealth_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "    if weight_col and weight_col in hh.columns:\n",
    "        cuts_wealth_weighted = weighted_percentile(hh['Gross_wealth'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "        hh['WealthGroup_weighted'] = pd.cut(hh['Gross_wealth'],\n",
    "            [-np.inf, *cuts_wealth_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "    else:\n",
    "        cuts_wealth_weighted = None\n",
    "        hh['WealthGroup_weighted'] = None\n",
    "\n",
    "    if 'MCE' in hh.columns:\n",
    "        cuts_mce_unweighted = hh['MCE'].quantile([0.25, 0.5, 0.75])\n",
    "        hh['MCEGroup'] = pd.cut(hh['MCE'],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "        if weight_col and weight_col in hh.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hh['MCE'], hh['Weight'], [0.25, 0.5, 0.75])\n",
    "            hh['MCEGroup_weighted'] = pd.cut(hh['MCE'],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hh['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "        hh['MCEGroup'] = None\n",
    "        hh['MCEGroup_weighted'] = None\n",
    "\n",
    "    return hh[['HHID','WealthGroup','WealthGroup_weighted','MCEGroup','MCEGroup_weighted']], \\\n",
    "           cuts_wealth_unweighted, cuts_wealth_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, category_col, value_col, use_weight=False):\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False).agg(\n",
    "        HH_Category_Value=(value_col, 'first'),\n",
    "        Weight=('Weight', 'first') if 'Weight' in df.columns else ('HHID', 'size')\n",
    "    )\n",
    "    if use_weight and 'Weight' in hh_cat.columns:\n",
    "        hh_cat['HH_Category_Value'] *= hh_cat['Weight']\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(TotalValue=('HH_Category_Value', 'sum')).reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)['HH_Category_Value'].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "\n",
    "    # Fix rounding\n",
    "    for cat, group in merged.groupby(category_col):\n",
    "        diff = round(100 - group['PercentageShare'].sum(), 2)\n",
    "        if abs(diff) > 0.001:\n",
    "            idx_last = group[group[group_col] == '75-100'].index if '75-100' in group[group_col].values else group.index[-1]\n",
    "            merged.loc[idx_last, 'PercentageShare'] += diff\n",
    "\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year, long_format_rows):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    hhid_groups, _, _, _, _ = assign_quartiles(df, weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Define combinations to process\n",
    "    combos = [\n",
    "        ('WealthGroup', 'Asset_Name', 'Asset_Value', False, 'Assets_WealthG'),\n",
    "        ('WealthGroup_weighted', 'Asset_Name', 'Asset_Value', True, 'Assets_WealthG_weighted'),\n",
    "        ('WealthGroup', 'Liability_name', 'Liability_value', False, 'Liabilities_WealthG'),\n",
    "        ('WealthGroup_weighted', 'Liability_name', 'Liability_value', True, 'Liabilities_WealthG_weighted'),\n",
    "        ('MCEGroup', 'Asset_Name', 'Asset_Value', False, 'Assets_MCEG'),\n",
    "        ('MCEGroup_weighted', 'Asset_Name', 'Asset_Value', True, 'Assets_MCEG_weighted'),\n",
    "        ('MCEGroup', 'Liability_name', 'Liability_value', False, 'Liabilities_MCEG'),\n",
    "        ('MCEGroup_weighted', 'Liability_name', 'Liability_value', True, 'Liabilities_MCEG_weighted')\n",
    "    ]\n",
    "\n",
    "    for group_col, category_col, value_col, use_weight, label in combos:\n",
    "        if group_col not in df.columns or df[group_col].isnull().all():\n",
    "            continue\n",
    "        comp = compute_composition(df, group_col, category_col, value_col, use_weight)\n",
    "        comp_long = comp.rename(columns={category_col: 'Name', group_col: 'Group'})[['Name','Group','PercentageShare']]\n",
    "        comp_long['Year'] = year\n",
    "        comp_long['CategoryType'] = label\n",
    "        long_format_rows.append(comp_long)\n",
    "\n",
    "# -------------------- Run --------------------\n",
    "long_format_rows = []\n",
    "datasets = [(df_1992, 1992), (df_2003, 2003), (df_2013, 2013), (df_2019, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year, long_format_rows)\n",
    "\n",
    "# Combine into one long DataFrame\n",
    "long_format_df = pd.concat(long_format_rows, ignore_index=True)\n",
    "long_file = os.path.join(output_folder, \"Household_Compositions_Long_Format.xlsx\")\n",
    "long_format_df.to_excel(long_file, index=False)\n",
    "print(f\"✅ Long format file exported: {long_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "173bddcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\\Household_Compositions2_1992.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\\Household_Compositions2_2003.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\\Household_Compositions2_2013.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\\Household_Compositions2_2019.xlsx\n",
      "✅ All files exported.\n"
     ]
    }
   ],
   "source": [
    "# Only names,groups,%ages\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# -------------------- Save Path --------------------\n",
    "EXPORT_PATH = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\"\n",
    "os.makedirs(EXPORT_PATH, exist_ok=True)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(\n",
    "        hhid_df[wealth_col],\n",
    "        [-np.inf, *cuts_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(\n",
    "            hhid_df[wealth_col],\n",
    "            [-np.inf, *cuts_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_weighted = None\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (if present)\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(\n",
    "            hhid_df[mce_col],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(\n",
    "                hhid_df[mce_col],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted'] + \n",
    "                   ([ 'MCEGroup','MCEGroup_weighted'] if mce_col and mce_col in df.columns else [])], \\\n",
    "           cuts_unweighted, cuts_weighted, cuts_mce_unweighted, cuts_mce_weighted\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col):\n",
    "    # Pre-aggregate per HHID per category\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    # Aggregate by group & category\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(TotalValue=(value_col, 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Compute category totals\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "\n",
    "    # Keep only Name, Group, Percentage\n",
    "    merged = merged[[category_col, group_col, 'PercentageShare']]\n",
    "    merged = merged.rename(columns={category_col: \"Name\", group_col: \"Group\"})\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "\n",
    "    # Weighted values\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    # Assign HHID-level quartiles\n",
    "    hhid_groups, w_cuts, w_cuts_wt, mce_cuts, mce_cuts_wt = assign_hhid_quartiles(df, \n",
    "        wealth_col='Gross_wealth', \n",
    "        mce_col='MCE', \n",
    "        weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Composition tables\n",
    "    wealth_assets = compute_composition(df, 'Gross_wealthGroup', 'Asset_Value_weighted', 'Asset_Name')\n",
    "    wealth_assets_wt = compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Value_weighted', 'Asset_Name') if has_weight else None\n",
    "    mce_assets = compute_composition(df, 'MCEGroup', 'Asset_Value_weighted', 'Asset_Name') if 'MCEGroup' in df.columns else None\n",
    "    mce_assets_wt = compute_composition(df, 'MCEGroup_weighted', 'Asset_Value_weighted', 'Asset_Name') if has_weight and 'MCEGroup_weighted' in df.columns else None\n",
    "\n",
    "    wealth_liabilities = compute_composition(df, 'Gross_wealthGroup', 'Liability_Value_weighted', 'Liability_name')\n",
    "    wealth_liabilities_wt = compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_Value_weighted', 'Liability_name') if has_weight else None\n",
    "    mce_liabilities = compute_composition(df, 'MCEGroup', 'Liability_Value_weighted', 'Liability_name') if 'MCEGroup' in df.columns else None\n",
    "    mce_liabilities_wt = compute_composition(df, 'MCEGroup_weighted', 'Liability_Value_weighted', 'Liability_name') if has_weight and 'MCEGroup_weighted' in df.columns else None\n",
    "\n",
    "    # Export to Excel\n",
    "    out_path = os.path.join(EXPORT_PATH, f'Household_Compositions2_{year}.xlsx')\n",
    "    with pd.ExcelWriter(out_path) as writer:\n",
    "        wealth_assets.to_excel(writer, sheet_name='Assets_WealthG', index=False)\n",
    "        if wealth_assets_wt is not None: wealth_assets_wt.to_excel(writer, sheet_name='Assets_WealthG_weighted', index=False)\n",
    "        if mce_assets is not None: mce_assets.to_excel(writer, sheet_name='Assets_MCEG', index=False)\n",
    "        if mce_assets_wt is not None: mce_assets_wt.to_excel(writer, sheet_name='Assets_MCEG_weighted', index=False)\n",
    "        wealth_liabilities.to_excel(writer, sheet_name='Liabilities_WealthG', index=False)\n",
    "        if wealth_liabilities_wt is not None: wealth_liabilities_wt.to_excel(writer, sheet_name='Liabilities_WealthG_weighted', index=False)\n",
    "        if mce_liabilities is not None: mce_liabilities.to_excel(writer, sheet_name='Liabilities_MCEG', index=False)\n",
    "        if mce_liabilities_wt is not None: mce_liabilities_wt.to_excel(writer, sheet_name='Liabilities_MCEG_weighted', index=False)\n",
    "    print(f\"Exported {out_path}\")\n",
    "\n",
    "# -------------------- Run for All Years --------------------\n",
    "datasets = [(df_1992_2, 1992), (df_2003_2, 2003), (df_2013_2, 2013), (df_2019_2, 2019)]\n",
    "for df, year in datasets:\n",
    "    process_dataset(df, year)\n",
    "print(\"✅ All files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "631aacec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'Gross_wealth', 'Liability_serial',\n",
       "       'Liability_name', 'Liability_value', 'Asset_serial', 'Asset_Name',\n",
       "       'Asset_Value', 'State_name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1992.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "6db13c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Assets_WealthG.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Assets_WealthG_weighted.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Liabilities_WealthG.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Liabilities_WealthG_weighted.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Assets_MCEG.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Assets_MCEG_weighted.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Liabilities_MCEG.xlsx\n",
      "Exported C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\\Liabilities_MCEG_weighted.xlsx\n",
      "✅ All 8 files exported.\n"
     ]
    }
   ],
   "source": [
    "# Only names, groups, %ages in 8 separate Excel files\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Save Path --------------------\n",
    "EXPORT_PATH = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww1\"\n",
    "os.makedirs(EXPORT_PATH, exist_ok=True)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(\n",
    "        hhid_df[wealth_col],\n",
    "        [-np.inf, *cuts_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(\n",
    "            hhid_df[wealth_col],\n",
    "            [-np.inf, *cuts_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (if present)\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(\n",
    "            hhid_df[mce_col],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(\n",
    "                hhid_df[mce_col],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted','MCEGroup','MCEGroup_weighted'] if 'MCEGroup' in hhid_df.columns else ['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted']]\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col, year):\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(TotalValue=(value_col, 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "\n",
    "    merged = merged[[category_col, group_col, 'PercentageShare']]\n",
    "    merged = merged.rename(columns={category_col: \"Name\", group_col: \"Group\"})\n",
    "    merged['Year'] = year\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    # Assign HHID-level quartiles\n",
    "    hhid_groups = assign_hhid_quartiles(df, 'Gross_wealth', 'MCE', 'Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Composition outputs\n",
    "    return {\n",
    "        \"Assets_WealthG\": compute_composition(df, 'Gross_wealthGroup', 'Asset_Value_weighted', 'Asset_Name', year),\n",
    "        \"Assets_WealthG_weighted\": compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year) if has_weight else None,\n",
    "        \"Liabilities_WealthG\": compute_composition(df, 'Gross_wealthGroup', 'Liability_Value_weighted', 'Liability_name', year),\n",
    "        \"Liabilities_WealthG_weighted\": compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year) if has_weight else None,\n",
    "        \"Assets_MCEG\": compute_composition(df, 'MCEGroup', 'Asset_Value_weighted', 'Asset_Name', year) if 'MCEGroup' in df.columns else None,\n",
    "        \"Assets_MCEG_weighted\": compute_composition(df, 'MCEGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year) if has_weight and 'MCEGroup_weighted' in df.columns else None,\n",
    "        \"Liabilities_MCEG\": compute_composition(df, 'MCEGroup', 'Liability_Value_weighted', 'Liability_name', year) if 'MCEGroup' in df.columns else None,\n",
    "        \"Liabilities_MCEG_weighted\": compute_composition(df, 'MCEGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year) if has_weight and 'MCEGroup_weighted' in df.columns else None,\n",
    "    }\n",
    "\n",
    "# -------------------- Run for All Years & Collect --------------------\n",
    "datasets = [(df_1992_3, 1992), (df_2003_3, 2003), (df_2013_3, 2013), (df_2019_3, 2019)]\n",
    "all_data = {k: [] for k in [\"Assets_WealthG\",\"Assets_WealthG_weighted\",\"Liabilities_WealthG\",\"Liabilities_WealthG_weighted\",\"Assets_MCEG\",\"Assets_MCEG_weighted\",\"Liabilities_MCEG\",\"Liabilities_MCEG_weighted\"]}\n",
    "\n",
    "for df, year in datasets:\n",
    "    outputs = process_dataset(df, year)\n",
    "    for key in all_data.keys():\n",
    "        if outputs[key] is not None:\n",
    "            all_data[key].append(outputs[key])\n",
    "\n",
    "# Concatenate and save each file\n",
    "for key, dfs in all_data.items():\n",
    "    if dfs:\n",
    "        combined = pd.concat(dfs, ignore_index=True)\n",
    "        out_path = os.path.join(EXPORT_PATH, f\"{key}.xlsx\")\n",
    "        combined.to_excel(out_path, index=False)\n",
    "        print(f\"Exported {out_path}\")\n",
    "\n",
    "print(\"✅ All 8 files exported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "127e75b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All data exported to C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww2\\Household_Compositions_AllSheets.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Only names, groups, %ages in 1 workbook with 8 sheets\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Save Path --------------------\n",
    "EXPORT_PATH = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww2\"\n",
    "os.makedirs(EXPORT_PATH, exist_ok=True)\n",
    "OUTPUT_FILE = os.path.join(EXPORT_PATH, \"Household_Compositions_AllSheets.xlsx\")\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(\n",
    "        hhid_df[wealth_col],\n",
    "        [-np.inf, *cuts_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(\n",
    "            hhid_df[wealth_col],\n",
    "            [-np.inf, *cuts_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (if present)\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(\n",
    "            hhid_df[mce_col],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(\n",
    "                hhid_df[mce_col],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted','MCEGroup','MCEGroup_weighted'] if 'MCEGroup' in hhid_df.columns else ['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted']]\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col, year):\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(TotalValue=(value_col, 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "\n",
    "    merged = merged[[category_col, group_col, 'PercentageShare']]\n",
    "    merged = merged.rename(columns={category_col: \"Name\", group_col: \"Group\"})\n",
    "    merged['Year'] = year\n",
    "    return merged\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    hhid_groups = assign_hhid_quartiles(df, 'Gross_wealth', 'MCE', 'Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    return {\n",
    "        \"Assets_WealthG\": compute_composition(df, 'Gross_wealthGroup', 'Asset_Value_weighted', 'Asset_Name', year),\n",
    "        \"Assets_WealthG_weighted\": compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year) if has_weight else None,\n",
    "        \"Liabilities_WealthG\": compute_composition(df, 'Gross_wealthGroup', 'Liability_Value_weighted', 'Liability_name', year),\n",
    "        \"Liabilities_WealthG_weighted\": compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year) if has_weight else None,\n",
    "        \"Assets_MCEG\": compute_composition(df, 'MCEGroup', 'Asset_Value_weighted', 'Asset_Name', year) if 'MCEGroup' in df.columns else None,\n",
    "        \"Assets_MCEG_weighted\": compute_composition(df, 'MCEGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year) if has_weight and 'MCEGroup_weighted' in df.columns else None,\n",
    "        \"Liabilities_MCEG\": compute_composition(df, 'MCEGroup', 'Liability_Value_weighted', 'Liability_name', year) if 'MCEGroup' in df.columns else None,\n",
    "        \"Liabilities_MCEG_weighted\": compute_composition(df, 'MCEGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year) if has_weight and 'MCEGroup_weighted' in df.columns else None,\n",
    "    }\n",
    "\n",
    "# -------------------- Run for All Years & Combine --------------------\n",
    "datasets = [(df_1992_4, 1992), (df_2003_4, 2003), (df_2013_4, 2013), (df_2019_4, 2019)]\n",
    "all_data = {k: [] for k in [\"Assets_WealthG\",\"Assets_WealthG_weighted\",\"Liabilities_WealthG\",\"Liabilities_WealthG_weighted\",\"Assets_MCEG\",\"Assets_MCEG_weighted\",\"Liabilities_MCEG\",\"Liabilities_MCEG_weighted\"]}\n",
    "\n",
    "for df, year in datasets:\n",
    "    outputs = process_dataset(df, year)\n",
    "    for key in all_data.keys():\n",
    "        if outputs[key] is not None:\n",
    "            all_data[key].append(outputs[key])\n",
    "\n",
    "# Merge and save to 1 workbook with 8 sheets\n",
    "with pd.ExcelWriter(OUTPUT_FILE) as writer:\n",
    "    for key, dfs in all_data.items():\n",
    "        if dfs:\n",
    "            combined = pd.concat(dfs, ignore_index=True)\n",
    "            combined.to_excel(writer, sheet_name=key, index=False)\n",
    "print(f\"✅ All data exported to {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6e9139a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Long-format Excel saved at: C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww56\\Household_Compositions_Long.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Save Path --------------------\n",
    "EXPORT_PATH = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww56\"\n",
    "os.makedirs(EXPORT_PATH, exist_ok=True)\n",
    "EXPORT_FILE = os.path.join(EXPORT_PATH, \"Household_Compositions_Long.xlsx\")\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    # Wealth quartiles\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(\n",
    "        hhid_df[wealth_col],\n",
    "        [-np.inf, *cuts_unweighted, np.inf],\n",
    "        labels=['0-25','25-50','50-75','75-100'],\n",
    "        include_lowest=True\n",
    "    )\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(\n",
    "            hhid_df[wealth_col],\n",
    "            [-np.inf, *cuts_weighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "    else:\n",
    "        cuts_weighted = None\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    # MCE quartiles (if present)\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(\n",
    "            hhid_df[mce_col],\n",
    "            [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "            labels=['0-25','25-50','50-75','75-100'],\n",
    "            include_lowest=True\n",
    "        )\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(\n",
    "                hhid_df[mce_col],\n",
    "                [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                labels=['0-25','25-50','50-75','75-100'],\n",
    "                include_lowest=True\n",
    "            )\n",
    "        else:\n",
    "            cuts_mce_weighted = None\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        cuts_mce_unweighted, cuts_mce_weighted = None, None\n",
    "\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted'] + \n",
    "                   ([ 'MCEGroup','MCEGroup_weighted'] if mce_col and mce_col in df.columns else [])]\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col, year, category_type):\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "\n",
    "    grouped = (\n",
    "        hh_cat.groupby([category_col, group_col], observed=True)\n",
    "        .agg(TotalValue=(value_col, 'sum'))\n",
    "        .reset_index()\n",
    "    )\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "\n",
    "    # Long format\n",
    "    merged = merged[[category_col, group_col, 'PercentageShare']].rename(\n",
    "        columns={category_col: \"Name\", group_col: \"Group\"}\n",
    "    )\n",
    "    merged['Year'] = year\n",
    "    merged['CategoryType'] = category_type\n",
    "    return merged[['Year','CategoryType','Name','Group','PercentageShare']]\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    # Assign quartiles\n",
    "    hhid_groups = assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Generate long-format rows\n",
    "    frames = []\n",
    "    frames.append(compute_composition(df, 'Gross_wealthGroup', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_WealthG'))\n",
    "    if has_weight: frames.append(compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_WealthG_weighted'))\n",
    "    frames.append(compute_composition(df, 'Gross_wealthGroup', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_WealthG'))\n",
    "    if has_weight: frames.append(compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_WealthG_weighted'))\n",
    "    if 'MCEGroup' in df.columns: frames.append(compute_composition(df, 'MCEGroup', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_MCEG'))\n",
    "    if has_weight and 'MCEGroup_weighted' in df.columns: frames.append(compute_composition(df, 'MCEGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_MCEG_weighted'))\n",
    "    if 'MCEGroup' in df.columns: frames.append(compute_composition(df, 'MCEGroup', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_MCEG'))\n",
    "    if has_weight and 'MCEGroup_weighted' in df.columns: frames.append(compute_composition(df, 'MCEGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_MCEG_weighted'))\n",
    "    return pd.concat(frames, ignore_index=True)\n",
    "\n",
    "# -------------------- Run for All Years --------------------\n",
    "long_frames = []\n",
    "datasets = [(df_1992_11, 1992), (df_2003_11, 2003), (df_2013_11, 2013), (df_2019_11, 2019)]\n",
    "for df, year in datasets:\n",
    "    long_frames.append(process_dataset(df, year))\n",
    "\n",
    "# Combine & Save\n",
    "final_long = pd.concat(long_frames, ignore_index=True)\n",
    "final_long.to_excel(EXPORT_FILE, sheet_name=\"Long_Format\", index=False)\n",
    "print(f\"✅ Long-format Excel saved at: {EXPORT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8c616368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_508\\3874072358.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_508\\3874072358.py:91: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  return pd.concat([\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported: C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\\Household_Compositions_Long_Corrected.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_508\\3874072358.py:69: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(['Name','Year','CategoryType'], group_keys=False).apply(adjust)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Save Path --------------------\n",
    "EXPORT_PATH = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\Neww\"\n",
    "os.makedirs(EXPORT_PATH, exist_ok=True)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups at HHID level --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + ([weight_col] if weight_col and weight_col in df.columns else [])].mean()\n",
    "\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(hhid_df[wealth_col], [-np.inf, *cuts_unweighted, np.inf],\n",
    "                                          labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "    if weight_col and weight_col in df.columns:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(hhid_df[wealth_col], [-np.inf, *cuts_weighted, np.inf],\n",
    "                                                       labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "    else:\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = None\n",
    "\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(hhid_df[mce_col], [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "                                     labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "        if weight_col and weight_col in df.columns:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25,0.50,0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(hhid_df[mce_col], [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                                                  labels=['0-25','25-50','50-75','75-100'], include_lowest=True)\n",
    "        else:\n",
    "            hhid_df['MCEGroup_weighted'] = None\n",
    "    else:\n",
    "        hhid_df['MCEGroup'] = None\n",
    "        hhid_df['MCEGroup_weighted'] = None\n",
    "\n",
    "    return hhid_df[['HHID','Gross_wealthGroup','Gross_wealthGroup_weighted','MCEGroup','MCEGroup_weighted']]\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col):\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "    grouped = (hh_cat.groupby([category_col, group_col], observed=True)\n",
    "               .agg(TotalValue=(value_col, 'sum')).reset_index())\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All'])\n",
    "    return merged[[category_col, group_col, 'PercentageShare']].rename(columns={category_col: \"Name\", group_col: \"Group\"})\n",
    "\n",
    "# -------------------- Adjust Percentages to 100% --------------------\n",
    "def adjust_percentages(df):\n",
    "    df = df.sort_values(['Name','Year','CategoryType','Group'])\n",
    "    def adjust(group):\n",
    "        group['PercentageShare'] = group['PercentageShare'].round(2)\n",
    "        diff = 100 - group['PercentageShare'].sum()\n",
    "        if not group.empty and '75-100' in group['Group'].values:\n",
    "            idx = group['Group'] == '75-100'\n",
    "            group.loc[idx, 'PercentageShare'] += diff\n",
    "        return group\n",
    "    return df.groupby(['Name','Year','CategoryType'], group_keys=False).apply(adjust)\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    hhid_groups = assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Build long-format for all 8 categories\n",
    "    def make_long(data, group, value, name, ctype):\n",
    "        temp = compute_composition(data, group, value, name)\n",
    "        temp['Year'] = year\n",
    "        temp['CategoryType'] = ctype\n",
    "        return temp\n",
    "\n",
    "    return pd.concat([\n",
    "        make_long(df,'Gross_wealthGroup','Asset_Value_weighted','Asset_Name','Assets_WealthG'),\n",
    "        make_long(df,'Gross_wealthGroup_weighted','Asset_Value_weighted','Asset_Name','Assets_WealthG_weighted'),\n",
    "        make_long(df,'Gross_wealthGroup','Liability_Value_weighted','Liability_name','Liabilities_WealthG'),\n",
    "        make_long(df,'Gross_wealthGroup_weighted','Liability_Value_weighted','Liability_name','Liabilities_WealthG_weighted'),\n",
    "        make_long(df,'MCEGroup','Asset_Value_weighted','Asset_Name','Assets_MCEG'),\n",
    "        make_long(df,'MCEGroup_weighted','Asset_Value_weighted','Asset_Name','Assets_MCEG_weighted'),\n",
    "        make_long(df,'MCEGroup','Liability_Value_weighted','Liability_name','Liabilities_MCEG'),\n",
    "        make_long(df,'MCEGroup_weighted','Liability_Value_weighted','Liability_name','Liabilities_MCEG_weighted'),\n",
    "    ], ignore_index=True)\n",
    "\n",
    "# -------------------- Run for All Years --------------------\n",
    "all_data = []\n",
    "datasets = [(df_1992_12, 1992), (df_2003_12, 2003), (df_2013_12, 2013), (df_2019_12, 2019)]\n",
    "for df, year in datasets:\n",
    "    all_data.append(process_dataset(df, year))\n",
    "\n",
    "final_long = pd.concat(all_data, ignore_index=True)\n",
    "final_long = adjust_percentages(final_long)\n",
    "\n",
    "# Export corrected long-format\n",
    "output_file = os.path.join(EXPORT_PATH, \"Household_Compositions_Long_Corrected.xlsx\")\n",
    "final_long.to_excel(output_file, index=False)\n",
    "print(f\"✅ Exported: {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "78ad3616",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 173-174: truncated \\UXXXXXXXX escape (1491497257.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[252], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 173-174: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# -------------------- Libraries --------------------\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "EXPORT_PATH = r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\Finals analysis\\StackedBars\"\n",
    "os.makedirs(EXPORT_PATH, exist_ok=True)\n",
    "\n",
    "# -------------------- Weighted Percentile --------------------\n",
    "def weighted_percentile(values, weights, percentiles):\n",
    "    sorter = np.argsort(values)\n",
    "    values, weights = np.array(values)[sorter], np.array(weights)[sorter]\n",
    "    cum_weights = np.cumsum(weights)\n",
    "    total_weight = cum_weights[-1]\n",
    "    return np.interp(np.array(percentiles) * total_weight, cum_weights, values)\n",
    "\n",
    "# -------------------- Assign Quartile Groups --------------------\n",
    "def assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col=None, weight_col=None):\n",
    "    group_cols = [wealth_col] + ([mce_col] if mce_col and mce_col in df.columns else [])\n",
    "    weight_cols = [weight_col] if weight_col and weight_col in df.columns else []\n",
    "    hhid_df = df.groupby('HHID', as_index=False)[group_cols + weight_cols].mean()\n",
    "\n",
    "    # Wealth groups\n",
    "    cuts_unweighted = hhid_df[wealth_col].quantile([0.25, 0.50, 0.75])\n",
    "    hhid_df['Gross_wealthGroup'] = pd.cut(hhid_df[wealth_col], [-np.inf, *cuts_unweighted, np.inf],\n",
    "                                          labels=['0-25', '25-50', '50-75', '75-100'], include_lowest=True)\n",
    "    if weight_cols:\n",
    "        cuts_weighted = weighted_percentile(hhid_df[wealth_col], hhid_df[weight_col], [0.25, 0.50, 0.75])\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = pd.cut(hhid_df[wealth_col], [-np.inf, *cuts_weighted, np.inf],\n",
    "                                                       labels=['0-25', '25-50', '50-75', '75-100'], include_lowest=True)\n",
    "    else:\n",
    "        hhid_df['Gross_wealthGroup_weighted'] = np.nan\n",
    "\n",
    "    # MCE groups (if applicable)\n",
    "    if mce_col and mce_col in df.columns:\n",
    "        cuts_mce_unweighted = hhid_df[mce_col].quantile([0.25, 0.50, 0.75])\n",
    "        hhid_df['MCEGroup'] = pd.cut(hhid_df[mce_col], [-np.inf, *cuts_mce_unweighted, np.inf],\n",
    "                                     labels=['0-25', '25-50', '50-75', '75-100'], include_lowest=True)\n",
    "        if weight_cols:\n",
    "            cuts_mce_weighted = weighted_percentile(hhid_df[mce_col], hhid_df[weight_col], [0.25, 0.50, 0.75])\n",
    "            hhid_df['MCEGroup_weighted'] = pd.cut(hhid_df[mce_col], [-np.inf, *cuts_mce_weighted, np.inf],\n",
    "                                                  labels=['0-25', '25-50', '50-75', '75-100'], include_lowest=True)\n",
    "        else:\n",
    "            hhid_df['MCEGroup_weighted'] = np.nan\n",
    "    else:\n",
    "        hhid_df['MCEGroup'] = np.nan\n",
    "        hhid_df['MCEGroup_weighted'] = np.nan\n",
    "\n",
    "    return hhid_df[['HHID', 'Gross_wealthGroup', 'Gross_wealthGroup_weighted', 'MCEGroup', 'MCEGroup_weighted']]\n",
    "\n",
    "# -------------------- Composition Calculation --------------------\n",
    "def compute_composition(df, group_col, value_col, category_col, year, group_type):\n",
    "    hh_cat = df.groupby(['HHID', category_col], as_index=False)[value_col].sum()\n",
    "    hh_cat = hh_cat.merge(df[['HHID', group_col]].drop_duplicates(), on='HHID', how='left')\n",
    "    grouped = hh_cat.groupby([category_col, group_col], observed=True).agg(TotalValue=(value_col, 'sum')).reset_index()\n",
    "    totals = hh_cat.groupby(category_col, observed=True)[value_col].sum().rename('TotalValue_All').reset_index()\n",
    "    merged = grouped.merge(totals, on=category_col, how='left')\n",
    "    merged['PercentageShare'] = (100 * merged['TotalValue'] / merged['TotalValue_All']).round(2)\n",
    "    merged = merged.rename(columns={category_col: \"Name\", group_col: \"Group\"})\n",
    "    merged['Year'] = year\n",
    "    merged['CategoryType'] = group_type\n",
    "    return merged[['Year', 'CategoryType', 'Name', 'Group', 'PercentageShare']]\n",
    "\n",
    "def adjust_percentages(df):\n",
    "    # Ensure each Name-Year-CategoryType sums exactly to 100\n",
    "    df = df.sort_values(['Name', 'Year', 'CategoryType', 'Group'])\n",
    "    def adjust(group):\n",
    "        group['PercentageShare'] = group['PercentageShare'].round(2)\n",
    "        diff = 100 - group['PercentageShare'].sum()\n",
    "        # Add/subtract the difference to the last group (75-100)\n",
    "        if not group.empty:\n",
    "            idx = group['Group'].eq('75-100')\n",
    "            if idx.any():\n",
    "                group.loc[idx, 'PercentageShare'] += diff\n",
    "        return group\n",
    "    return df.groupby(['Name','Year','CategoryType'], group_keys=False).apply(adjust)\n",
    "\n",
    "\n",
    "# -------------------- Full Processing --------------------\n",
    "def process_dataset(df, year):\n",
    "    has_weight = 'Weight' in df.columns\n",
    "    if has_weight:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value'] * df['Weight']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value'] * df['Weight']\n",
    "    else:\n",
    "        df['Asset_Value_weighted'] = df['Asset_Value']\n",
    "        df['Liability_Value_weighted'] = df['Liability_value']\n",
    "\n",
    "    # Assign HHID-level groups\n",
    "    hhid_groups = assign_hhid_quartiles(df, wealth_col='Gross_wealth', mce_col='MCE', weight_col='Weight' if has_weight else None)\n",
    "    df = df.merge(hhid_groups, on='HHID', how='left')\n",
    "\n",
    "    # Build all 8 categories\n",
    "    data_frames = []\n",
    "    data_frames.append(compute_composition(df, 'Gross_wealthGroup', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_WealthG'))\n",
    "    data_frames.append(compute_composition(df, 'Gross_wealthGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_WealthG_weighted'))\n",
    "    data_frames.append(compute_composition(df, 'Gross_wealthGroup', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_WealthG'))\n",
    "    data_frames.append(compute_composition(df, 'Gross_wealthGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_WealthG_weighted'))\n",
    "    data_frames.append(compute_composition(df, 'MCEGroup', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_MCEG'))\n",
    "    data_frames.append(compute_composition(df, 'MCEGroup_weighted', 'Asset_Value_weighted', 'Asset_Name', year, 'Assets_MCEG_weighted'))\n",
    "    data_frames.append(compute_composition(df, 'MCEGroup', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_MCEG'))\n",
    "    data_frames.append(compute_composition(df, 'MCEGroup_weighted', 'Liability_Value_weighted', 'Liability_name', year, 'Liabilities_MCEG_weighted'))\n",
    "    return pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "# -------------------- Load & Process --------------------\n",
    "# Replace these with your actual DataFrames\n",
    "datasets = [(df_1992, 1992), (df_2003, 2003), (df_2013, 2013), (df_2019, 2019)]\n",
    "\n",
    "all_data = []\n",
    "for df, year in datasets:\n",
    "    print(f\"Processing {year}...\")\n",
    "    all_data.append(process_dataset(df, year))\n",
    "final_long = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# -------------------- Export --------------------\n",
    "out_path = os.path.join(EXPORT_PATH, \"Household_Compositions_Long.xlsx\")\n",
    "final_long.to_excel(out_path, index=False)\n",
    "print(f\"✅ Exported long-format data to {out_path}\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
