{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "4ec8c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "os.chdir(r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\Data Cleaning\\22 July 2025\\2019\")\n",
    "os.getcwd()\n",
    "\n",
    "# Filepaths for the datasets\n",
    "filepaths = {\n",
    "    'df1': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 04 (Block 4) - Household characteristics.dta\",\n",
    "    \n",
    "    'df2': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 12 (Block 11a) - Financial assets including receivables (excluding shares) owned.dta\",\n",
    "\n",
    "    'df3': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 13 (Block 11b) - Investments in share and related instruments owned in co-operative societies & companies..dta\",\n",
    "\n",
    "    'df4': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 14 (Block 12) - particulars of cash loans payable to institutional, non-institutional agencies.dta\",\n",
    "\n",
    "    'df5': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 07 (Block 6) - Buildings and other constructions owned by the household.dta\",\n",
    "\n",
    "    'df6': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 06 (Block 5.2) -URBAN LAND owned by the household.dta\",\n",
    "\n",
    "    'df7': r\"C:\\Users\\siddu\\Downloads\\US Replication Folder\\Trying for India\\AIDIS Data\\AIDIS 2019 nss 77th round\\AIDIS 2019 All datasets\\Visit1  Level - 05 (Block 5pt1) - RURAL LAND owned by the household .dta\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "430528af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 116461\n",
      "Unique HHIDs in df2: 115943\n",
      "Unique HHIDs in df3: 1615\n",
      "Unique HHIDs in df4: 72938\n",
      "Unique HHIDs in df5: 97572\n",
      "Unique HHIDs in df6: 31206\n",
      "Unique HHIDs in df7: 71400\n",
      "Common HHIDs across: 1123\n",
      "df1: 1123 rows after filtering\n",
      "df2: 6434 rows after filtering\n",
      "df3: 2297 rows after filtering\n",
      "df4: 3571 rows after filtering\n",
      "df5: 3048 rows after filtering\n",
      "df6: 1111 rows after filtering\n",
      "df7: 2559 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Load dataframes\n",
    "dataframes = {name: pd.read_stata(path) for name, path in filepaths.items()}\n",
    "\n",
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs: intersection of first five & union of df6, df7\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 6)\n",
    "]) & set.union(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(6, 8)\n",
    "])\n",
    "\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6, df7 = [dataframes[f'df{i}'] for i in range(1, 8)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "312f6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1: Keep relevant columns\n",
    "df1 = df1[['HHID', 'State', 'b4q10dot5', 'MLT']].copy()\n",
    "\n",
    "# Checking for missing values in MCE\n",
    "df1['b4q10dot5'].isna().sum()\n",
    "\n",
    "\n",
    "# df2 is Fin Asset- 2019\n",
    "df2 = df2[['HHID', 'b11aq1', 'b11aq3', 'b11aq6']].copy()\n",
    "df2 = df2[df2['b11aq1'].isin([1,2,3,4,5,6,7,8,9,10,11,13])] \n",
    "\n",
    "# df3: Keep relevant columns\n",
    "df3 = df3[['HHID', 'b11bq1', 'b11bq3', 'b11bq6']].copy()\n",
    "df3 = df3[df3['b11bq1'] == 5] \n",
    "\n",
    "# df4: Keep relevant columns\n",
    "df4 = df4[['HHID', 'b12q1', 'b12q5', 'b12q14', 'b12q15']].copy()\n",
    "df4 = df4[df4['b12q5'].isin([1,2,3,4,5,6,8,10,13])] \n",
    "\n",
    "# df5: Keep only rows where b6q3 == 10\n",
    "df5 = df5[['HHID', 'b6q3', 'b6q5']].copy()\n",
    "df5 = df5[df5['b6q3'] == 10].copy()\n",
    "\n",
    "# df6: Keep only rows where b5pt2q1 == 99\n",
    "df6 = df6[['HHID', 'b5pt2q1', 'b5pt2q5']].copy()\n",
    "df6 = df6[df6['b5pt2q1'] == 99].copy()\n",
    "\n",
    "# df7: Keep only rows where b5pt1q1 == 99\n",
    "df7 = df7[['HHID', 'b5pt1q1', 'b5pt1q5']].copy()\n",
    "df7 = df7[df7['b5pt1q1'] == 99].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "bd4aee94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming, creating, dropping columns \n",
    "\n",
    "df1  = df1.rename(columns={\n",
    "    'b4q10dot5': 'MCE'\n",
    "})\n",
    "\n",
    "df2  = df2.rename(columns={\n",
    "    'b11aq1': 'Fin_Asset_Serial',\n",
    "    'b11aq3': 'Fin_Asset_val1',\n",
    "    'b11aq6': 'Fin_Asset_val2'\n",
    "})\n",
    "\n",
    "df3  = df3.rename(columns={\n",
    "    'b11bq1': 'Share_Asset_serial',\n",
    "    'b11bq3': 'Share_Asset_val1',\n",
    "    'b11bq6': 'Share_Asset_val2'\n",
    "})\n",
    "\n",
    "df4  = df4.rename(columns={\n",
    "    'b12q1': 'Liability_serial',\n",
    "    'b12q5': 'Credit_Agency',\n",
    "    'b12q14': 'Liability_val1',\n",
    "    'b12q15': 'Liability_val2'\n",
    "})\n",
    "\n",
    "df5  = df5.rename(columns={\n",
    "    'b6q3': 'Building_serial',\n",
    "    'b6q5': 'Building_value'\n",
    "})\n",
    "\n",
    "df6  = df6.rename(columns={\n",
    "    'b5pt2q1': 'Urban_land_serial',\n",
    "    'b5pt2q5': 'Urban_land_value'\n",
    "})\n",
    "\n",
    "df7  = df7.rename(columns={\n",
    "    'b5pt1q1': 'Rural_land_serial',\n",
    "    'b5pt1q5': 'Rural_land_value',\n",
    "})\n",
    "\n",
    "#df1['MCE'] = df1['MCE']/1000\n",
    "df1['Weight'] = df1['MLT']/200\n",
    "\n",
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "e11578a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'MLT', 'Weight'], dtype='object')"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "77f9ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Fin_Asset_Value'] = np.where(\n",
    "    df2['Fin_Asset_Serial'] == 1,\n",
    "    df2['Fin_Asset_val1'],\n",
    "    df2['Fin_Asset_val1'].combine_first(df2['Fin_Asset_val2'])\n",
    ")\n",
    "df2['Fin_Asset_Value'] = pd.Series(df2['Fin_Asset_Value']).fillna(0)\n",
    "\n",
    "# df3 (Share Asset value)\n",
    "\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_val1'].combine_first(df3['Share_Asset_val2'])\n",
    "df3['Share_Asset_Value'] = df3['Share_Asset_Value'].fillna(0)\n",
    "\n",
    "# df4 (Loan value)\n",
    "df4['Liability_value'] = df4['Liability_val1'].combine_first(df4['Liability_val2'])\n",
    "df4['Liability_value'] = df4['Liability_value'].fillna(0)\n",
    "\n",
    "# df5 (Buildings value)\n",
    "df5['Building_value'] = df5['Building_value'].fillna(0)\n",
    "\n",
    "# df6 (Urban land value)\n",
    "df6['Urban_land_value'] = df6['Urban_land_value'].fillna(0)\n",
    "\n",
    "# df7 (Rural land value)\n",
    "df7['Rural_land_value'] = df7['Rural_land_value'].fillna(0)\n",
    "\n",
    "\n",
    "# Drop unnecessary variables\n",
    "df1.drop(['MLT'], axis=1, inplace=True)\n",
    "df2.drop(['Fin_Asset_val1', 'Fin_Asset_val2'], axis=1, inplace=True)\n",
    "df3.drop(['Share_Asset_val1', 'Share_Asset_val2'], axis=1, inplace=True)\n",
    "df4.drop(['Liability_serial', 'Liability_val1', 'Liability_val2'], axis=1, inplace=True)\n",
    "df5.drop(['Building_serial'], axis=1, inplace=True)\n",
    "df6.drop(['Urban_land_serial'], axis=1, inplace=True)\n",
    "df7.drop(['Rural_land_serial'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Save to Stata\n",
    "df1.to_stata(\"df1_2019.dta\", write_index=False)\n",
    "df2.to_stata(\"df2_2019.dta\", write_index=False)\n",
    "df3.to_stata(\"df3_2019.dta\", write_index=False)\n",
    "df4.to_stata(\"df4_2019.dta\", write_index=False)\n",
    "df5.to_stata(\"df5_2019.dta\", write_index=False)\n",
    "df6.to_stata(\"df6_2019.dta\", write_index=False)\n",
    "df7.to_stata(\"df7_2019.dta\", write_index=False)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "d3597837",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 8):\n",
    "    globals()[f\"df{i}\"] = globals()[f\"df{i}\"].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "298fea82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin_Asset_Serial\n",
      "1     1116\n",
      "3     1086\n",
      "13     505\n",
      "9      498\n",
      "5      234\n",
      "4      164\n",
      "10     148\n",
      "7       85\n",
      "2       80\n",
      "11      73\n",
      "8       32\n",
      "6       16\n",
      "Name: count, dtype: int64\n",
      "        HHID  Fin_Asset_Serial  Fin_Asset_Value\n",
      "0  501171011                 1           3000.0\n",
      "1  501171011                 3          20000.0\n",
      "2  501171011               999          23000.0\n",
      "3  501171031                 1           4000.0\n",
      "4  501171031                 3          60000.0\n",
      "1122\n"
     ]
    }
   ],
   "source": [
    "print(df2['Fin_Asset_Serial'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Fin_Asset_Value\n",
    "summary_rows = df2.groupby('HHID', as_index=False)['Fin_Asset_Value'].sum()\n",
    "summary_rows['Fin_Asset_Serial'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df2.columns:\n",
    "    if col not in ['HHID', 'Fin_Asset_Serial', 'Fin_Asset_Value']:\n",
    "        summary_rows[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df2_mod = pd.concat([df2, summary_rows], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df2_mod = df2_mod.sort_values(by=['HHID', 'Fin_Asset_Serial']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "print(df2_mod.head())\n",
    "print(df2_mod['HHID'].nunique())\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts(dropna=False)\n",
    "\n",
    "# Modify serials and labels, if any\n",
    "\n",
    "# df2_mod\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([1]), 'Fin_Asset_Serial'] = 31\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([2,3,4,7,8,9]), 'Fin_Asset_Serial'] = 32\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([13]), 'Fin_Asset_Serial'] = 34\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([10,11]), 'Fin_Asset_Serial'] = 35\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([5,6]), 'Fin_Asset_Serial'] = 36\n",
    "df2_mod.loc[df2_mod['Fin_Asset_Serial'].isin([999]), 'Fin_Asset_Serial'] = 37\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "# Fin Asset Mapping\n",
    "\n",
    "Fin_Asset_map = {\n",
    "\n",
    "    31.0: 'Currency',\n",
    "    32.0: 'Deposits',\n",
    "    34.0: 'Life Insurance fund',\n",
    "    35.0: 'Provident and Pension fund',\n",
    "    36.0: 'Claims on Government',\n",
    "    37.0: 'Total financial assets (other thanshares and related instruments)'\n",
    "}\n",
    "\n",
    "df2_mod = df2_mod.groupby(['HHID', 'Fin_Asset_Serial'], as_index=False).agg({\n",
    "    'Fin_Asset_Value': 'sum'  \n",
    "})\n",
    "\n",
    "df2_mod = df2_mod.reset_index(drop=True)\n",
    "\n",
    "df2_mod.to_stata(\"mergedforu.dta\", write_index=False)\n",
    "\n",
    "df2_mod['Fin_Asset_Serial'].value_counts()\n",
    "\n",
    "df2 = df2_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "dd977318",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Share_Asset_serial'].value_counts()\n",
    "\n",
    "df3 = df3.reset_index(drop=True)\n",
    "\n",
    "# Share_Asset_Map\n",
    "\n",
    "# Mapping dictionary using float keys (since your column is float64)\n",
    "Share_Asset_Map = {\n",
    "    5.0: 'Shares and Debentures'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "ee97e6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Credit_Agency\n",
      "1.0     750\n",
      "3.0     501\n",
      "4.0     221\n",
      "10.0     79\n",
      "8.0      53\n",
      "2.0      51\n",
      "5.0      18\n",
      "13.0     12\n",
      "6.0       5\n",
      "Name: count, dtype: int64\n",
      "        HHID  Credit_Agency  Liability_value\n",
      "0  501171011            3.0          10000.0\n",
      "1  501171011          999.0          10000.0\n",
      "2  501171031            3.0         100000.0\n",
      "3  501171031          999.0         100000.0\n",
      "4  501171041            3.0         500000.0\n",
      "1004\n"
     ]
    }
   ],
   "source": [
    "print(df4['Credit_Agency'].value_counts(dropna=False))\n",
    "\n",
    "# Step 2: Group by HHID and sum Liability_value\n",
    "summary_rows_liab = df4.groupby('HHID', as_index=False)['Liability_value'].sum()\n",
    "summary_rows_liab['Credit_Agency'] = 999\n",
    "\n",
    "# Step 3: Add other columns with NaN or defaults\n",
    "for col in df4.columns:\n",
    "    if col not in ['HHID', 'Credit_Agency', 'Liability_value']:\n",
    "        summary_rows_liab[col] = np.nan\n",
    "\n",
    "# Step 4: Concatenate the new rows\n",
    "df4_mod = pd.concat([df4, summary_rows_liab], ignore_index=True)\n",
    "\n",
    "# Step 5: Sort\n",
    "df4_mod = df4_mod.sort_values(by=['HHID', 'Credit_Agency']).reset_index(drop=True)\n",
    "\n",
    "# Step 6: Save\n",
    "df4_mod.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "print(df4_mod.head())\n",
    "print(df4_mod['HHID'].nunique())\n",
    "\n",
    "# df4_mod\n",
    "\n",
    "df4_mod.loc[df4_mod['Credit_Agency'].between(1, 4), 'Credit_Agency'] = 91\n",
    "df4_mod.loc[df4_mod['Credit_Agency'].isin([5, 6, 8, 10, 13]), 'Credit_Agency'] = 92\n",
    "\n",
    "df4_mod = df4_mod.reset_index(drop=True)\n",
    "\n",
    "# Liability Map\n",
    "\n",
    "Credit_Agency_Map = {\n",
    "\n",
    "    91.0: 'Bank advances',\n",
    "    92.0: 'Non-banking loans and advances',\n",
    "    999.0: 'Total Institutional Liabilities'\n",
    "}\n",
    "\n",
    "df4_mod = df4_mod.groupby(['HHID', 'Credit_Agency'], as_index=False).agg({\n",
    "    'Liability_value': 'sum'  \n",
    "})\n",
    "\n",
    "df4_mod = df4_mod.reset_index(drop=True)\n",
    "\n",
    "df4_mod.to_stata(\"mergedforu_liab.dta\", write_index=False)\n",
    "\n",
    "df4_mod['Credit_Agency'].value_counts()\n",
    "\n",
    "df4 = df4_mod.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "38eb8f9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HHID</th>\n",
       "      <th>Urban_land_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>532614011</td>\n",
       "      <td>500000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>532601011</td>\n",
       "      <td>900000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536206031</td>\n",
       "      <td>700000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536101021</td>\n",
       "      <td>780000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>554221021</td>\n",
       "      <td>1000000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        HHID  Urban_land_value\n",
       "0  532614011          500000.0\n",
       "1  532601011          900000.0\n",
       "2  536206031          700000.0\n",
       "3  536101021          780000.0\n",
       "4  554221021         1000000.0"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "0e0bdf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes['df1'] = df1\n",
    "dataframes['df2'] = df2\n",
    "dataframes['df3'] = df3\n",
    "dataframes['df4'] = df4\n",
    "dataframes['df5'] = df5\n",
    "dataframes['df6'] = df6\n",
    "dataframes['df7'] = df7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "194877b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs in df1: 1123\n",
      "Unique HHIDs in df2: 1122\n",
      "Unique HHIDs in df3: 1123\n",
      "Unique HHIDs in df4: 1004\n",
      "Unique HHIDs in df5: 1123\n",
      "Unique HHIDs in df6: 467\n",
      "Unique HHIDs in df7: 793\n",
      "Common HHIDs across: 1004\n",
      "df1: 1004 rows after filtering\n",
      "df2: 3873 rows after filtering\n",
      "df3: 1004 rows after filtering\n",
      "df4: 2084 rows after filtering\n",
      "df5: 1004 rows after filtering\n",
      "df6: 430 rows after filtering\n",
      "df7: 703 rows after filtering\n"
     ]
    }
   ],
   "source": [
    "# Print unique HHIDs in first seven datasets\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    unique_hhids = set(df['HHID'].dropna())\n",
    "    print(f\"Unique HHIDs in df{i}: {len(unique_hhids)}\")\n",
    "\n",
    "# Compute common HHIDs: intersection of first five & union of df6, df7\n",
    "common_hhids = set.intersection(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(1, 6)\n",
    "]) & set.union(*[\n",
    "    set(dataframes[f'df{i}']['HHID'].dropna()) for i in range(6, 8)\n",
    "])\n",
    "\n",
    "print(f\"Common HHIDs across: {len(common_hhids)}\")\n",
    "\n",
    "# Filter each dataframe to keep only rows with HHID in common_hhids\n",
    "for name in dataframes:\n",
    "    df = dataframes[name]\n",
    "    df_filtered = df[df['HHID'].isin(common_hhids)].copy()\n",
    "    dataframes[name] = df_filtered  # Replace with filtered version\n",
    "\n",
    "# Optional: Print how many rows remain in each\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name}: {len(df)} rows after filtering\")\n",
    "\n",
    "# Reset index\n",
    "for key in dataframes:\n",
    "    dataframes[key] = dataframes[key].reset_index(drop=True)\n",
    "\n",
    "# Convert all columns to numeric except 'HHID' for each dataframe\n",
    "for i in range(1, 8):\n",
    "    df = dataframes[f'df{i}']\n",
    "    cols_to_convert = df.columns.difference(['HHID'])\n",
    "    df[cols_to_convert] = df[cols_to_convert].apply(pd.to_numeric, errors='coerce')\n",
    "    dataframes[f'df{i}'] = df  # Update dictionary\n",
    "\n",
    "# Unpack if needed\n",
    "df1, df2, df3, df4, df5, df6, df7 = [dataframes[f'df{i}'] for i in range(1, 8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "8a96d981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs: 1004\n",
      "Number of duplicates: 0\n",
      "Unique HHIDs with Fin_Asset_Serial = 37: 1004\n",
      "Unique HHIDs with Share_Asset_serial = 5: 1004\n",
      "Unique HHIDs with Credit_Agency = 999: 1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\1925433272.py:21: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\1925433272.py:31: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged1.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Merging all datasets\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df1, df2, df3, df4, df5, df6, df7]\n",
    "\n",
    "merged_df = reduce(lambda left, right: pd.merge(left, right, on='HHID', how='outer'), dfs)\n",
    "\n",
    "merged_df.to_stata(\"merged.dta\", write_index=False)\n",
    "\n",
    "unique_hhids = set(merged_df['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "\n",
    "# Land value\n",
    "merged_df['Land_value'] = merged_df[['Urban_land_value', 'Rural_land_value']].sum(axis=1, min_count=1).fillna(0)\n",
    "\n",
    "# Real Estate Value\n",
    "\n",
    "merged_df['Real Estate'] = merged_df['Building_value'] + merged_df['Land_value']\n",
    "\n",
    "merged_df.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1 = merged_df.copy()\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "merged_df1.drop(['Building_value', 'Urban_land_value',\n",
    "                'Rural_land_value', 'Land_value'\n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "merged_df1.to_stata(\"merged1.dta\", write_index=False)\n",
    "\n",
    "merged_df1.columns\n",
    "\n",
    "# Check for duplicates\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "dup_check = merged_df1.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Fin_Asset_Serial'] == 37, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Fin_Asset_Serial = 37: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Share_Asset_serial'] == 5, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Share_Asset_serial = 5: {unique_count}\")\n",
    "\n",
    "unique_count = merged_df1.loc[merged_df1['Credit_Agency'] == 999, 'HHID'].nunique()\n",
    "print(f\"Unique HHIDs with Credit_Agency = 999: {unique_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "a8b7f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Column  Missing_Values  Zero_Values\n",
      "0                 HHID               0            0\n",
      "1                State               0            0\n",
      "2                  MCE               0            0\n",
      "3     Fin_Asset_Serial               0            0\n",
      "4   Share_Asset_serial               0            0\n",
      "5        Credit_Agency               0            0\n",
      "6    Share_Asset_Value               0            0\n",
      "7      Fin_Asset_Value               0            4\n",
      "8      Liability_value               0          112\n",
      "9          Real Estate               0            0\n",
      "10              Weight               0            0\n",
      "Unique HHIDs: 1004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\724980383.py:20: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "cols = ['HHID', 'State', 'MCE', 'Fin_Asset_Serial', 'Share_Asset_serial',\n",
    "        'Credit_Agency', 'Share_Asset_Value', 'Fin_Asset_Value',\n",
    "        'Liability_value', 'Real Estate', 'Weight']\n",
    "\n",
    "# Create summary dataframe\n",
    "summary = pd.DataFrame({\n",
    "    'Column': cols,\n",
    "    'Missing_Values': [merged_df1[col].isna().sum() for col in cols],\n",
    "    'Zero_Values': [(merged_df1[col] == 0).sum() for col in cols]\n",
    "})\n",
    "\n",
    "print(summary)\n",
    "\n",
    "\n",
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(merged_df1['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")\n",
    "\n",
    "merged_df1.to_stata(\"merged22.dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "493bea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df3 = merged_df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a4fc8e",
   "metadata": {},
   "source": [
    "# Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "3fb14be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the new column\n",
    "merged_df3['Fin_Asset'] = merged_df3['Fin_Asset_Serial'].map(Fin_Asset_map)\n",
    "merged_df3['Share_Asset'] = merged_df3['Share_Asset_serial'].map(Share_Asset_Map)\n",
    "merged_df3['Liability'] = merged_df3['Credit_Agency'].map(Credit_Agency_Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "4c041fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs: 1004\n"
     ]
    }
   ],
   "source": [
    "# Unique HHIDs\n",
    "\n",
    "unique_hhids = set(merged_df3['HHID'].dropna())\n",
    "print(f\"Unique HHIDs: {len(unique_hhids)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "a94bc600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique HHIDs present in all three groups: 1004\n"
     ]
    }
   ],
   "source": [
    "# HHIDs satisfying each condition\n",
    "hhid_fin = set(merged_df3.loc[merged_df3['Fin_Asset_Serial'] == 37, 'HHID'])\n",
    "hhid_share = set(merged_df3.loc[merged_df3['Share_Asset_serial'] == 5, 'HHID'])\n",
    "hhid_credit = set(merged_df3.loc[merged_df3['Credit_Agency'] == 999, 'HHID'])\n",
    "\n",
    "# Intersection: HHIDs present in all three\n",
    "common_hhids = hhid_fin & hhid_share & hhid_credit\n",
    "\n",
    "# Keep only those rows\n",
    "filtered_df = merged_df3[merged_df3['HHID'].isin(common_hhids)].copy()\n",
    "\n",
    "# Print how many unique HHIDs remain\n",
    "print(f\"Unique HHIDs present in all three groups: {len(common_hhids)}\")\n",
    "\n",
    "#filtered_df.drop(['Fin_Asset', 'Share_Asset', 'Liability'], axis=1, inplace=True)\n",
    "\n",
    "#filtered_df.to_stata(\"final(1).dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "c7c2ec45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'Weight', 'Fin_Asset_Serial', 'Fin_Asset_Value',\n",
       "       'Share_Asset_serial', 'Share_Asset_Value', 'Credit_Agency',\n",
       "       'Liability_value', 'Real Estate', 'Fin_Asset', 'Share_Asset',\n",
       "       'Liability'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "ee11f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 32, 34, 35, 36, 37]\n",
      "[5]\n",
      "[91.0, 92.0, 999.0]\n"
     ]
    }
   ],
   "source": [
    "# Sanity checks if grouping worked correctly\n",
    "\n",
    "print(sorted(filtered_df['Fin_Asset_Serial'].unique()))\n",
    "#print(filtered_df['Fin_Asset_Serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Share_Asset_serial'].unique()))\n",
    "#print(filtered_df['Share_Asset_serial'].value_counts())\n",
    "\n",
    "print(sorted(filtered_df['Credit_Agency'].unique()))\n",
    "#print(filtered_df['Credit_Agency'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "5471c6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped1 = filtered_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f99a58e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         HHID  Gross wealth\n",
      "0   501171011     1523500.0\n",
      "6   501171031     5329000.0\n",
      "14  501171041     2693000.0\n",
      "26  501174041     3388000.0\n",
      "32  501271011     2013000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\3101814955.py:31: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  df_grouped1.to_stata(\"final(25).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Filter the relevant rows satisfying your condition\n",
    "wealth_rows = df_grouped1[\n",
    "    (df_grouped1['Credit_Agency'] == 999) &\n",
    "    (df_grouped1['Fin_Asset_Serial'] == 37) &\n",
    "    (df_grouped1['Share_Asset_serial'] == 5)\n",
    "].copy()\n",
    "\n",
    "# Step 2: Compute wealth for these rows\n",
    "wealth_rows['Gross wealth'] = (\n",
    "    wealth_rows['Fin_Asset_Value'] +\n",
    "    wealth_rows['Share_Asset_Value'] +\n",
    "    wealth_rows['Real Estate']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Financial Assets'] = (\n",
    "    wealth_rows['Fin_Asset_Value']+\n",
    "    wealth_rows['Share_Asset_Value']\n",
    ")\n",
    "\n",
    "wealth_rows['Total Institutional liabilities'] = wealth_rows['Liability_value']\n",
    "\n",
    "# Step 3: Keep only HHID and wealth columns\n",
    "wealth_per_hhid = wealth_rows[['HHID', 'Gross wealth', 'Total Financial Assets', 'Total Institutional liabilities']]\n",
    "\n",
    "# Step 4: Merge wealth back to the full dataset based on HHID\n",
    "df_grouped1 = df_grouped1.merge(wealth_per_hhid, on='HHID', how='left')\n",
    "\n",
    "# Step 5: View result\n",
    "print(df_grouped1[['HHID', 'Gross wealth']].drop_duplicates().head())\n",
    "\n",
    "df_grouped1.to_stata(\"final(25).dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6a409625",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grouped2 =  df_grouped1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "8d0b0868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize columns with NaN\n",
    "df_grouped2['Asset_serial'] = np.nan\n",
    "df_grouped2['Asset_Value'] = np.nan\n",
    "df_grouped2['Asset_Name'] = np.nan\n",
    "\n",
    "group_cols = ['HHID', 'Credit_Agency', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "\n",
    "df_grouped2['HHID'] = df_grouped2['HHID'].astype(str)\n",
    "\n",
    "#df_grouped2.to_stata(\"final(26).dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "737c0e90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'Weight', 'Fin_Asset_Serial', 'Fin_Asset_Value',\n",
       "       'Share_Asset_serial', 'Share_Asset_Value', 'Credit_Agency',\n",
       "       'Liability_value', 'Real Estate', 'Fin_Asset', 'Share_Asset',\n",
       "       'Liability', 'Gross wealth', 'Total Financial Assets',\n",
       "       'Total Institutional liabilities', 'Asset_serial', 'Asset_Value',\n",
       "       'Asset_Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_grouped2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "82595119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        HHID  Credit_Agency  Fin_Asset_Serial  Share_Asset_serial  \\\n",
      "0  501171011           91.0                31                   5   \n",
      "1  501171011           91.0                31                   5   \n",
      "2  501171011           91.0                31                   5   \n",
      "3  501171011           91.0                31                   5   \n",
      "4  501171011           91.0                31                   5   \n",
      "\n",
      "   Asset_serial  Asset_Value  State   MCE  Weight  Fin_Asset_Value  \\\n",
      "0            31       3000.0     24  3200  95.815           3000.0   \n",
      "1           205        500.0     24  3200  95.815           3000.0   \n",
      "2           300    1500000.0     24  3200  95.815           3000.0   \n",
      "3          1000    1523500.0     24  3200  95.815           3000.0   \n",
      "4          1500      23500.0     24  3200  95.815           3000.0   \n",
      "\n",
      "   Share_Asset_Value  Liability_value  Real Estate Fin_Asset  \\\n",
      "0              500.0          10000.0    1500000.0  Currency   \n",
      "1              500.0          10000.0    1500000.0  Currency   \n",
      "2              500.0          10000.0    1500000.0  Currency   \n",
      "3              500.0          10000.0    1500000.0  Currency   \n",
      "4              500.0          10000.0    1500000.0  Currency   \n",
      "\n",
      "             Share_Asset      Liability  Gross wealth  Total Financial Assets  \\\n",
      "0  Shares and Debentures  Bank advances     1523500.0                 23500.0   \n",
      "1  Shares and Debentures  Bank advances     1523500.0                 23500.0   \n",
      "2  Shares and Debentures  Bank advances     1523500.0                 23500.0   \n",
      "3  Shares and Debentures  Bank advances     1523500.0                 23500.0   \n",
      "4  Shares and Debentures  Bank advances     1523500.0                 23500.0   \n",
      "\n",
      "   Total Institutional liabilities              Asset_Name  \n",
      "0                          10000.0                Currency  \n",
      "1                          10000.0   Shares and Debentures  \n",
      "2                          10000.0             Real Estate  \n",
      "3                          10000.0            Gross wealth  \n",
      "4                          10000.0  Total Financial Assets  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\347409114.py:69: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  asset_df.to_stata(\"final(28).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# Empty list to store new rows\n",
    "rows = []\n",
    "\n",
    "# Loop over each unique group\n",
    "for _, group in df_grouped2.groupby(group_cols):\n",
    "\n",
    "    # Get first row for copying other columns\n",
    "    first_row = group.iloc[0].to_dict()\n",
    "\n",
    "    # 1. Financial Asset Row\n",
    "    if not np.isnan(first_row['Fin_Asset_Value']):\n",
    "        fa_row = first_row.copy()\n",
    "        fa_row['Asset_serial'] = first_row['Fin_Asset_Serial']\n",
    "        fa_row['Asset_Value'] = first_row['Fin_Asset_Value']\n",
    "        fa_row['Asset_Name'] = first_row['Fin_Asset']\n",
    "        rows.append(fa_row)\n",
    "\n",
    "    # 2. Share Asset Row\n",
    "    if not np.isnan(first_row['Share_Asset_Value']):\n",
    "        sa_row = first_row.copy()\n",
    "        sa_row['Asset_serial'] = 200 + first_row['Share_Asset_serial']  # As you specified\n",
    "        sa_row['Asset_Value'] = first_row['Share_Asset_Value']\n",
    "        sa_row['Asset_Name'] = first_row['Share_Asset']\n",
    "        rows.append(sa_row)\n",
    "\n",
    "    # 3. Real Estate Row\n",
    "    if not np.isnan(first_row['Real Estate']):\n",
    "        re_row = first_row.copy()\n",
    "        re_row['Asset_serial'] = 300\n",
    "        re_row['Asset_Value'] = first_row['Real Estate']\n",
    "        re_row['Asset_Name'] = 'Real Estate'\n",
    "        rows.append(re_row)\n",
    "\n",
    "    # 4. Gross wealth Row\n",
    "    if not np.isnan(first_row['Gross wealth']):\n",
    "        w_row = first_row.copy()\n",
    "        w_row['Asset_serial'] = 1000\n",
    "        w_row['Asset_Value'] = first_row['Gross wealth']\n",
    "        w_row['Asset_Name'] = 'Gross wealth'\n",
    "        rows.append(w_row)\n",
    "\n",
    "    # 6. Total Financial Assets Row\n",
    "    if not np.isnan(first_row['Total Financial Assets']):\n",
    "        tfa_row = first_row.copy()\n",
    "        tfa_row['Asset_serial'] = 1500\n",
    "        tfa_row['Asset_Value'] = first_row['Total Financial Assets']\n",
    "        tfa_row['Asset_Name'] = 'Total Financial Assets'\n",
    "        rows.append(tfa_row)   \n",
    "\n",
    "# Convert to DataFrame\n",
    "asset_df = pd.DataFrame(rows)\n",
    "\n",
    "# Optional: Arrange columns nicely\n",
    "cols_order = group_cols + ['Asset_serial', 'Asset_Value'] + [col for col in df_grouped2.columns if col not in group_cols + ['Asset_serial', 'Asset_Value']]\n",
    "asset_df = asset_df[cols_order]\n",
    "\n",
    "# Check result\n",
    "print(asset_df.head())\n",
    "\n",
    "\n",
    "\n",
    "# Renaming Liabilities\n",
    "\n",
    "asset_df = asset_df.rename(columns={\n",
    "    'Credit_Agency': 'Liability_serial',\n",
    "    'Liability': 'Liability_name'\n",
    "})\n",
    "\n",
    "asset_df.to_stata(\"final(28).dta\", write_index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "c15da636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique subsets: 8062\n",
      "Number of duplicates: 32248\n",
      "Number of duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "group_cols = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial']\n",
    "group_cols1 = ['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial', 'Asset_serial']\n",
    "\n",
    "# Count unique combinations\n",
    "num_subsets = asset_df[group_cols].drop_duplicates().shape[0]\n",
    "\n",
    "print(f\"Number of unique subsets: {num_subsets}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")\n",
    "\n",
    "# Check for duplicates\n",
    "dup_check = asset_df.duplicated(subset=group_cols1)\n",
    "print(f\"Number of duplicates: {dup_check.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "0f5dd9ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HHID                                object\n",
       "Liability_serial                   float64\n",
       "Fin_Asset_Serial                     int64\n",
       "Share_Asset_serial                   int64\n",
       "Asset_serial                         int64\n",
       "Asset_Value                        float64\n",
       "State                                int64\n",
       "MCE                                  int64\n",
       "Weight                             float64\n",
       "Fin_Asset_Value                    float64\n",
       "Share_Asset_Value                  float64\n",
       "Liability_value                    float64\n",
       "Real Estate                        float64\n",
       "Fin_Asset                           object\n",
       "Share_Asset                         object\n",
       "Liability_name                      object\n",
       "Gross wealth                       float64\n",
       "Total Financial Assets             float64\n",
       "Total Institutional liabilities    float64\n",
       "Asset_Name                          object\n",
       "dtype: object"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "693dfeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset_serial    0\n",
      "Asset_Value     0\n",
      "Asset_Name      0\n",
      "dtype: int64\n",
      "\n",
      "Total rows with missing values: 0\n",
      "Empty DataFrame\n",
      "Columns: [HHID, Liability_serial, Fin_Asset_Serial, Share_Asset_serial, Asset_serial, Asset_Value, State, MCE, Weight, Fin_Asset_Value, Share_Asset_Value, Liability_value, Real Estate, Fin_Asset, Share_Asset, Liability_name, Gross wealth, Total Financial Assets, Total Institutional liabilities, Asset_Name]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "\n",
    "# Check how many missing in each column\n",
    "print(asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().sum())\n",
    "\n",
    "# Optionally, view rows with missing in any of them\n",
    "missing_rows = asset_df[\n",
    "    asset_df[['Asset_serial', 'Asset_Value', 'Asset_Name']].isna().any(axis=1)\n",
    "]\n",
    "\n",
    "print(f\"\\nTotal rows with missing values: {len(missing_rows)}\")\n",
    "print(missing_rows.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "b1d229ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = asset_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "10f234c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Liability_serial', 'Fin_Asset_Serial', 'Share_Asset_serial',\n",
       "       'Asset_serial', 'Asset_Value', 'State', 'MCE', 'Weight',\n",
       "       'Fin_Asset_Value', 'Share_Asset_Value', 'Liability_value',\n",
       "       'Real Estate', 'Fin_Asset', 'Share_Asset', 'Liability_name',\n",
       "       'Gross wealth', 'Total Financial Assets',\n",
       "       'Total Institutional liabilities', 'Asset_Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "aa117e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\486095085.py:9: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(2019).dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "final_df.drop(['Fin_Asset_Serial', \n",
    "               'Share_Asset_serial', \n",
    "               'Fin_Asset_Value',\n",
    "                'Share_Asset_Value', \n",
    "                'Fin_Asset',\n",
    "                'Share_Asset',               \n",
    "                ], axis=1, inplace=True)\n",
    "\n",
    "final_df.to_stata(\"final(2019).dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "dc5166e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'Liability_serial', 'Asset_serial', 'Asset_Value', 'State',\n",
       "       'MCE', 'Weight', 'Liability_value', 'Real Estate', 'Liability_name',\n",
       "       'Gross wealth', 'Total Financial Assets',\n",
       "       'Total Institutional liabilities', 'Asset_Name'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "0387db2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\340742307.py:7: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "    Real Estate   ->   Real_Estate\n",
      "    Total Financial Assets   ->   Total_Financial_Assets\n",
      "    Total Institutional liabilities   ->   Total_Institutional_liabilities\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df.to_stata(\"final(2019).dta\", write_index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = final_df[['HHID', 'State', 'MCE' , 'Weight','Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value',\n",
    "                    'Real Estate', 'Total Financial Assets', 'Total Institutional liabilities', \n",
    "                    ]].copy()\n",
    "\n",
    "final_df.to_stata(\"final(2019).dta\", write_index=False)\n",
    "\n",
    "final_df['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e733fc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\1597902558.py:6: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df1.to_stata(\"final(2019)_1.dta\", write_index=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df1 = final_df[['HHID', 'State', 'MCE', 'Weight', 'Gross wealth',  \n",
    "                    'Liability_serial', 'Liability_name','Liability_value',\n",
    "                    'Asset_serial', 'Asset_Name', 'Asset_Value'\n",
    "                    ]].copy()\n",
    "\n",
    "final_df1.to_stata(\"final(2019)_1.dta\", write_index=False)\n",
    "\n",
    "final_df1['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "bc6cbcd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HHID', 'State', 'MCE', 'Weight', 'Gross wealth', 'Liability_serial',\n",
       "       'Liability_name', 'Liability_value', 'Asset_serial', 'Asset_Name',\n",
       "       'Asset_Value'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "6f69958a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 32, 34, 35, 36, 37, 205, 300, 1000, 1500]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(final_df1['Asset_serial'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "47e6648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols1 = ['HHID', 'Liability_serial', 'Asset_serial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "dcf2bd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df2 = final_df1.drop_duplicates(subset=group_cols1, keep='first').reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "fa820f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\2964308011.py:1: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df2.to_stata(\"final(2019)_2.dta\", write_index=False)\n"
     ]
    }
   ],
   "source": [
    "final_df2.to_stata(\"final(2019)_2.dta\", write_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "7d24c624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All HHIDs are unique within each Asset_serial category.\n",
      " All HHIDs are unique within each Liability_serial category.\n"
     ]
    }
   ],
   "source": [
    "# Check for repetition\n",
    "\n",
    "# Check for uniqueness \n",
    "\n",
    "duplicates = final_df2.duplicated(subset=['Asset_serial', 'HHID', 'Liability_serial'], keep=False)\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Asset_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Asset_serial', 'HHID']].sort_values(['Asset_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Asset_serial category.\")\n",
    "\n",
    "# Check if there are any duplicates\n",
    "if duplicates.any():\n",
    "    print(\"Some (Liability_serial, HHID) combinations are repeated:\")\n",
    "    print(final_df2.loc[duplicates, ['Liability_serial', 'HHID']].sort_values(['Liability_serial', 'HHID']))\n",
    "else:\n",
    "    print(\" All HHIDs are unique within each Liability_serial category.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "c20fab48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1004"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df2['HHID'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f60db1f",
   "metadata": {},
   "source": [
    "# Add State names corresponding to their codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "14214b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\siddu\\AppData\\Local\\Temp\\ipykernel_46848\\2889506323.py:49: InvalidColumnName: \n",
      "Not all pandas column names were valid Stata variable names.\n",
      "The following replacements have been made:\n",
      "\n",
      "    Gross wealth   ->   Gross_wealth\n",
      "\n",
      "If this is not what you expect, please make sure you have Stata-compliant\n",
      "column names in your DataFrame (strings only, max 32 characters, only\n",
      "alphanumerics and underscores, no Stata reserved words)\n",
      "\n",
      "  final_df3.to_stata('2019_final.dta', write_index=False)\n"
     ]
    }
   ],
   "source": [
    "# final_df2 (Merge D&D, D&N)\n",
    "final_df2.loc[final_df2['State'].isin([25,26]), 'State'] = 25\n",
    "final_df2['State'].value_counts()\n",
    "\n",
    "# State Map\n",
    "\n",
    "State_map = {\n",
    "    1.0: 'Jammu and Kashmir',\n",
    "    2.0: 'Himachal Pradesh',\n",
    "    3.0: 'Punjab',\n",
    "    4.0: 'Chandigarh',\n",
    "    5.0: 'Uttarakhand',\n",
    "    6.0: 'Haryana',\n",
    "    7.0: 'Delhi',\n",
    "    8.0: 'Rajasthan',\n",
    "    9.0: 'Uttar Pradesh',\n",
    "    10.0: 'Bihar',\n",
    "    11.0: 'Sikkim',\n",
    "    12.0: 'Arunachal Pradesh',\n",
    "    13.0: 'Nagaland',\n",
    "    14.0: 'Manipur',\n",
    "    15.0: 'Mizoram',\n",
    "    16.0: 'Tripura',\n",
    "    17.0: 'Meghalaya',\n",
    "    18.0: 'Assam',\n",
    "    19.0: 'West Bengal',\n",
    "    20.0: 'Jharkhand',\n",
    "    21.0: 'Odisha',\n",
    "    22.0: 'Chhattishgarh',\n",
    "    23.0: 'Madhya Pradesh',\n",
    "    24.0: 'Gujarat',\n",
    "    25.0: 'Daman and Diu and Dadra and Nagar Haveli',\n",
    "    27.0: 'Maharashtra',\n",
    "    28.0: 'Andhra Pradesh',\n",
    "    29.0: 'Karnataka',\n",
    "    30.0: 'Goa',\n",
    "    31.0: 'Lakshadweep',\n",
    "    32.0: 'Kerala',\n",
    "    33.0: 'Tamilnadu',\n",
    "    34.0: 'Puducherry',\n",
    "    35.0: 'Andaman & Nicobar',\n",
    "    36.0: 'Telengana'\n",
    "}\n",
    "\n",
    "final_df3 = final_df2.copy()\n",
    "\n",
    "final_df3['State_name'] = final_df3['State'].map(State_map)\n",
    "\n",
    "final_df3.to_stata('2019_final.dta', write_index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
